[
{
	"uri": "http://localhost:1313/1-setup-infrastructure/",
	"title": "1. Setup &amp; Infrastructure Deployment",
	"tags": [],
	"description": "",
	"content": "Setup \u0026amp; Infrastructure Deployment ğŸš€ Complete guide for setting up AWS infrastructure for DynamoDB Advanced Patterns workshop\nThis module provides the foundational setup required for the DynamoDB Advanced Patterns workshop, ensuring all participants have a working environment using AWS Free Tier.\nğŸ“‹ Learning Objectives By the end of this module, you will:  âœ… Verify AWS account and Free Tier eligibility âœ… Navigate AWS Console efficiently âœ… Deploy infrastructure via CloudFormation âœ… Verify all resources created successfully âœ… Setup monitoring and billing alerts  â±ï¸ Module Duration: 90 minutes  Theory: 15 minutes Demo: 25 minutes Hands-on: 40 minutes Review: 10 minutes  ğŸ“š AWS Free Tier Overview AWS Free Tier Limits for our workshop:\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ Service â”‚ Free Tier â”‚ Our Usage â”‚\râ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\râ”‚ DynamoDB â”‚ 25 RCU/WCU â”‚ 15 RCU/WCU â”‚\râ”‚ Lambda â”‚ 1M requests â”‚ ~100/day â”‚\râ”‚ CloudWatch â”‚ 10 metrics â”‚ 6 metrics â”‚\râ”‚ Data Transfer â”‚ 1GB â”‚ \u0026lt;100MB â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\rWe're using only 60% of limits = 100% safe!\r\rCRITICAL SAFETY NOTE: We\u0026rsquo;re using AWS Free Tier exclusively. If you see ANY charges appearing during this workshop, please stop immediately and verify your configuration. All resources will be deployed within Free Tier limits.\n\rğŸ—ï¸ Architecture Overview We\u0026rsquo;ll deploy infrastructure across two AWS regions: ğŸ¯ What We\u0026rsquo;ll Build We\u0026rsquo;re building a simplified e-commerce platform with:  Users: Customer profiles and authentication Products: Catalog with categories and pricing Orders: Shopping cart and order management Real-time processing: Stream-based updates Global availability: Multi-region deployment  ğŸ“¦ Resources Created This CloudFormation template will create:  DynamoDB Table: Global table with streams enabled Lambda Function: Stream processor for real-time updates IAM Roles: Secure access policies CloudWatch Dashboard: Monitoring and metrics Billing Alerts: Cost protection mechanisms  ğŸš€ Prerequisites Before starting, ensure you have:  AWS Account with Free Tier eligibility Administrative access to AWS Console Basic understanding of AWS services Modern web browser (Chrome, Firefox, Safari)  Let\u0026rsquo;s begin with the infrastructure setup that will support our multi-region DynamoDB implementation.  You can choose Personal or Business account  Add payment method  Enter your credit card information and select Verify and Add.  Note: You can choose a different address for your account by selecting Use a new address before Verify and Add.    Verify your phone number  Enter the phone number. Enter the security check code then select Call me now. AWS will contact and verify account opening.  Select Support Plan  In the Select a support plan page, select an effective plan, to compare plans, see Compare AWS Support Plans.  Wait for your account to be activated  After selecting Support plan, the account is usually activated after a few minutes, but the process can take up to 24 hours. You will still be able to log in to your AWS account at this time, the AWS Home page may show a â€œComplete Sign Upâ€ button during this time, even if you have completed all the steps in the registration section. After receiving an email confirming your account has been activated, you can access all AWS services.  Important  The following AWS Identity and Access Management (IAM) actions will reach the end of standard support on July 2023: aws-portal:ModifyAccount and aws-portal:ViewAccount. See the Using fine-grained AWS Billing actions to replace these actions with fine-grained actions so you have access to AWS Billing, AWS Cost Management, and AWS accounts consoles. If you created your AWS account or AWS Organizations Management account before March 6, 2023, the fine-grained actions will be effective starting July 2023. We recommend you to add the fine-grained actions, but not remove your existing permissions with aws-portal or purchase-orders prefixes. If you created your AWS account or AWS Organizations Management account on or after March 6, 2023, the fine-grained actions are effective immediately. AWS assigns the following unique identifiers to each AWS account: AWS account ID: A 12-digit number, such as 012345678901, that uniquely identifies an AWS account. Many AWS resources include the account ID in their Amazon Resource Names (ARNs). The account ID portion distinguishes resources in one account from the resources in another account. If you\u0026rsquo;re an AWS Identity and Access Management (IAM) user, you can sign in to the AWS Management Console using either the account ID or account alias. While account IDs, like any identifying information, should be used and shared carefully, they are not considered secret, sensitive, or confidential information. Canonical user ID: An alpha-numeric identifier, such as 79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be, that is an obfuscated form of the AWS account ID. You can use this ID to identify an AWS account when granting cross-account access to buckets and objects using Amazon Simple Storage Service (Amazon S3). You can retrieve the canonical user ID for your AWS account as either the root user or an IAM user. You must be authenticated with AWS to view these identifiers.  Warning Do not provide your AWS credentials (including passwords and access keys) to a third party that needs your AWS account identifiers to share AWS resources with you. Doing so would give them the same access to the AWS account that you have.\n"
},
{
	"uri": "http://localhost:1313/",
	"title": "DynamoDB Advanced Patterns Workshop",
	"tags": [],
	"description": "",
	"content": "DynamoDB Advanced Patterns Building Multi-Region Architectures with Global Tables and Streams Overview In this comprehensive workshop, you will be building a Multi-Region E-commerce Platform using DynamoDB Advanced Patterns and AWS Free Tier. You will learn Single Table Design, implement Global Tables for multi-region replication, and build real-time stream processing with Lambda. Finally, we will implement comprehensive monitoring and optimization strategies while maintaining strict cost control within Free Tier limits.\nSingle Table Design Single Table Design is a DynamoDB modeling approach where you store multiple entity types in one table using composite keys. This pattern optimizes for performance and cost by reducing the number of requests and leveraging DynamoDB\u0026rsquo;s partition-based architecture. When implemented correctly, it provides sub-millisecond query performance while minimizing capacity consumption.\nAs a best practice, design your access patterns first before creating your table structure. Single Table Design requires careful planning of partition keys (PK) and sort keys (SK) to support all your query patterns efficiently. This workshop uses a proven e-commerce data model that supports 6 optimized access patterns while staying within Free Tier limits.\n\rGlobal Tables Multi-Region Global Tables provide fully managed multi-region, multi-active database replication. Data written to any region is automatically replicated to all other regions within seconds. This enables you to build globally distributed applications with local read and write access, improving performance and providing disaster recovery capabilities.\nDynamoDB Streams \u0026amp; Lambda DynamoDB Streams capture data modification events in your table in near real-time. When combined with AWS Lambda, you can build event-driven architectures that automatically process changes, update derived data, send notifications, or trigger business workflows. This pattern is essential for building reactive, scalable applications.\nGlobal Secondary Indexes (GSI) Global Secondary Indexes allow you to query your data using different access patterns than your main table. GSIs have their own partition and sort keys, enabling efficient queries across different dimensions of your data. Proper GSI design is crucial for performance optimization and cost control.\nMonitoring \u0026amp; Cost Optimization CloudWatch monitoring provides real-time visibility into your DynamoDB performance, capacity utilization, and costs. Combined with billing alerts and Free Tier tracking, you can ensure optimal performance while maintaining strict cost control. This workshop implements comprehensive monitoring dashboards and automated alerting.\nFree Tier Compliance AWS Free Tier provides generous limits for learning and experimentation. This workshop is designed to use only 60% of available Free Tier resources, ensuring zero cost while providing enterprise-grade learning experience. All participants will implement production-ready patterns without incurring any charges.\nMain Content  Setup \u0026amp; Infrastructure Deployment Single Table Design Implementation Global Tables Multi-Region Setup DynamoDB Streams \u0026amp; Lambda Processing Monitoring \u0026amp; Performance Optimization Advanced Patterns \u0026amp; Best Practices Cleanup \u0026amp; Resource Management  "
},
{
	"uri": "http://localhost:1313/2-single-table-design/",
	"title": "2. Single Table Design Implementation",
	"tags": [],
	"description": "",
	"content": "Single Table Design Implementation ğŸ“Š Learn how to implement DynamoDB Single Table Design patterns for optimal performance and cost efficiency\nOverview  Single Table Design is a revolutionary approach to data modeling in DynamoDB. Instead of using multiple tables like in relational databases, we store all entity types (Users, Products, Orders) in one table using composite keys for relationships.  Why Single Table Design? Traditional Relational Approach Problems:  Multiple tables = Multiple queries = Higher latency JOINs are expensive and not available in DynamoDB Inconsistent performance across different query patterns Higher costs from managing multiple tables  DynamoDB Single Table Benefits:  Single query retrieves related data Consistent performance across all access patterns Lower costs with fewer tables and operations Atomic transactions across entity types  Learning Objectives By the end of this module, you will:  âœ… Understand Single Table Design principles and benefits âœ… Design composite keys (PK + SK) for multiple entity types âœ… Create and query data using DynamoDB Console âœ… Implement Global Secondary Indexes (GSI) for flexible access patterns âœ… Analyze performance metrics and costs  Module Duration: 90 minutes  Theory: 20 minutes - Core concepts and principles Demo: 25 minutes - Console navigation and data creation Hands-on: 35 minutes - Create your own e-commerce data Review: 10 minutes - Performance analysis and Q\u0026amp;A  E-commerce Data Model Overview We\u0026rsquo;ll build a simplified e-commerce platform with these entities: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ EcommerceTable â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ PK â”‚ SK â”‚ Entity â”‚ Data â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ USER#user1 â”‚ PROFILE â”‚ User â”‚ name, email â”‚ â”‚ USER#user1 â”‚ ORDER#ord1 â”‚ Order â”‚ status, $ â”‚ â”‚ PRODUCT#p1 â”‚ DETAILS â”‚ Product â”‚ name, price â”‚ â”‚ ORDER#ord1 â”‚ ITEM#p1 â”‚ OrderItem â”‚ qty, price â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Access Patterns We\u0026rsquo;ll Implement    Pattern Description Query Method     1 Get user profile PK = USER#id, SK = PROFILE   2 Get user\u0026rsquo;s orders PK = USER#id, SK begins_with ORDER#   3 Get order details with items PK = ORDER#id   4 Get products by category GSI1: CATEGORY# queries   5 Get products by price range GSI2: PRICE# queries   6 Get orders by status GSI2: STATUS# queries    Key Concepts Composite Keys Strategy  Partition Key (PK): Groups related items together Sort Key (SK): Enables range queries and relationships GSI Keys: Enable additional query patterns  Entity Namespacing  USER#: All user-related data PRODUCT#: All product-related data ORDER#: All order-related data CATEGORY#: Product groupings STATUS#: Order status groupings  Design Philosophy: In Single Table Design, we model our table structure based on HOW we\u0026rsquo;ll query the data, not how we\u0026rsquo;ll store it. This is the opposite of relational database design!\n\rWhat You\u0026rsquo;ll Build By the end of this module, you\u0026rsquo;ll have created:  User profiles with proper key structure Product catalog with category and price indexing Order management with item relationships Efficient queries using table and GSI patterns Performance insights from CloudWatch metrics  Cost Safety: All exercises use minimal data and stay well within AWS Free Tier limits. Monitor the CloudWatch dashboard to track usage.\n\rPrerequisites Before starting this module, ensure you have:  Completed Module 1: Infrastructure Setup DynamoDB table demo-ecommerce-freetier is Active Access to AWS Console with DynamoDB permissions Basic understanding of NoSQL concepts  Ready to revolutionize your data modeling approach? Let\u0026rsquo;s dive into Single Table Design!\n"
},
{
	"uri": "http://localhost:1313/3-global-tables-setup/",
	"title": "3. Global Tables Multi-Region Setup",
	"tags": [],
	"description": "",
	"content": "Global Tables Multi-Region Setup ğŸŒ Implement DynamoDB Global Tables for worldwide low-latency access and automatic multi-region replication\nOverview Global Tables transforms your single-region DynamoDB table into a globally distributed database that serves users worldwide with single-digit millisecond latency. This module demonstrates how to leverage the Global Tables setup from your CloudFormation deployment.\nWhy Global Tables? The Global Challenge Modern applications serve users across continents. Traditional single-region databases create problems:  High Latency: Users far from the database experience slow response times No Disaster Recovery: Single point of failure if region goes down Limited Scale: All traffic funneled through one region  Global Tables Solution DynamoDB Global Tables solves these challenges: Before Global Tables: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ US-EAST-1 â”‚ Global Users â”‚ â”‚ (High Latency) â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ DynamoDB â”‚ â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€ ğŸŒ All Traffic â”‚ â”‚ Table â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ Single Point â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ of Failure After Global Tables: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ US-EAST-1 â”‚ â”‚ EU-WEST-1 â”‚ â”‚ (Primary) â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ (Replica) â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ DynamoDB â”‚ â”‚ â”‚ â”‚ DynamoDB â”‚ â”‚ â”‚ â”‚ Table â”‚ â”‚ â”‚ â”‚ Replica â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ ğŸ‡ºğŸ‡¸ US Users ğŸ‡ªğŸ‡º EU Users (Low Latency) (Low Latency) Learning Objectives By the end of this module, you will:  âœ… Understand Global Tables architecture and replication mechanics âœ… Verify multi-region setup from CloudFormation deployment âœ… Test cross-region read/write operations through AWS Console âœ… Experience eventual consistency and conflict resolution âœ… Monitor replication performance and health metrics âœ… Handle real-world global database scenarios  Module Duration: 120 minutes  Theory: 25 minutes - Architecture and concepts Demo: 35 minutes - Console navigation and verification Hands-on: 50 minutes - Multi-region operations practice Review: 10 minutes - Performance analysis and Q\u0026amp;A  Global Tables Architecture Replication Mechanics DynamoDB Global Tables uses asynchronous replication powered by DynamoDB Streams: Global Replication Flow: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ 1. Write to US-EAST-1 â”‚ â”‚ User creates order: ORDER#12345 â”‚ â”‚ â”‚ â”‚ 2. DynamoDB Streams captures change â”‚ â”‚ Stream record: INSERT ORDER#12345 â”‚ â”‚ â”‚ â”‚ 3. Cross-region replication â”‚ â”‚ Stream â†’ EU-WEST-1 replica â”‚ â”‚ â”‚ â”‚ 4. Write to EU-WEST-1 â”‚ â”‚ ORDER#12345 now available in Europe â”‚ â”‚ â”‚ â”‚ 5. Typical replication time: 0.5-2 seconds â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Key Characteristics  Bi-directional: Can read and write from any region Eventually Consistent: Changes propagate within seconds Automatic: No manual intervention required Conflict Resolution: Last Writer Wins based on timestamps  Real-World Benefits Performance Benefits  Sub-10ms latency for users worldwide Regional failover for disaster recovery Horizontal scaling across continents  Cost Benefits  No cross-region data transfer charges for replication Free Tier applies per region (2x the capacity!) No additional complexity in application code  Operational Benefits  Zero downtime for regional outages Simplified global architecture Built-in monitoring and health checks  Workshop Setup Our CloudFormation template has already configured:\n   Region Role Status Purpose     US-East-1 Primary âœ… Active Main production region   EU-West-1 Replica âœ… Active European users    What You\u0026rsquo;ll Experience Multi-Region Operations  Create data in US â†’ Verify it appears in EU Write from EU â†’ See reverse replication to US Simultaneous updates â†’ Experience conflict resolution Monitor replication â†’ Track performance metrics  Conflict Resolution Scenarios Conflict Example: Time: 10:00:00 - User updates profile in US-EAST-1 name: \u0026#34;John Doe\u0026#34; â†’ \u0026#34;John Smith\u0026#34; Time: 10:00:01 - Same user updates profile in EU-WEST-1 name: \u0026#34;John Doe\u0026#34; â†’ \u0026#34;John D. Doe\u0026#34; Result: \u0026#34;Last Writer Wins\u0026#34; - EU update at 10:00:01 wins Final value in both regions: \u0026#34;John D. Doe\u0026#34; Consistency Model Eventually Consistent Reads  Writes are immediately consistent in the region where written Cross-region reads may show stale data for 0.5-2 seconds All regions eventually have identical data  Application Design Implications  Design for eventual consistency Handle temporary inconsistencies gracefully Use timestamps for conflict detection Consider strongly consistent reads when needed (single region only)  Global Advantage: While traditional databases require complex master-slave setups, DynamoDB Global Tables provides multi-master replication out of the box!\n\rPrerequisites Before starting this module, ensure you have:  Completed Module 1: Infrastructure Setup Completed Module 2: Single Table Design DynamoDB table demo-ecommerce-freetier is Active in both regions Understanding of eventual consistency concepts Ability to switch between AWS regions in console  Cost Safety: Global Tables replication is included in AWS Free Tier. All exercises stay within Free Tier limits across both regions.\n\rSuccess Metrics By module completion, you will have:  âœ… Verified Global Tables configuration in both regions âœ… Created data in multiple regions and observed replication âœ… Experienced conflict resolution with Last Writer Wins âœ… Monitored replication performance through CloudWatch âœ… Understood global consistency trade-offs  Ready to go global? Let\u0026rsquo;s explore how your local DynamoDB table becomes a worldwide database!\n"
},
{
	"uri": "http://localhost:1313/4-streams-lambda-processing/",
	"title": "4. Streams &amp; Lambda Processing",
	"tags": [],
	"description": "",
	"content": "DynamoDB Streams \u0026amp; Lambda Processing âš¡ Real-time event processing with DynamoDB Streams and AWS Lambda\nModule Overview Transform your static database into a reactive, event-driven system that automatically responds to every data change in real-time.\nWhat You\u0026rsquo;ll Learn  Stream Architecture: Understand how DynamoDB captures and processes data changes Lambda Integration: Configure serverless functions to respond to database events Event Processing: Handle INSERT, MODIFY, and REMOVE events effectively Real-time Patterns: Implement common event-driven architecture patterns Monitoring \u0026amp; Debugging: Track performance and troubleshoot stream processing  Architecture Overview â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ Application â”‚ â”‚ DynamoDB â”‚ â”‚ DynamoDB Streamsâ”‚\râ”‚ â”‚ â”‚ Table â”‚ â”‚ â”‚\râ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\râ”‚ â”‚ Write â”‚â”€â”€â”¼â”€â”€â”€â–ºâ”‚ â”‚ Item â”‚â”€â”€â”¼â”€â”€â”€â–ºâ”‚ â”‚ Stream â”‚ â”‚\râ”‚ â”‚ Item â”‚ â”‚ â”‚ â”‚ Created â”‚ â”‚ â”‚ â”‚ Record â”‚ â”‚\râ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\râ”‚\râ–¼\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ Lambda â”‚ â”‚ Event Source â”‚ â”‚ Stream â”‚\râ”‚ Function â”‚ â”‚ Mapping â”‚ â”‚ Shards â”‚\râ”‚ â”‚ â”‚ â”‚ â”‚ â”‚\râ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\râ”‚ â”‚ Process â”‚â—„â”€â”¼â”€â”€â”€â”€â”‚ â”‚ Poll â”‚â—„â”€â”¼â”€â”€â”€â”€â”‚ â”‚ Records â”‚ â”‚\râ”‚ â”‚ Records â”‚ â”‚ â”‚ â”‚ Stream â”‚ â”‚ â”‚ â”‚ Queue â”‚ â”‚\râ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\rKey Benefits Real-time Processing\n Process data changes within 100-500 milliseconds No polling required - events pushed automatically Scale to millions of events per second  Event-Driven Architecture\n Decouple data storage from business logic Trigger multiple downstream systems Build reactive, responsive applications  Cost Effective\n Pay only for actual processing time AWS Free Tier includes 1M Lambda invocations No infrastructure to manage  Common Use Cases    Pattern Trigger Action     Audit Trail Any change Log to S3/CloudWatch   Cache Invalidation Item update Clear Redis/ElastiCache   Notifications Order created Send email/SMS   Analytics User activity Update metrics dashboard   Search Index Product change Update Elasticsearch   Workflow Status change Trigger Step Functions    Stream Processing Patterns Fan-out Pattern: One change triggers multiple Lambda functions\nDynamoDB Change â†’ Stream â†’ Lambda 1 (Email)\râ†’ Lambda 2 (Analytics) â†’ Lambda 3 (Cache Update)\rPipeline Pattern: Sequential processing through multiple stages\nOrder Created â†’ Validate â†’ Process Payment â†’ Update Inventory â†’ Ship\rAggregation Pattern: Combine multiple changes into summaries\nSales Records â†’ Real-time Revenue Dashboard\rUser Actions â†’ Activity Analytics\rPerformance Characteristics  Latency: Typically 100-500ms from change to processing Throughput: Scales automatically with your data volume Reliability: Automatic retries and error handling Ordering: Changes processed in order per item Retention: Stream records available for 24 hours  Module Structure This module is organized into hands-on sections that build upon each other:  Stream Configuration - Enable and configure DynamoDB Streams Lambda Function Setup - Create and deploy stream processing functions Event Processing Practice - Test with real data changes Monitoring \u0026amp; Debugging - Track performance and troubleshoot issues  Each section includes:  âœ… Step-by-step AWS Console instructions âœ… Code examples and templates âœ… Screenshot placeholders for documentation âœ… Troubleshooting guides âœ… Real-world scenarios  Prerequisites Before starting this module, ensure you have:  âœ… Completed Module 1 (DynamoDB table setup) âœ… Basic understanding of AWS Lambda âœ… Familiarity with JSON and event-driven concepts âœ… AWS Console access with appropriate permissions  Free Tier Optimization: All exercises are designed to stay within AWS Free Tier limits, including Lambda invocations and DynamoDB streams.\n\rLearning Objectives By the end of this module, you will:  Understand DynamoDB Streams architecture and event flow Configure Lambda functions to process stream events Implement common event-driven patterns Monitor stream processing performance Debug stream processing issues Design scalable event-driven applications  Ready to Build: Transform your static database into a reactive, event-driven system that automatically responds to every change!\n\r\r4.1 Stream Configuration\r\r\r4.2 Lambda Function Setup\r\r\r4.3 Event Processing Practice\r\r\r4.4 Monitoring \u0026amp; Debugging\r\r\r DynamoDB Streams configuration Lambda stream processor functions Real-time order processing pipeline Error handling and dead letter queues  Let\u0026rsquo;s implement real-time stream processing for your e-commerce platform.\n"
},
{
	"uri": "http://localhost:1313/5-monitoring-optimization/",
	"title": "5. Monitoring &amp; Optimization",
	"tags": [],
	"description": "",
	"content": "Monitoring \u0026amp; Optimization ğŸ“ˆ Comprehensive monitoring, alerting, and cost optimization for production DynamoDB systems\nModule Overview Transform from a builder to an operator with production-ready monitoring, proactive alerting, and cost optimization strategies that ensure your DynamoDB system runs efficiently and stays within budget.\nWhat You\u0026rsquo;ll Master  CloudWatch Dashboards: Create comprehensive monitoring views Proactive Alerting: Set up intelligent alarms for critical metrics Cost Analysis: Monitor Free Tier usage and optimize expenses Performance Optimization: Right-size capacity and improve efficiency Operational Excellence: Build runbooks and best practices  Monitoring Strategy Building robust systems requires monitoring at multiple levels:\nMonitoring Pyramid:\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ Business Metrics â”‚\râ”‚ (Orders/min, Revenue, Users) â”‚\râ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\râ”‚ Application Metrics â”‚\râ”‚ (Response time, Error rate, Throughput) â”‚\râ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\râ”‚ Infrastructure Metrics â”‚\râ”‚ (CPU, Memory, Disk, Network, Capacity) â”‚\râ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\râ”‚ System Logs â”‚\râ”‚ (Errors, Debug info, Audit trail) â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\rKey DynamoDB Metrics Essential metrics for production operations:\n   Category Metric Alert Threshold Business Impact     Capacity ConsumedRCU/WCU \u0026gt;80% of limit Performance throttling   Performance SuccessfulLatency \u0026gt;50ms User experience   Reliability UserErrors \u0026gt;1% Application failures   Global Tables ReplicationLatency \u0026gt;5 seconds Data consistency   Streams IteratorAge \u0026gt;30 seconds Processing delays   Cost Storage Size \u0026gt;20GB (Free Tier) Billing impact    Cost Optimization Framework AWS Free Tier limits and optimization strategies:\nDynamoDB Cost Components:\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ Component â”‚ Free Tier Limit â”‚ Optimization â”‚\râ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\râ”‚ Read Capacity â”‚ 25 RCU â”‚ Efficient queriesâ”‚\râ”‚ Write Capacity â”‚ 25 WCU â”‚ Batch operations â”‚\râ”‚ Storage â”‚ 25 GB â”‚ Data lifecycle â”‚\râ”‚ Global Tables â”‚ Same limits â”‚ Region strategy â”‚\râ”‚ Backup â”‚ Continuous (Free) â”‚ Point-in-time â”‚\râ”‚ Data Transfer â”‚ 1 GB out â”‚ Regional access â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\rMonitoring Architecture Comprehensive observability setup:\n Real-time Dashboards: Visual health monitoring Intelligent Alerting: Proactive issue detection Cost Tracking: Prevent billing surprises Performance Analysis: Optimization insights Operational Runbooks: Incident response procedures  Benefits of Proper Monitoring Production readiness advantages:\nProactive Management\n Detect issues before users notice Prevent capacity throttling Optimize costs continuously  Operational Excellence\n Reduce mean time to resolution Enable data-driven decisions Scale confidently with visibility  Cost Control\n Stay within Free Tier limits Identify optimization opportunities Track spending patterns  Module Structure This module covers monitoring from setup to optimization:  CloudWatch Dashboards - Visual monitoring setup Alerting \u0026amp; Notifications - Proactive issue detection Cost Analysis \u0026amp; Optimization - Free Tier management Performance Tuning - Capacity and efficiency optimization  Each section includes:  âœ… Step-by-step AWS Console instructions âœ… Dashboard configuration examples âœ… Alert templates and thresholds âœ… Screenshot placeholders for documentation âœ… Real-world optimization scenarios  Prerequisites Before starting this module, ensure you have:  âœ… Completed previous modules (working DynamoDB setup) âœ… Generated some data and activity for metrics âœ… Understanding of CloudWatch concepts âœ… Access to AWS Billing and Cost Management  Free Tier Focus: All monitoring and optimization strategies are designed to maximize value while staying within AWS Free Tier limits.\n\rLearning Objectives By the end of this module, you will:  Design comprehensive monitoring dashboards Configure intelligent alerting for critical metrics Analyze costs and optimize Free Tier usage Implement performance optimization strategies Create operational runbooks for production Understand scaling and capacity planning  Real-World Impact Enterprise-grade monitoring that would typically cost hundreds per month:\n Advanced CloudWatch dashboards Multi-layered alerting system Cost optimization automation Performance monitoring suite Operational excellence framework  Production Ready: Build monitoring systems that rival enterprise solutions - all within AWS Free Tier!\n\r\r5.1 CloudWatch Dashboards\r\r\r5.2 Alerting \u0026amp; Notifications\r\r\r5.3 Cost Analysis \u0026amp; Optimization\r\r\r5.4 Performance Tuning\r\r\r CloudWatch monitoring dashboards Performance and cost optimization alerts Automated scaling configurations Best practices for ongoing maintenance  Let\u0026rsquo;s optimize your DynamoDB performance and costs.\n"
},
{
	"uri": "http://localhost:1313/6-advanced-patterns/",
	"title": "6. Advanced Patterns",
	"tags": [],
	"description": "",
	"content": "Advanced DynamoDB Patterns ğŸš€ Master enterprise-grade DynamoDB techniques for production-scale applications\nModule Overview This module focuses on advanced patterns that separate beginners from experts in DynamoDB development. You\u0026rsquo;ll learn the sophisticated techniques used by companies like Netflix, Airbnb, and other organizations running at massive scale.\nWhat You\u0026rsquo;ll Master Advanced Operations:\n Batch Operations: Process multiple items efficiently with single API calls Conditional Updates: Implement data integrity and prevent race conditions Optimistic Locking: Handle concurrent access with version control Advanced Queries: Optimize performance and minimize costs  Enterprise Patterns:\n Data Integrity: Prevent overselling and data corruption Performance Optimization: Reduce API calls and improve response times Cost Efficiency: Maximize Free Tier usage with optimal patterns Production Readiness: Implement robust, scalable solutions  Key Concepts Efficiency Through Batching Single vs Batch Operations:\nâŒ Inefficient (Single Operations):\rfor item in items:\rdynamodb.put_item(item) # 100 API calls\râœ… Efficient (Batch Operations):\rdynamodb.batch_write_item(items) # 4 API calls (25 items each)\rBenefits of batch operations:\n Reduced API calls: Up to 25 items per batch for writes Lower latency: Fewer network round trips Cost savings: Fewer request units consumed Free Tier optimization: Maximize efficiency within limits  Data Integrity with Conditions Problem: Race Conditions\nScenario: Two users try to buy the last item\rUser A reads: stock = 1\rUser B reads: stock = 1\rUser A updates: stock = 0 âœ…\rUser B updates: stock = 0 âŒ (Should fail!)\rWithout conditions: Both succeed (oversold!)\rWith conditions: Only first succeeds âœ…\rConditional update types:\n attribute_exists: Ensure item exists before update attribute_not_exists: Prevent duplicate creation Comparison operators: Check values before modification Version checking: Implement optimistic locking  Query Optimization Patterns Performance optimization techniques:\n Projection Expressions: Select only needed attributes to reduce RCU Filter Expressions: Refine results with post-query filtering Pagination: Handle large result sets efficiently Parallel Operations: Process multiple segments concurrently  Learning Path This module follows a progressive learning approach:\n 6.1 Batch Operations: Learn efficient multi-item processing 6.2 Conditional Updates: Implement data integrity patterns 6.3 Advanced Query Techniques: Optimize performance and costs 6.4 Production Patterns: Apply enterprise-grade best practices  Real-World Applications E-commerce Platform:\n Batch product imports and updates Inventory management with conditional updates Order processing with optimistic locking Efficient catalog queries with projections  Content Management:\n Bulk content operations Version control for collaborative editing User permission checks with conditions Optimized content delivery queries  Analytics Dashboard:\n Batch data ingestion Concurrent metric updates Efficient data aggregation queries Performance-optimized reporting  Prerequisites Before starting this module, ensure you have:\n âœ… Completed Modules 1-5 (setup through monitoring) âœ… Understanding of single-table design principles âœ… Familiarity with GSI patterns âœ… Basic query and scan operations knowledge  Expected Outcomes By the end of this module, you will:\n âœ… Master batch operations for efficient data processing âœ… Implement conditional updates to ensure data integrity âœ… Use optimistic locking for concurrent access control âœ… Optimize queries for performance and cost efficiency âœ… Apply production patterns used by enterprise applications  Free Tier Focus: All exercises are designed to work within AWS Free Tier limits while teaching production-grade patterns used by companies processing billions of requests.\n\r Let\u0026rsquo;s dive into advanced DynamoDB patterns that will transform you from a beginner into an expert!\n Advanced query patterns and optimizations Security configurations and access controls Backup and disaster recovery strategies Production-ready deployment patterns  Let\u0026rsquo;s explore advanced DynamoDB patterns for enterprise applications.\n"
},
{
	"uri": "http://localhost:1313/7-cleanup-resources/",
	"title": "7. Cleanup Resources",
	"tags": [],
	"description": "",
	"content": "Resource Cleanup In this section, you will learn how to clean up resources on AWS Cloud to prevent incurring unnecessary costs.\nWhy Cleanup is Important  Cost Control: Prevent unexpected AWS charges Resource Management: Remove unused infrastructure Best Practices: Learn proper resource lifecycle management  1. Delete the DynamoDB Table Created in the Lab  Access the DynamoDB Management Console On the left navigation bar, select Tables Select the DynamoDB table demo-ecommerce-freetier related to the lab Click on Actions Choose Delete table Type the table name to confirm Confirm by clicking Delete  2. Delete CloudWatch Resources Created in the Lab  Access the CloudWatch Management Console On the left navigation bar, go to Dashboards Select all dashboards related to the lab Click on Actions Choose Delete dashboards Confirm deletion by clicking Delete On the left navigation bar, go to Alarms Select all alarms related to the lab Click on Actions Choose Delete Confirm deletion by clicking Delete  3. Delete Lambda Functions (if created)  Access the Lambda Management Console On the left navigation bar, navigate to Functions Select the Lambda functions related to the lab Click on Actions Choose Delete function Confirm by clicking Delete  4. Delete SNS Topics (if created)  Access the SNS Management Console On the left navigation bar, select Topics Select all SNS topics related to the lab Click on Actions Choose Delete Type \u0026ldquo;delete me\u0026rdquo; to confirm Confirm deletion by clicking Delete  5. Verify Billing and Costs  Access the Billing and Cost Management Console Check the current month charges Verify that all charges show $0.00 Confirm no unexpected services are running  Final Verification âœ… Cleanup Checklist:\n DynamoDB table deleted CloudWatch dashboards and alarms removed Lambda functions deleted (if any) SNS topics deleted (if any) Billing shows $0.00 charges   Congratulations! You have successfully completed the DynamoDB Advanced Patterns workshop and cleaned up all resources.\n"
},
{
	"uri": "http://localhost:1313/1-setup-infrastructure/1.1-aws-freetier-overview/",
	"title": "1.1 AWS Free Tier Overview",
	"tags": [],
	"description": "",
	"content": "AWS Free Tier Overview ğŸ†“ Understanding AWS Free Tier limits and ensuring zero-cost workshop experience\nWhat is AWS Free Tier? AWS Free Tier provides new customers access to AWS services at no charge for a limited time. It includes three types of offerings:\n1. 12 Months Free Available for 12 months following your AWS sign-up date:\n Amazon DynamoDB: 25 GB storage, 25 WCU, 25 RCU AWS Lambda: 1 million requests per month Amazon CloudWatch: 10 metrics, 10 alarms, 1 million API requests  2. Always Free Available to all AWS customers indefinitely:\n DynamoDB: 25 GB storage (always free) Lambda: 1 million requests, 400,000 GB-seconds compute time CloudWatch: 10 custom metrics and 10 alarms  3. Trials Short-term free access to certain services\nFree Tier Limits for This Workshop Service Usage Breakdown: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Service â”‚ Free Tier â”‚ Our Usage â”‚ Safety % â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ DynamoDB RCU â”‚ 25 units â”‚ 15 units â”‚ 60% â”‚ â”‚ DynamoDB WCU â”‚ 25 units â”‚ 15 units â”‚ 60% â”‚ â”‚ DynamoDB Storageâ”‚ 25 GB â”‚ \u0026lt;1 GB â”‚ 4% â”‚ â”‚ Lambda Requests â”‚ 1M/month â”‚ ~100/day â”‚ 0.3% â”‚ â”‚ Lambda Duration â”‚ 400K GB-sec â”‚ \u0026lt;1K GB-sec â”‚ 0.25% â”‚ â”‚ CloudWatch â”‚ 10 metrics â”‚ 6 metrics â”‚ 60% â”‚ â”‚ Data Transfer â”‚ 1 GB â”‚ \u0026lt;100 MB â”‚ 10% â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Overall Free Tier Usage: ~50% = 100% SAFE! ğŸ›¡ï¸ \rCost Protection: We\u0026rsquo;re using only 50% of available Free Tier limits, ensuring zero charges throughout the workshop.\n\rRegional Considerations Primary Region: US East (N. Virginia)  Why chosen: Highest Free Tier allowances DynamoDB: Full 25 RCU/WCU available Lambda: Full 1M requests available Best for: Primary workloads and testing  Secondary Region: EU West (Ireland)  Purpose: Global Tables replication Free Tier: Same limits as primary region Usage: Minimal (replica table only) Cost impact: Near zero  Monitoring Free Tier Usage AWS Billing Dashboard  Navigate to: AWS Console â†’ Billing \u0026amp; Cost Management Free Tier usage: Track current consumption Alerts: Set up when approaching 80% of limits Forecasting: Predict monthly usage  CloudWatch Billing Alarms Automatic alerts when:\n DynamoDB consumed units \u0026gt; 20 (80% of limit) Lambda invocations \u0026gt; 800K/month (80% of limit) Overall estimated charges \u0026gt; $1.00  Free Tier Best Practices 1. Monitor Regularly  Check Free Tier dashboard daily during workshop Set up billing alerts before deployment Monitor resource utilization in CloudWatch  2. Use Appropriate Capacity DynamoDB Provisioned Capacity: - Read Capacity Units (RCU): 5 (well under 25 limit) - Write Capacity Units (WCU): 5 (well under 25 limit) - On-Demand: NOT recommended (can exceed Free Tier) 3. Clean Up Resources  Delete test data regularly Remove unused Lambda functions Clean up CloudWatch logs older than 7 days  Common Free Tier Pitfalls to Avoid Avoid These Mistakes: 1. On-Demand DynamoDB: Can quickly exceed Free Tier 2. Multiple Regions: Deploying same resources in \u0026gt;2 regions 3. Large Data Sets: Uploading \u0026gt;25GB to DynamoDB 4. Forgot Cleanup: Leaving resources running beyond workshop\n\rMistake #1: Provisioned vs On-Demand âŒ WRONG: DynamoDB On-Demand - Pay per request (can be expensive) - Unpredictable costs - No Free Tier protection âœ… CORRECT: DynamoDB Provisioned - Fixed capacity units - Predictable costs - Protected by Free Tier limits Mistake #2: Resource Multiplication âŒ WRONG: Deploy to 5 regions - 5x resource consumption - 5x Free Tier usage - Likely to exceed limits âœ… CORRECT: Deploy to 2 regions max - Minimal resource usage - Well within Free Tier - Global availability maintained Emergency Procedures If You See Charges Appearing  STOP IMMEDIATELY: Pause all workshop activities Check Billing Dashboard: Identify source of charges Review Resources: List all active resources Contact Support: Use AWS Free Tier support if needed Delete Resources: Remove anything outside workshop scope  Quick Resource Audit Commands # List all DynamoDB tables aws dynamodb list-tables --region us-east-1 # List all Lambda functions  aws lambda list-functions --region us-east-1 # Check CloudFormation stacks aws cloudformation list-stacks --region us-east-1 Pre-Workshop Checklist Before starting the infrastructure deployment:\n AWS account created and verified Free Tier eligibility confirmed (account \u0026lt;12 months old) Billing alerts configured Free Tier dashboard bookmarked Emergency contact information ready Understanding of resource limits confirmed  Ready to Continue: Once you\u0026rsquo;ve reviewed the Free Tier limits and understand the safety measures, proceed to the Architecture Overview to understand what we\u0026rsquo;ll be building.\n\r"
},
{
	"uri": "http://localhost:1313/1-setup-infrastructure/1.2-architecture-overview/",
	"title": "1.2 Architecture Overview",
	"tags": [],
	"description": "",
	"content": "Architecture Overview ğŸ—ï¸ Understanding the complete infrastructure we\u0026rsquo;ll deploy for the DynamoDB workshop\nHigh-Level Architecture Our workshop infrastructure spans two AWS regions to demonstrate Global Tables functionality while maintaining Free Tier compliance.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ GLOBAL ARCHITECTURE â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ US-EAST-1 â”‚ EU-WEST-1 â”‚ â”‚ (Primary) â”‚ (Secondary) â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ DynamoDB â”‚ â”‚ â”‚ DynamoDB â”‚ â”‚ â”‚ â”‚ Global Table â”‚â—„â”€â”€â”¼â”€â”€â–ºâ”‚ Replica Table â”‚ â”‚ â”‚ â”‚ - Users â”‚ â”‚ â”‚ - Read Replicas â”‚ â”‚ â”‚ â”‚ - Products â”‚ â”‚ â”‚ - Cross-region sync â”‚ â”‚ â”‚ â”‚ - Orders â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ DynamoDB â”‚ â”‚ â”‚ â”‚ Streams â”‚ â”‚ â”‚ â–¼ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ Lambda â”‚ â”‚ â”‚ â”‚ â”‚ Stream Processor â”‚ â”‚ â”‚ â”‚ â”‚ - Real-time updates â”‚ â”‚ â”‚ â”‚ â”‚ - Data validation â”‚ â”‚ â”‚ â”‚ â”‚ - Audit logging â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â–¼ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ CloudWatch â”‚ â”‚ â”‚ â”‚ â”‚ - Metrics Dashboard â”‚ â”‚ â”‚ â”‚ â”‚ - Billing Alarms â”‚ â”‚ â”‚ â”‚ â”‚ - Performance Logs â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Core Components 1. DynamoDB Global Table Primary Table (US-East-1):\n Table Name: demo-ecommerce-freetier Partition Key: PK (String) Sort Key: SK (String) Capacity: 5 RCU / 5 WCU (Provisioned) Streams: Enabled (NEW_AND_OLD_IMAGES) Point-in-time Recovery: Enabled  Replica Table (EU-West-1):\n Synchronized: Automatic replication Read Capacity: 5 RCU Eventual Consistency: Cross-region Local Reads: Low latency for EU users  2. Lambda Stream Processor Function Configuration:\nRuntime: Python 3.9 Memory: 128 MB (Free Tier optimized) Timeout: 30 seconds Environment: Demo Trigger: DynamoDB Streams Processing Logic:\n Stream Records: Process INSERT, MODIFY, REMOVE events Data Validation: Ensure data integrity Audit Logging: Track all changes Error Handling: Dead letter queue for failed records  3. CloudWatch Monitoring Dashboard Components:\n DynamoDB Metrics: Read/Write capacity utilization Lambda Metrics: Invocation count, duration, errors Cost Tracking: Real-time Free Tier usage Performance: Latency and throughput metrics  Billing Alarms:\n Alert at $1.00: Early warning system Alert at 80% Free Tier: Usage monitoring Email Notifications: Immediate awareness  Data Model Architecture Single Table Design Pattern Our e-commerce platform uses a single DynamoDB table with multiple entity types:\nEntity Types and Access Patterns: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Entity Type â”‚ Partition Key â”‚ Sort Key â”‚ Purpose â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ User Profile â”‚ USER#id â”‚ PROFILE â”‚ User metadata â”‚ â”‚ User Addresses â”‚ USER#id â”‚ ADDRESS#id â”‚ Shipping info â”‚ â”‚ Product â”‚ PRODUCT#id â”‚ DETAILS â”‚ Product catalog â”‚ â”‚ Product Reviews â”‚ PRODUCT#id â”‚ REVIEW#user_id â”‚ Customer reviewsâ”‚ â”‚ Order Header â”‚ ORDER#id â”‚ DETAILS â”‚ Order metadata â”‚ â”‚ Order Items â”‚ ORDER#id â”‚ ITEM#product_id â”‚ Order contents â”‚ â”‚ Category â”‚ CATEGORY#id â”‚ DETAILS â”‚ Product groups â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Access Patterns  Get User Profile: PK=USER#123, SK=PROFILE Get User\u0026rsquo;s Orders: PK=USER#123, SK begins_with ORDER# Get Product Details: PK=PRODUCT#456, SK=DETAILS Get Order with Items: PK=ORDER#789, SK begins_with ITEM# Get Product Reviews: PK=PRODUCT#456, SK begins_with REVIEW#  Security Architecture IAM Roles and Policies Lambda Execution Role:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:DescribeStream\u0026#34;, \u0026#34;dynamodb:GetRecords\u0026#34;, \u0026#34;dynamodb:GetShardIterator\u0026#34;, \u0026#34;dynamodb:ListStreams\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:*:*:table/demo-ecommerce-freetier/stream/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; } ] } Principle of Least Privilege:\n Lambda: Only stream read permissions CloudWatch: Only metrics and logs write DynamoDB: Table-specific access only  Network Architecture Regional Deployment Strategy Primary Region (US-East-1):\n Availability: 99.99% SLA Latency: \u0026lt;10ms for US users Capacity: Full read/write operations Backup: Point-in-time recovery  Secondary Region (EU-West-1):\n Availability: 99.99% SLA (independent) Latency: \u0026lt;10ms for EU users Capacity: Read optimized Sync: Eventually consistent replication  Data Flow  Write Operations: Always to primary region Read Operations: Can be from either region Replication: Automatic cross-region sync Conflict Resolution: Last-writer-wins Failover: Manual promotion if needed  Cost Architecture Free Tier Optimization DynamoDB Costs:\nPrimary Table (US-East-1): - Provisioned RCU: 5 units (Free: 25) = $0.00 - Provisioned WCU: 5 units (Free: 25) = $0.00 - Storage: \u0026lt;1 GB (Free: 25 GB) = $0.00 Replica Table (EU-West-1): - Provisioned RCU: 5 units (Free: 25) = $0.00 - Cross-region replication: \u0026lt;1 GB/month = $0.00 Lambda Costs:\nStream Processor: - Invocations: ~100/day (Free: 1M/month) = $0.00 - Duration: 128MB Ã— 1s Ã— 100 = \u0026lt;1 GB-second = $0.00 Total Workshop Cost: $0.00 âœ…\nScalability Considerations Horizontal Scaling DynamoDB:\n Auto Scaling: Can enable if needed On-Demand: Switch from provisioned Global Secondary Indexes: Add for new access patterns  Lambda:\n Concurrent Executions: Up to 1000 default Dead Letter Queue: Handle failures Reserved Concurrency: Control scaling  Performance Optimization Read Performance:\n Consistent Reads: When data consistency required Eventually Consistent: For better performance DAX: DynamoDB Accelerator for caching  Write Performance:\n Batch Operations: Reduce API calls Parallel Writes: Multiple partition keys Write Sharding: Distribute hot partitions  Monitoring and Observability Key Metrics to Monitor DynamoDB:\n Consumed Read Capacity: Target \u0026lt;80% of provisioned Consumed Write Capacity: Target \u0026lt;80% of provisioned Throttled Requests: Should be 0 Error Rate: Target \u0026lt;1%  Lambda:\n Invocation Count: Track processing volume Duration: Monitor performance trends Error Rate: Target \u0026lt;1% Dead Letter Queue: Monitor failed records  Alerting Strategy Critical Alerts:\n DynamoDB throttling events Lambda function errors Billing threshold exceeded Free Tier limit approaching  Warning Alerts:\n High capacity utilization (\u0026gt;70%) Increased error rates Performance degradation  Architecture Ready: This architecture provides a production-ready foundation for learning DynamoDB advanced patterns while staying within AWS Free Tier limits.\n\rNext Steps With the architecture understanding complete, we\u0026rsquo;ll now proceed to deploy this infrastructure using CloudFormation templates in the next section.\n"
},
{
	"uri": "http://localhost:1313/1-setup-infrastructure/1.3-cloudformation-deployment/",
	"title": "1.3 CloudFormation Deployment",
	"tags": [],
	"description": "",
	"content": "CloudFormation Deployment ğŸš€ Step-by-step guide to deploy AWS infrastructure using Infrastructure as Code\nOverview In this section, you\u0026rsquo;ll deploy the complete DynamoDB workshop infrastructure using AWS CloudFormation. This approach ensures consistent, reproducible deployments and follows AWS best practices.\nPrerequisites Before starting deployment, ensure:\n AWS Console access with administrative permissions Verified region: US East (N. Virginia) us-east-1 Current billing status: $0.00 CloudFormation template downloaded  CloudFormation Template Overview Our template creates the following resources:\nCore Infrastructure  DynamoDB Table: Single table with Global Tables enabled Lambda Function: Stream processor for real-time events IAM Roles: Least privilege access policies DynamoDB Streams: Change data capture configuration  Monitoring \u0026amp; Alerting  CloudWatch Dashboard: Real-time metrics visualization CloudWatch Alarms: Threshold-based alerting Billing Alerts: Cost protection mechanisms  Security Configuration  IAM Policies: Fine-grained permissions Resource Encryption: Data protection at rest VPC Integration: Network isolation (optional)  Step-by-Step Deployment Step 1: Access CloudFormation Service   Navigate to CloudFormation\n Open AWS Management Console Search for \u0026ldquo;CloudFormation\u0026rdquo; or find it in services menu Ensure you\u0026rsquo;re in US East (N. Virginia) region    Create New Stack\n Click \u0026ldquo;Create stack\u0026rdquo; button Select \u0026ldquo;With new resources (standard)\u0026rdquo;    Step 2: Upload Template   Choose Template Source\n Select \u0026ldquo;Upload a template file\u0026rdquo; Click \u0026ldquo;Choose file\u0026rdquo; button Select the infrastructure.yaml file    Validate Template\n CloudFormation will automatically validate syntax Review template details if needed Click \u0026ldquo;Next\u0026rdquo; to proceed    Step 3: Configure Stack Parameters Stack Details:\nStack name: demo-dynamodb-freetier\rDescription: DynamoDB Advanced Patterns Workshop Infrastructure\rParameters:\nEnvironment: demo\rPrimaryRegion: us-east-1\rSecondaryRegion: eu-west-1\rTableName: demo-ecommerce-freetier\rResource Configuration:\nReadCapacityUnits: 5\rWriteCapacityUnits: 5\rStreamViewType: NEW_AND_OLD_IMAGES\rStep 4: Configure Stack Options Tags (Optional):\nWorkshop: DynamoDB-Advanced-Patterns\rEnvironment: Demo\rCostCenter: Learning\rPermissions:\n Use existing service role (if available) Or allow CloudFormation to create new role  Advanced Options:\n Keep all defaults Rollback on failure: Enabled Stack creation timeout: 10 minutes  Step 5: Review and Deploy   Review Configuration\n Verify all parameters are correct Check resource list matches expectations Confirm estimated costs ($0.00 for Free Tier)    Acknowledge Capabilities\n âœ… Check: \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources\u0026rdquo; âœ… Check: \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources with custom names\u0026rdquo;    Create Stack\n Click \u0026ldquo;Create stack\u0026rdquo; button Deployment begins immediately    Monitoring Deployment Stack Events Tab Monitor real-time deployment progress:\nCREATE_IN_PROGRESS â”‚ AWS::CloudFormation::Stack â”‚ demo-dynamodb-freetier\rCREATE_IN_PROGRESS â”‚ AWS::IAM::Role â”‚ LambdaExecutionRole\rCREATE_IN_PROGRESS â”‚ AWS::DynamoDB::Table â”‚ EcommerceTable\rCREATE_IN_PROGRESS â”‚ AWS::Lambda::Function â”‚ StreamProcessor\rCREATE_IN_PROGRESS â”‚ AWS::CloudWatch::Dashboard â”‚ MonitoringDashboard\rCREATE_COMPLETE â”‚ AWS::IAM::Role â”‚ LambdaExecutionRole\rCREATE_COMPLETE â”‚ AWS::DynamoDB::Table â”‚ EcommerceTable\rCREATE_COMPLETE â”‚ AWS::Lambda::Function â”‚ StreamProcessor\rCREATE_COMPLETE â”‚ AWS::CloudWatch::Dashboard â”‚ MonitoringDashboard\rCREATE_COMPLETE â”‚ AWS::CloudFormation::Stack â”‚ demo-dynamodb-freetier\rExpected Timeline  Total Duration: 5-7 minutes IAM Resources: 1-2 minutes DynamoDB Table: 2-3 minutes Lambda Function: 1-2 minutes CloudWatch Components: 1-2 minutes  Troubleshooting Common Issues Issue: Insufficient Permissions Symptoms: CREATE_FAILED for IAM resources Solution:\n Verify account has administrator access Check IAM permissions for CloudFormation Use root account if necessary (for workshop only)  Issue: Resource Limits Exceeded Symptoms: CREATE_FAILED for DynamoDB or Lambda Solution:\n Check Free Tier usage in billing console Verify no existing resources consuming limits Contact AWS support if needed  Issue: Region Mismatch Symptoms: Template validation errors Solution:\n Verify region is us-east-1 Check all parameters are region-appropriate Restart deployment in correct region  Verification Steps After successful deployment:\n Stack Status: CREATE_COMPLETE âœ… Navigate to Outputs tab Record important values:  Table Name Lambda Function ARN Dashboard URL Stream ARN    Deployment Complete! Your DynamoDB Advanced Patterns infrastructure is now running. In the next section, we\u0026rsquo;ll verify all components are working correctly.\n\rNext Steps  Verify DynamoDB table is active Test Lambda function deployment Check CloudWatch dashboard Confirm zero billing charges Begin data modeling exercises  "
},
{
	"uri": "http://localhost:1313/1-setup-infrastructure/1.4-infrastructure-verification/",
	"title": "1.4 Infrastructure Verification",
	"tags": [],
	"description": "",
	"content": "Infrastructure Verification âœ… Comprehensive testing to ensure all AWS resources are properly deployed and functioning\nOverview After CloudFormation deployment, it\u0026rsquo;s critical to verify that all resources are working correctly. This section provides step-by-step verification procedures to ensure your infrastructure is ready for the workshop.\nVerification Checklist Use this checklist to systematically verify each component:\n CloudFormation stack status: CREATE_COMPLETE DynamoDB table: Active and accessible DynamoDB Global Tables: Replication configured Lambda function: Deployed and connected to stream CloudWatch dashboard: Metrics visible IAM roles: Properly configured permissions Billing status: $0.00 charges Free Tier usage: Within limits  Step 1: Verify CloudFormation Stack 1.1 Check Stack Status Navigate to CloudFormation:\n AWS Console â†’ CloudFormation â†’ Stacks Find stack: demo-dynamodb-freetier Status should be: CREATE_COMPLETE âœ…  If status shows anything else:\n CREATE_IN_PROGRESS: Wait for completion CREATE_FAILED: Check Events tab for errors ROLLBACK_COMPLETE: Delete and redeploy  1.2 Review Stack Outputs Click on your stack â†’ Outputs tab:\nRecord these values - you\u0026rsquo;ll use them for further verification.\nStep 2: Verify DynamoDB Table 2.1 Access DynamoDB Console Navigate to DynamoDB:\n AWS Console â†’ Services â†’ DynamoDB Click Tables in left sidebar Find table: demo-ecommerce-freetier  2.2 Check Table Status Table Overview:\nTable Details: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Property â”‚ Expected Value â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ Table Status â”‚ Active âœ… â”‚ â”‚ Partition Key â”‚ PK (String) â”‚ â”‚ Sort Key â”‚ SK (String) â”‚ â”‚ Read Capacity â”‚ 5 (Provisioned) â”‚ â”‚ Write Capacity â”‚ 5 (Provisioned) â”‚ â”‚ Point-in-time Rec. â”‚ Enabled â”‚ â”‚ Streams â”‚ Enabled (NEW_AND_OLD_IMAGES) â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 2.3 Verify Table Configuration Click on table name to view details:\nGeneral Tab:\n Table name: demo-ecommerce-freetier Primary key: PK (String), SK (String) Table status: Active Creation date: Today\u0026rsquo;s date  Capacity Tab:\n Read capacity: 5 units (Provisioned) Write capacity: 5 units (Provisioned) Auto scaling: Disabled (for Free Tier safety)  2.4 Check DynamoDB Streams Exports and streams Tab:\n DynamoDB stream: Enabled âœ… Stream view type: New and old images Stream ARN: Should match CloudFormation output  Step 3: Verify Global Tables Setup 3.1 Check Global Tables Configuration Global Tables Tab:\n Primary region: us-east-1 (US East N. Virginia) Replica regions: eu-west-1 (Europe Ireland) Replication status: Active  3.2 Verify Secondary Region Switch to EU-West-1:\n Change region in AWS Console to \u0026ldquo;Europe (Ireland)\u0026rdquo; Navigate to DynamoDB â†’ Tables Find replica table: demo-ecommerce-freetier Status should be: Active  Replica Table Properties:\n Table status: Active Read capacity: 5 units Global table: Yes (replica) Primary region: us-east-1  Step 4: Verify Lambda Function 4.1 Access Lambda Console Navigate to Lambda:\n Switch back to US-East-1 region AWS Console â†’ Services â†’ Lambda Click Functions in left sidebar Find function: demo-dynamodb-stream-processor  4.2 Check Function Configuration Function Overview:\nLambda Function Details: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Property â”‚ Expected Value â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ Function Name â”‚ demo-dynamodb-stream-processor â”‚ â”‚ Runtime â”‚ Python 3.9 â”‚ â”‚ Memory â”‚ 128 MB â”‚ â”‚ Timeout â”‚ 30 seconds â”‚ â”‚ Handler â”‚ lambda_function.lambda_handler â”‚ â”‚ Last Modified â”‚ Today\u0026#39;s date â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 4.3 Verify Stream Trigger Configuration Tab:\n Triggers: DynamoDB stream should be listed Source: demo-ecommerce-freetier table State: Enabled Batch size: 100 (default)  4.4 Test Function Permissions Permissions Tab:\n Execution role: Should have DynamoDB stream read permissions Resource-based policy: Should be configured automatically  Step 5: Verify CloudWatch Dashboard 5.1 Access CloudWatch Console Navigate to CloudWatch:\n AWS Console â†’ Services â†’ CloudWatch Click Dashboards in left sidebar Find dashboard: demo-dynamodb-freetier-monitoring  5.2 Check Dashboard Widgets Expected Widgets:\nDashboard Layout: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Widget â”‚ Description â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ DynamoDB RCU â”‚ Read Capacity Utilization â”‚ â”‚ DynamoDB WCU â”‚ Write Capacity Utilization â”‚ â”‚ DynamoDB Throttles â”‚ Throttled Read/Write Requests â”‚ â”‚ Lambda Invocations â”‚ Function invocation count â”‚ â”‚ Lambda Errors â”‚ Function error rate â”‚ â”‚ Lambda Duration â”‚ Function execution duration â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 5.3 Verify Metrics Data Initial State (no activity yet):\n DynamoDB metrics: Should show 0 consumed capacity Lambda metrics: Should show 0 invocations All metrics: Should be visible but with no data points yet  Step 6: Verify IAM Roles and Policies 6.1 Check Lambda Execution Role Navigate to IAM:\n AWS Console â†’ Services â†’ IAM Click Roles in left sidebar Find role with name containing: demo-dynamodb-freetier  6.2 Verify Role Permissions Attached Policies:\n AWS managed policy: AWSLambdaBasicExecutionRole Inline policy: Custom DynamoDB stream permissions  Custom Policy Permissions:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:DescribeStream\u0026#34;, \u0026#34;dynamodb:GetRecords\u0026#34;, \u0026#34;dynamodb:GetShardIterator\u0026#34;, \u0026#34;dynamodb:ListStreams\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:us-east-1:*:table/demo-ecommerce-freetier/stream/*\u0026#34; } ] } Step 7: Test Data Operations 7.1 Create Test Item Add sample data to verify table functionality:\n Go to DynamoDB Console â†’ Tables â†’ demo-ecommerce-freetier Click \u0026ldquo;Explore table items\u0026rdquo; Click \u0026ldquo;Create item\u0026rdquo;  Test Item:\n{ \u0026#34;PK\u0026#34;: \u0026#34;USER#test123\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;PROFILE\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Test User\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;test@example.com\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-08-11T10:00:00Z\u0026#34; } 7.2 Verify Item Creation Confirm item appears:\n Item should be visible in table Item count should increase to 1 No errors should appear  7.3 Check Lambda Trigger Verify stream processing:\n Go to Lambda Console Click on stream processor function Check \u0026ldquo;Monitor\u0026rdquo; tab Should see 1 invocation (from the item creation)  Step 8: Cost and Free Tier Verification 8.1 Check Current Billing Navigate to Billing:\n AWS Console â†’ Services â†’ Billing \u0026amp; Cost Management Current charges: Should show $0.00 âœ… Month-to-date: Should show $0.00 âœ…  8.2 Verify Free Tier Usage Free Tier Dashboard:\n DynamoDB: Should show minimal usage Lambda: Should show \u0026lt;10 invocations CloudWatch: Should show active metrics  Usage Breakdown:\nCurrent Free Tier Usage: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Service â”‚ Used â”‚ Available â”‚ % Used â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ DynamoDB RCU â”‚ \u0026lt;1 unit â”‚ 25 units â”‚ \u0026lt;4% â”‚ â”‚ DynamoDB WCU â”‚ \u0026lt;1 unit â”‚ 25 units â”‚ \u0026lt;4% â”‚ â”‚ DynamoDB Storageâ”‚ \u0026lt;0.01 GB â”‚ 25 GB â”‚ \u0026lt;0.1% â”‚ â”‚ Lambda Requests â”‚ 1 request â”‚ 1M requests â”‚ \u0026lt;0.001% â”‚ â”‚ Lambda Duration â”‚ \u0026lt;1 GB-sec â”‚ 400K GB-sec â”‚ \u0026lt;0.001% â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Total Usage: \u0026lt;1% of Free Tier limits âœ… Step 9: Troubleshooting Common Issues 9.1 DynamoDB Table Issues Table not found:\n Check region (must be us-east-1) Verify CloudFormation stack completed Check table name spelling  Table not Active:\n Wait 2-3 minutes for creation Check CloudFormation Events for errors Verify account limits not exceeded  9.2 Lambda Function Issues Function not found:\n Check region Verify CloudFormation outputs Check IAM permissions  No invocations after adding data:\n Verify stream is enabled on table Check trigger configuration Review function logs for errors  9.3 Permission Issues Access denied errors:\n Verify IAM role attached to Lambda Check execution role permissions Ensure resource ARNs match  9.4 Billing Issues Unexpected charges appearing:\n STOP all activities immediately Check Billing dashboard for charge sources Review resource configuration Contact AWS support if needed Consider deleting and redeploying resources  Step 10: Final Verification Summary 10.1 Complete Verification Checklist Infrastructure Status:\n CloudFormation: CREATE_COMPLETE DynamoDB: Active table with streams Global Tables: Replication working Lambda: Function deployed and triggered CloudWatch: Dashboard accessible IAM: Proper permissions configured Billing: $0.00 charges Test data: Successfully created and processed  10.2 Ready for Next Module Infrastructure Health Check:\nğŸŸ¢ All Systems Operational â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Component â”‚ Status â”‚ Health â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ DynamoDB Table â”‚ Active â”‚ ğŸŸ¢ Healthy â”‚ â”‚ Global Tables â”‚ Replicating â”‚ ğŸŸ¢ Healthy â”‚ â”‚ Lambda Function â”‚ Active â”‚ ğŸŸ¢ Healthy â”‚ â”‚ CloudWatch â”‚ Monitoring â”‚ ğŸŸ¢ Healthy â”‚ â”‚ Cost Management â”‚ $0.00 â”‚ ğŸŸ¢ On Track â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ \rInfrastructure Verification Complete: All resources are properly deployed and functioning. You\u0026rsquo;re ready to proceed to Module 2: Single Table Design.\n\rNext Steps With infrastructure successfully verified, you now have:\n Production-ready DynamoDB table with Global Tables Fully functional Lambda stream processor Complete monitoring and alerting setup Zero-cost Free Tier deployment  Ready for: Module 2: Single Table Design\nKeep This Environment: Don\u0026rsquo;t delete these resources yet - we\u0026rsquo;ll use them throughout the workshop for hands-on exercises.\n\r"
},
{
	"uri": "http://localhost:1313/2-single-table-design/2.1-design-principles/",
	"title": "2.1 Design Principles",
	"tags": [],
	"description": "",
	"content": "Single Table Design Principles ğŸ¯ Understanding the core concepts that make Single Table Design powerful\nThe Paradigm Shift From Relational to NoSQL Thinking Traditional relational databases organize data by entities (separate tables for Users, Products, Orders). DynamoDB organizes data by access patterns (how you\u0026rsquo;ll query the data).\nRelational Approach:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Users â”‚ â”‚ Products â”‚ â”‚ Orders â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ user_id â”‚ â”‚ product_id â”‚ â”‚ order_id â”‚ â”‚ name â”‚ â”‚ name â”‚ â”‚ user_id â”‚ â”‚ email â”‚ â”‚ category â”‚ â”‚ status â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Single Table Approach:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ One EcommerceTable â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ PK â”‚ SK â”‚ Entity â”‚ Additional Data â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ USER#user1 â”‚ PROFILE â”‚ User â”‚ name, email, phone â”‚ â”‚ USER#user1 â”‚ ORDER#ord1 â”‚ Order â”‚ status, total, date â”‚ â”‚ PRODUCT#p1 â”‚ DETAILS â”‚ Product â”‚ name, price, stock â”‚ â”‚ ORDER#ord1 â”‚ ITEM#p1 â”‚ OrderItem â”‚ quantity, price â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Core Principles 1. Composite Primary Key Strategy Partition Key (PK) + Sort Key (SK) creates unique item identification and enables relationships:\n PK: Groups related items together (like a namespace) SK: Sorts items within a partition and creates hierarchies Together: Enable 1-to-1, 1-to-many, and many-to-many relationships  2. Entity Namespacing Use prefixes to create logical separation:\n   Entity Type PK Pattern SK Pattern Purpose     User Profile USER#user123 PROFILE Store user information   User Orders USER#user123 ORDER#order456 Link orders to users   Product Details PRODUCT#prod789 DETAILS Store product information   Order Items ORDER#order456 ITEM#prod789 Link products to orders    3. Access Pattern First Design Start with questions, then design the key structure:\n \u0026ldquo;How will I get user profile?\u0026rdquo; â†’ PK=USER#id, SK=PROFILE \u0026ldquo;How will I get user\u0026rsquo;s orders?\u0026rdquo; â†’ PK=USER#id, SK begins_with ORDER# \u0026ldquo;How will I get order details?\u0026rdquo; â†’ PK=ORDER#id, SK begins_with ITEM# \u0026ldquo;How will I get products by category?\u0026rdquo; â†’ Use GSI with CATEGORY# keys  Global Secondary Index (GSI) Strategy When to Use GSIs Use GSIs when you need to query data by attributes other than the primary key:\n Different grouping: Products by category instead of by product ID Different sorting: Orders by status instead of by user Cross-entity queries: All pending orders across all users  GSI Key Design GSI1 - Category-based queries:\nGSI1PK: CATEGORY#electronics GSI1SK: PRODUCT#prod1 GSI1PK: CATEGORY#electronics GSI1SK: PRODUCT#prod2 GSI1PK: CATEGORY#books GSI1SK: PRODUCT#prod3 GSI2 - Status/Price-based queries:\nGSI2PK: STATUS#pending GSI2SK: ORDER#order1 GSI2PK: STATUS#shipped GSI2SK: ORDER#order2 GSI2PK: PRICE#100-500 GSI2SK: PRODUCT#prod1 Benefits in Practice Performance Benefits  Single Query: Get user profile + all orders in one query Predictable Latency: Single-digit millisecond response times No JOINs: All related data retrieved together Efficient Scaling: Consistent performance at any scale  Cost Benefits  Fewer Tables: Lower DynamoDB table costs Fewer Operations: Batch queries instead of multiple calls Optimized Capacity: Better utilization of provisioned capacity Reduced Data Transfer: Less network overhead  Operational Benefits  Atomic Transactions: Update related items together Simplified Backup: One table to backup/restore Easier Monitoring: Single table metrics to track Consistent Security: One set of IAM policies  Key Design Patterns 1. Adjacency List Pattern Store related items next to each other:\nPK=USER#user1, SK=PROFILE (User details) PK=USER#user1, SK=ORDER#order1 (Order 1) PK=USER#user1, SK=ORDER#order2 (Order 2) 2. Hierarchical Data Pattern Use sort key to represent hierarchy:\nPK=ORDER#order1, SK=DETAILS (Order header) PK=ORDER#order1, SK=ITEM#prod1 (Order item 1) PK=ORDER#order1, SK=ITEM#prod2 (Order item 2) 3. GSI Overloading Pattern Use same GSI for multiple query patterns:\nGSI1PK=CATEGORY#electronics, GSI1SK=PRODUCT#prod1 GSI1PK=USER#user1@email.com, GSI1SK=PROFILE \rDesign Rule: Always start with your access patterns, then design your key structure. Don\u0026rsquo;t start with entities!\n\rCommon Anti-Patterns to Avoid âŒ Don\u0026rsquo;t Use Scan Operations  Wrong: Scan entire table to find items Right: Use Query with proper key structure  âŒ Don\u0026rsquo;t Create Too Many GSIs  Wrong: One GSI per query pattern Right: Overload GSIs for multiple patterns  âŒ Don\u0026rsquo;t Ignore Hot Partitions  Wrong: All items have same partition key Right: Distribute items across multiple partitions  âŒ Don\u0026rsquo;t Use Relational Patterns  Wrong: Normalize data across multiple items Right: Denormalize related data together  Remember: Single Table Design requires a mindset shift. Think in terms of access patterns, not entity relationships!\n\rReady for Implementation Now that you understand the principles, let\u0026rsquo;s move to the AWS Console to see these concepts in action. In the next section, we\u0026rsquo;ll navigate the DynamoDB Console and start creating our e-commerce data model.\n"
},
{
	"uri": "http://localhost:1313/2-single-table-design/2.2-console-navigation/",
	"title": "2.2 Console Navigation",
	"tags": [],
	"description": "",
	"content": "DynamoDB Console Navigation ğŸ–¥ï¸ Quick guide to navigate DynamoDB Console for Single Table Design\nAccessing Your Table Step 1: Navigate to DynamoDB Service  Open AWS Console: https://console.aws.amazon.com Verify Region: Ensure you\u0026rsquo;re in US East (N. Virginia) Access DynamoDB: Services â†’ Database â†’ DynamoDB  Screenshot Location: Add screenshot of AWS Console homepage with DynamoDB service highlighted\n\rStep 2: Find Your Workshop Table  Click \u0026ldquo;Tables\u0026rdquo; in the left sidebar Find table: demo-ecommerce-freetier Verify Status: Should show \u0026ldquo;Active\u0026rdquo; Click table name to enter table details  Screenshot Location: Add screenshot of DynamoDB Tables list showing demo-ecommerce-freetier table\n\rTable Overview Dashboard Understanding the Table Layout When you click on your table, you\u0026rsquo;ll see several tabs:\n   Tab Purpose What You\u0026rsquo;ll Use It For     Overview Table configuration Check status, keys, capacity   Items Data management Create, view, edit items   Metrics Performance data Monitor usage and costs   Indexes GSI management View Global Secondary Indexes   Global tables Multi-region setup Check replication status    Screenshot Location: Add screenshot of table overview page showing all tabs\n\rKey Information to Note Table Configuration:\n Table name: demo-ecommerce-freetier Partition key: PK (String) Sort key: SK (String) Table status: Active Item count: Currently 0 (empty table)  Screenshot Location: Add screenshot of table details showing PK and SK configuration\n\rItems Tab - Your Data Workspace Accessing the Items View  Click \u0026ldquo;Items\u0026rdquo; tab View table structure: Currently empty Note the columns: PK, SK, and any additional attributes  This is where you\u0026rsquo;ll:\n âœ… Create new items (users, products, orders) âœ… View existing data âœ… Edit item attributes âœ… Delete items if needed  Screenshot Location: Add screenshot of empty Items tab showing PK and SK columns\n\rCreating Items Interface To create a new item:\n Click \u0026ldquo;Create item\u0026rdquo; button Choose input method:  Form view: Point-and-click interface JSON view: Direct JSON editing (recommended)   Switch to JSON view for easier data entry  Screenshot Location: Add screenshot of \u0026ldquo;Create item\u0026rdquo; dialog showing Form vs JSON view options\n\rJSON View for Data Entry Understanding JSON Format When creating items, you\u0026rsquo;ll use this JSON structure:\n{ \u0026#34;PK\u0026#34;: \u0026#34;USER#user123\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;PROFILE\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john@example.com\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-08-11T10:00:00Z\u0026#34; } Key points:\n PK and SK: Always required (your composite primary key) Additional attributes: Add as needed for each entity type Data types: Strings, numbers, booleans, lists, maps supported  Screenshot Location: Add screenshot of JSON view in create item dialog with sample data\n\rQuery Interface Accessing Query Functionality To query your table:\n Go to Items tab Click \u0026ldquo;Query\u0026rdquo; button (next to Create item) Choose query type:  Table query: Query main table Index query: Query GSI    Screenshot Location: Add screenshot showing Query button location and query type selection\n\rQuery Parameters For table queries, you\u0026rsquo;ll specify:\n Partition key (PK): Exact value (e.g., USER#user123) Sort key (SK): Optional conditions:  Exact match: PROFILE Begins with: ORDER# Between: Range queries    Screenshot Location: Add screenshot of query dialog showing PK and SK input fields\n\rGlobal Secondary Index (GSI) Navigation Viewing GSI Configuration  Click \u0026ldquo;Indexes\u0026rdquo; tab View GSI1: Used for category-based queries View GSI2: Used for status/price-based queries  GSI Structure:\n GSI1: GSI1PK (Partition) + GSI1SK (Sort) GSI2: GSI2PK (Partition) + GSI2SK (Sort)  Screenshot Location: Add screenshot of Indexes tab showing GSI1 and GSI2 configuration\n\rQuerying GSIs To query a GSI:\n Click \u0026ldquo;Query\u0026rdquo; in Items tab Select \u0026ldquo;Query (index)\u0026quot; Choose GSI: GSI1 or GSI2 Enter GSI key values  Screenshot Location: Add screenshot of GSI query interface with index selection dropdown\n\rMetrics and Monitoring Checking Usage and Performance  Click \u0026ldquo;Metrics\u0026rdquo; tab Monitor key metrics:  Consumed read/write capacity Throttled requests (should be 0) Item count (increases as you add data)    Why this matters:\n âœ… Stay within Free Tier limits âœ… Monitor performance âœ… Detect any issues early  Screenshot Location: Add screenshot of Metrics tab showing capacity utilization graphs\n\rQuick Actions Reference Common Console Actions    Action Location Purpose     Create Item Items tab â†’ Create item Add new data   Query Table Items tab â†’ Query Search by PK/SK   Query GSI Items tab â†’ Query (index) Search by GSI keys   View Metrics Metrics tab Monitor performance   Check Capacity Overview tab Verify provisioned capacity    Console Tips Efficiency Tips  Use JSON view for faster item creation Copy/paste item structures for consistency Use Query, not Scan for better performance Check metrics regularly to monitor usage  Navigation Shortcuts  Tables list: DynamoDB home â†’ Tables Quick table access: Bookmark your table URL Region switching: Use region selector in top-right Service search: Use Ctrl+K for quick service access  Pro Tip: Keep the DynamoDB console open in a separate browser tab during the workshop for quick access between exercises.\n\rReady for Data Creation Now that you\u0026rsquo;re familiar with the console interface, you\u0026rsquo;re ready to start creating your e-commerce data model. In the next section, we\u0026rsquo;ll create users, products, and orders using the patterns you\u0026rsquo;ve learned.\nBefore Starting: Make sure you\u0026rsquo;re in the correct table (demo-ecommerce-freetier) and understand the difference between Query and Scan operations.\n\r"
},
{
	"uri": "http://localhost:1313/2-single-table-design/2.3-create-data-items/",
	"title": "2.3 Create Data Items",
	"tags": [],
	"description": "",
	"content": "Create Data Items ğŸ“ Step-by-step guide to create e-commerce data using Single Table Design patterns\nOverview In this section, you\u0026rsquo;ll create the core entities for our e-commerce platform: Users, Products, Orders, and Order Items. Each entity type follows specific key patterns to enable efficient querying.\nEntity Types We\u0026rsquo;ll Create    Entity PK Pattern SK Pattern Purpose     User Profile USER#userID PROFILE Store customer information   Product PRODUCT#productID DETAILS Store product catalog   Order USER#userID ORDER#orderID Link orders to customers   Order Item ORDER#orderID ITEM#productID Link products to orders    Step 1: Create User Profile Access Item Creation  Navigate to: DynamoDB â†’ Tables â†’ demo-ecommerce-freetier Click: \u0026ldquo;Items\u0026rdquo; tab Click: \u0026ldquo;Create item\u0026rdquo; button Switch to: JSON view  Screenshot Location: Add screenshot of Items tab with \u0026ldquo;Create item\u0026rdquo; button highlighted\n\rUser Profile JSON Template Copy and paste this template, then modify with your details:\n{ \u0026#34;PK\u0026#34;: \u0026#34;USER#user001\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;PROFILE\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;USER#john.doe@email.com\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;PROFILE\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;user001\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john.doe@email.com\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;+1-555-0123\u0026#34;, \u0026#34;address\u0026#34;: { \u0026#34;street\u0026#34;: \u0026#34;123 Main St\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Seattle\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;WA\u0026#34;, \u0026#34;zip\u0026#34;: \u0026#34;98101\u0026#34; }, \u0026#34;created_at\u0026#34;: \u0026#34;2025-08-11T10:00:00Z\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34; } Key Patterns Explained:\n PK: USER#user001 - Groups all user data together SK: PROFILE - Identifies this as the user\u0026rsquo;s profile record GSI1PK: USER#john.doe@email.com - Enables email-based lookups GSI1SK: PROFILE - Maintains consistency in GSI  Screenshot Location: Add screenshot of JSON editor with user profile data entered\n\rSave Your User  Review the JSON for syntax errors Click \u0026ldquo;Create item\u0026rdquo; Verify creation: Item should appear in the table view  Screenshot Location: Add screenshot showing successfully created user item in table view\n\rStep 2: Create Products Electronics Product Example Create your first product - customize the values:\n{ \u0026#34;PK\u0026#34;: \u0026#34;PRODUCT#laptop001\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;DETAILS\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;CATEGORY#electronics\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;PRODUCT#laptop001\u0026#34;, \u0026#34;GSI2PK\u0026#34;: \u0026#34;PRICE#500-1000\u0026#34;, \u0026#34;GSI2SK\u0026#34;: \u0026#34;PRODUCT#laptop001\u0026#34;, \u0026#34;product_id\u0026#34;: \u0026#34;laptop001\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Professional Laptop\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;High-performance laptop for professionals\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;electronics\u0026#34;, \u0026#34;price\u0026#34;: 799, \u0026#34;stock\u0026#34;: 25, \u0026#34;brand\u0026#34;: \u0026#34;TechCorp\u0026#34;, \u0026#34;specifications\u0026#34;: { \u0026#34;processor\u0026#34;: \u0026#34;Intel i7\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;16GB RAM\u0026#34;, \u0026#34;storage\u0026#34;: \u0026#34;512GB SSD\u0026#34; }, \u0026#34;created_at\u0026#34;: \u0026#34;2025-08-11T10:00:00Z\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34; } Key Patterns Explained:\n GSI1PK: CATEGORY#electronics - Enables category-based queries GSI2PK: PRICE#500-1000 - Enables price range queries Nested attributes: Store complex product details  Screenshot Location: Add screenshot of product creation in JSON view\n\rBooks Product Example Create a second product in a different category:\n{ \u0026#34;PK\u0026#34;: \u0026#34;PRODUCT#book001\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;DETAILS\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;CATEGORY#books\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;PRODUCT#book001\u0026#34;, \u0026#34;GSI2PK\u0026#34;: \u0026#34;PRICE#10-50\u0026#34;, \u0026#34;GSI2SK\u0026#34;: \u0026#34;PRODUCT#book001\u0026#34;, \u0026#34;product_id\u0026#34;: \u0026#34;book001\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;DynamoDB Patterns Guide\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Complete guide to DynamoDB design patterns\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;books\u0026#34;, \u0026#34;price\u0026#34;: 29, \u0026#34;stock\u0026#34;: 100, \u0026#34;author\u0026#34;: \u0026#34;Database Expert\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;978-1234567890\u0026#34;, \u0026#34;pages\u0026#34;: 350, \u0026#34;created_at\u0026#34;: \u0026#34;2025-08-11T10:00:00Z\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34; } Notice the differences:\n Different category: CATEGORY#books Different price range: PRICE#10-50 Book-specific attributes: author, isbn, pages  Screenshot Location: Add screenshot showing both products created in table view\n\rStep 3: Create Order Order Header Create an order linked to your user:\n{ \u0026#34;PK\u0026#34;: \u0026#34;USER#user001\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;ORDER#order001\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;ORDER#order001\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;DETAILS\u0026#34;, \u0026#34;GSI2PK\u0026#34;: \u0026#34;STATUS#pending\u0026#34;, \u0026#34;GSI2SK\u0026#34;: \u0026#34;ORDER#order001\u0026#34;, \u0026#34;order_id\u0026#34;: \u0026#34;order001\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;user001\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;total_amount\u0026#34;: 828, \u0026#34;tax_amount\u0026#34;: 66.24, \u0026#34;shipping_cost\u0026#34;: 0, \u0026#34;shipping_address\u0026#34;: { \u0026#34;street\u0026#34;: \u0026#34;123 Main St\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Seattle\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;WA\u0026#34;, \u0026#34;zip\u0026#34;: \u0026#34;98101\u0026#34; }, \u0026#34;payment_method\u0026#34;: \u0026#34;credit_card\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-08-11T11:00:00Z\u0026#34;, \u0026#34;updated_at\u0026#34;: \u0026#34;2025-08-11T11:00:00Z\u0026#34; } Key Patterns Explained:\n PK: USER#user001 - Links order to user (enables \u0026ldquo;get user\u0026rsquo;s orders\u0026rdquo;) SK: ORDER#order001 - Identifies this as an order record GSI1PK: ORDER#order001 - Enables direct order lookup GSI2PK: STATUS#pending - Enables status-based queries  Screenshot Location: Add screenshot of order creation showing the relationship structure\n\rStep 4: Create Order Items First Order Item Add the laptop to the order:\n{ \u0026#34;PK\u0026#34;: \u0026#34;ORDER#order001\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;ITEM#laptop001\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;PRODUCT#laptop001\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;ORDER#order001\u0026#34;, \u0026#34;order_id\u0026#34;: \u0026#34;order001\u0026#34;, \u0026#34;product_id\u0026#34;: \u0026#34;laptop001\u0026#34;, \u0026#34;product_name\u0026#34;: \u0026#34;Professional Laptop\u0026#34;, \u0026#34;quantity\u0026#34;: 1, \u0026#34;unit_price\u0026#34;: 799, \u0026#34;total_price\u0026#34;: 799, \u0026#34;added_at\u0026#34;: \u0026#34;2025-08-11T11:00:00Z\u0026#34; } Second Order Item Add the book to the same order:\n{ \u0026#34;PK\u0026#34;: \u0026#34;ORDER#order001\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;ITEM#book001\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;PRODUCT#book001\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;ORDER#order001\u0026#34;, \u0026#34;order_id\u0026#34;: \u0026#34;order001\u0026#34;, \u0026#34;product_id\u0026#34;: \u0026#34;book001\u0026#34;, \u0026#34;product_name\u0026#34;: \u0026#34;DynamoDB Patterns Guide\u0026#34;, \u0026#34;quantity\u0026#34;: 1, \u0026#34;unit_price\u0026#34;: 29, \u0026#34;total_price\u0026#34;: 29, \u0026#34;added_at\u0026#34;: \u0026#34;2025-08-11T11:00:00Z\u0026#34; } Key Patterns Explained:\n PK: ORDER#order001 - Groups items with their order SK: ITEM#productID - Identifies specific items GSI1: Creates product-to-order relationship  Screenshot Location: Add screenshot showing all order items created under the same order\n\rVerify Your Data Structure Check Table Contents After creating all items, your table should contain:\nItems in Table: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ PK â”‚ SK â”‚ Entity Type â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ USER#user001 â”‚ PROFILE â”‚ User Profile â”‚ â”‚ USER#user001 â”‚ ORDER#order001 â”‚ Order (linked to user) â”‚ â”‚ PRODUCT#laptop1 â”‚ DETAILS â”‚ Product (Electronics) â”‚ â”‚ PRODUCT#book001 â”‚ DETAILS â”‚ Product (Books) â”‚ â”‚ ORDER#order001 â”‚ ITEM#laptop001 â”‚ Order Item (Laptop) â”‚ â”‚ ORDER#order001 â”‚ ITEM#book001 â”‚ Order Item (Book) â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ \rScreenshot Location: Add screenshot of complete table view showing all created items\n\rValidate Relationships Verify these relationships work:\n User â†’ Orders: PK=USER#user001 returns profile + orders Order â†’ Items: PK=ORDER#order001 returns order details + items Category grouping: GSI1 with CATEGORY#electronics returns products Status grouping: GSI2 with STATUS#pending returns orders  Data Creation Tips JSON Best Practices  Use consistent naming: Follow the established patterns Validate JSON syntax: Check for missing commas, brackets Include required attributes: PK, SK, and GSI keys Use meaningful IDs: Make them readable and unique  Common Mistakes to Avoid  âŒ Missing GSI keys: Always populate GSI1PK/GSI1SK and GSI2PK/GSI2SK âŒ Inconsistent patterns: Stick to ENTITY#ID format âŒ Wrong data types: Use strings for keys, appropriate types for values âŒ JSON syntax errors: Missing quotes, commas, or brackets  Important: If you get JSON syntax errors, check your quotation marks, commas, and brackets. The console will highlight syntax issues.\n\rCustomize Your Data Make It Personal Customize these values to make the workshop yours:\n User names and emails: Use your own information Product names: Create products you find interesting Addresses: Use your city/state Prices: Realistic values for your products  Add More Items Consider creating additional:\n More users (friends, family names) More products (different categories) Additional orders Multiple items per order  Ready for Querying Now that you have a complete e-commerce dataset with proper Single Table Design patterns, you\u0026rsquo;re ready to explore different query patterns. In the next section, we\u0026rsquo;ll learn how to efficiently retrieve this data using various query techniques.\nData Creation Complete: You\u0026rsquo;ve successfully implemented Single Table Design with proper entity relationships, composite keys, and GSI patterns!\n\r"
},
{
	"uri": "http://localhost:1313/2-single-table-design/2.4-query-patterns/",
	"title": "2.4 Query Patterns",
	"tags": [],
	"description": "",
	"content": "Query Patterns ğŸ” Learn efficient query techniques for Single Table Design using DynamoDB Console\nOverview Now that you have created your e-commerce data, let\u0026rsquo;s explore the powerful query patterns that make Single Table Design so effective. You\u0026rsquo;ll learn how to retrieve data efficiently using both table queries and Global Secondary Index (GSI) queries.\nQuery vs Scan - Critical Difference Always Use Query (Not Scan)  Query: Fast, efficient, cost-effective (uses primary keys) Scan: Slow, expensive, reads entire table (avoid in production)  Important: Always use Query operations in this workshop. Scan operations are inefficient and can quickly exceed Free Tier limits.\n\rPattern 1: Get User Profile Single Item Lookup Objective: Retrieve a specific user\u0026rsquo;s profile information\nAccess the Query Interface:\n Go to: Items tab in your DynamoDB table Click: \u0026ldquo;Query\u0026rdquo; button (not Scan) Ensure: Table query is selected (not index)  Screenshot Location: Add screenshot of Query button location and table/index selection\n\rConfigure User Profile Query Query Parameters:\n Partition key (PK): USER#user001 Sort key (SK): PROFILE Query condition: Use \u0026ldquo;equals\u0026rdquo; (default)  Expected Result: Single item containing user profile data\nScreenshot Location: Add screenshot of query parameters filled in for user profile lookup\n\rExecute and Verify  Click \u0026ldquo;Run\u0026rdquo; Check results: Should return 1 item Verify data: Profile information should be displayed  Performance: ~1-2ms latency, 1 RCU consumed\nScreenshot Location: Add screenshot of query results showing user profile data\n\rPattern 2: Get User\u0026rsquo;s Orders One-to-Many Relationship Query Objective: Retrieve all orders for a specific user\nQuery Configuration:\n Partition key (PK): USER#user001 Sort key condition: \u0026ldquo;begins_with\u0026rdquo; Sort key value: ORDER#  This pattern retrieves both the user profile AND all their orders in a single query.\nScreenshot Location: Add screenshot showing \u0026ldquo;begins_with\u0026rdquo; sort key condition setup\n\rAdvanced Sort Key Options Available sort key conditions:\n = (equals): Exact match begins_with: Prefix matching between: Range queries \u0026gt;, \u0026gt;=, \u0026lt;, \u0026lt;=: Comparison operators  For this pattern: Use \u0026ldquo;begins_with\u0026rdquo; to get all items where SK starts with \u0026ldquo;ORDER#\u0026rdquo;\nScreenshot Location: Add screenshot of sort key condition dropdown menu\n\rExecute User Orders Query Expected Results:\n User profile (SK = PROFILE) All user orders (SK = ORDER#order001, etc.)  Why this works: All items with PK = USER#user001 are stored together and can be retrieved in one efficient query.\nScreenshot Location: Add screenshot of query results showing profile + orders\n\rPattern 3: Get Order Details with Items Hierarchical Data Query Objective: Get complete order information including all items\nQuery Configuration:\n Partition key (PK): ORDER#order001 Sort key: Leave empty (gets all items in partition)  Expected Results:\n Order details (SK = DETAILS) All order items (SK = ITEM#laptop001, SK = ITEM#book001)  Screenshot Location: Add screenshot of order details query showing order + items\n\rPattern 4: Products by Category (GSI Query) Using Global Secondary Index Objective: Find all products in a specific category\nSwitch to GSI Query:\n Click Query dropdown: Select \u0026ldquo;Query (index)\u0026rdquo; Choose Index: GSI1 Query the GSI: Use GSI key structure  Screenshot Location: Add screenshot showing index selection dropdown with GSI1 highlighted\n\rConfigure Category Query GSI1 Query Parameters:\n GSI1 Partition key: CATEGORY#electronics GSI1 Sort key: Leave empty (gets all products in category)  Why this works: All electronics products have GSI1PK = CATEGORY#electronics\nScreenshot Location: Add screenshot of GSI1 query setup for category search\n\rExecute Category Query Expected Results: All products where category = \u0026ldquo;electronics\u0026rdquo;\nTry additional categories:\n CATEGORY#books CATEGORY#clothing (if you created any)  Screenshot Location: Add screenshot of category query results showing all electronics products\n\rPattern 5: Orders by Status (GSI Query) Status-based Filtering Objective: Find all orders with a specific status\nGSI2 Query Configuration:\n Choose Index: GSI2 GSI2 Partition key: STATUS#pending GSI2 Sort key: Leave empty  Expected Results: All orders with status = \u0026ldquo;pending\u0026rdquo;\nScreenshot Location: Add screenshot of GSI2 query for order status\n\rTry Different Status Values Query other statuses:\n STATUS#shipped STATUS#delivered STATUS#cancelled  Pattern 6: Price Range Queries (GSI Query) Range-based Product Search Objective: Find products within a price range\nGSI2 Query Configuration:\n Choose Index: GSI2 GSI2 Partition key: PRICE#500-1000 GSI2 Sort key: Leave empty  Price Ranges Used in Our Data:\n PRICE#10-50 (books, accessories) PRICE#50-200 (mid-range items) PRICE#200-500 (premium items) PRICE#500-1000 (high-end items)  Screenshot Location: Add screenshot of price range query showing products in range\n\rQuery Performance Analysis Monitor Query Efficiency Check Performance Metrics:\n Go to: Metrics tab Monitor: Consumed read capacity Verify: No throttled requests  Expected Performance:\n Single item queries: ~1-2ms, 1 RCU Multi-item queries: ~3-5ms, 2-5 RCU GSI queries: ~2-4ms, 1-3 RCU  Screenshot Location: Add screenshot of Metrics tab showing query performance data\n\rAdvanced Query Techniques Combining Conditions Sort Key with Multiple Conditions:\n Between dates: SK between 2025-01-01 and 2025-12-31 Greater than: SK \u0026gt; ORDER#order001 Prefix + range: Complex filtering patterns  Query Result Options Additional Query Settings:\n Limit: Maximum number of items to return Scan index forward: Sort order (ascending/descending) Projection expression: Specific attributes to return Filter expression: Additional filtering after query  Screenshot Location: Add screenshot showing advanced query options and settings\n\rQuery Pattern Summary Patterns You\u0026rsquo;ve Mastered    Pattern Query Type Key Structure Use Case     User Profile Table PK + SK exact Get specific user   User Orders Table PK + SK prefix Get user\u0026rsquo;s orders   Order Items Table PK all items Get order details   Category Products GSI1 Category grouping Product catalog   Status Orders GSI2 Status grouping Order management   Price Range GSI2 Price grouping Product search    Query Best Practices  Always use Query: Never use Scan for production workloads Design keys for queries: Think about access patterns first Use GSIs strategically: Enable multiple query patterns Monitor performance: Track consumed capacity and latency Test query patterns: Verify they return expected results  Key Insight: Single Table Design enables all these query patterns with consistent performance and minimal cost. Traditional relational approaches would require multiple queries and JOINs.\n\rQuery Troubleshooting Common Issues No Results Returned:\n âœ… Check partition key spelling âœ… Verify sort key conditions âœ… Ensure data exists with those keys  Unexpected Results:\n âœ… Review sort key conditions (exact vs begins_with) âœ… Check index selection (table vs GSI) âœ… Verify data was created correctly  Performance Issues:\n âœ… Avoid Scan operations âœ… Use specific partition keys âœ… Monitor consumed capacity  Ready for Global Scale You\u0026rsquo;ve now mastered Single Table Design query patterns! In the next module, we\u0026rsquo;ll take this data global by configuring Global Tables for multi-region deployment, enabling users worldwide to access your e-commerce platform with low latency.\nQuery Mastery Achieved: You can now efficiently retrieve data using all major Single Table Design patterns with consistent performance and cost optimization!\n\r"
},
{
	"uri": "http://localhost:1313/3-global-tables-setup/3.1-global-tables-overview/",
	"title": "3.1 Global Tables Overview",
	"tags": [],
	"description": "",
	"content": "Global Tables Overview ğŸŒ Understanding DynamoDB Global Tables architecture and replication mechanisms\nWhat Are Global Tables? Global Tables enable you to create a multi-region, multi-master database that provides local read and write performance for globally distributed applications. Your CloudFormation deployment has already configured this for you.\nArchitecture Components Current Workshop Setup Your infrastructure already includes:\n   Component US-East-1 EU-West-1 Status     DynamoDB Table Primary Replica âœ… Active   Table Name demo-ecommerce-freetier demo-ecommerce-freetier âœ… Synced   DynamoDB Streams Enabled Enabled âœ… Replicating   Replication Bi-directional Bi-directional âœ… Healthy    Replication Process How Data Flows Between Regions Write Operation Flow: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Step 1: User writes to US-EAST-1 â”‚ â”‚ â”œâ”€ Item: USER#john, SK: PROFILE â”‚ â”‚ â”œâ”€ Local write: Immediate success â”‚ â”‚ â””â”€ Stream record: Created â”‚ â”‚ â”‚ â”‚ Step 2: DynamoDB Streams captures change â”‚ â”‚ â”œâ”€ Stream record: NEW_AND_OLD_IMAGES â”‚ â”‚ â”œâ”€ Timestamp: 2025-08-11T15:30:00.123Z â”‚ â”‚ â””â”€ Event: INSERT â”‚ â”‚ â”‚ â”‚ Step 3: Cross-region replication â”‚ â”‚ â”œâ”€ Source: us-east-1 stream â”‚ â”‚ â”œâ”€ Target: eu-west-1 table â”‚ â”‚ â””â”€ Latency: 500ms - 2 seconds â”‚ â”‚ â”‚ â”‚ Step 4: EU-WEST-1 receives update â”‚ â”‚ â”œâ”€ Item appears: USER#john, SK: PROFILE â”‚ â”‚ â”œâ”€ Available for reads: Immediately â”‚ â”‚ â””â”€ Status: Replicated âœ… â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Consistency Model Eventually Consistent Global Tables provides eventual consistency across regions:\n Immediate: Write succeeds immediately in the source region Propagation: Changes replicate to other regions within 0.5-2 seconds Convergence: All regions eventually have identical data Durability: Data is never lost during replication  Conflict Resolution When the same item is modified in multiple regions simultaneously:\nLast Writer Wins strategy:\n Compare timestamps of conflicting updates Keep the later timestamp (more recent change) Overwrite earlier changes in all regions Notify through CloudWatch metrics  Example conflict scenario:\nTime: 15:30:00 - US user updates: name = \u0026#34;John Smith\u0026#34; Time: 15:30:01 - EU user updates: name = \u0026#34;John Doe\u0026#34; Result: All regions will have name = \u0026#34;John Doe\u0026#34; (EU update wins due to later timestamp) Global Tables Benefits Performance Benefits  Local Latency: Sub-10ms response times in each region Global Scale: Serve users worldwide without performance penalty Load Distribution: Traffic distributed across regions  Availability Benefits  Regional Failover: Automatic failover if one region becomes unavailable Disaster Recovery: Built-in DR across geographic regions 99.999% Availability: Higher availability than single-region deployments  Operational Benefits  No Code Changes: Applications work with any region Automatic Scaling: Each region scales independently Unified Management: Single table view across all regions  Key Concepts to Remember Multi-Master Replication  Any region can accept writes All regions can serve reads No single point of failure  Stream-Based Replication  DynamoDB Streams power the replication Ordered delivery ensures consistency Retry logic handles temporary failures  Region Independence  Each region operates independently Network partitions don\u0026rsquo;t affect local operations Cross-region connectivity only needed for replication  Workshop Advantage: Your CloudFormation template has already configured all Global Tables components. You can focus on understanding and testing the functionality!\n\rLimitations to Understand Eventual Consistency Challenges  Temporary inconsistencies possible for 0.5-2 seconds Application design must handle eventual consistency Strong consistency only available within single region  Conflict Resolution Limitations  Last Writer Wins can overwrite changes No custom conflict resolution logic Application-level conflict handling may be needed  Cross-Region Dependencies  Network connectivity required for replication Regional outages can delay replication Cross-region latency affects replication speed  Real-World Use Cases Ideal for Global Tables    Use Case Why It Works Considerations     User Profiles Infrequent updates, read-heavy Handle profile conflicts   Product Catalogs Content distribution, global access Inventory sync challenges   Gaming Leaderboards Global competition, eventual consistency OK Score conflicts possible   IoT Sensor Data Time-series data, append-only High write volume    Challenging Scenarios  Financial transactions (require strong consistency) Inventory management (stock levels need accuracy) Real-time collaboration (immediate consistency needed)  Next Steps Now that you understand Global Tables architecture, let\u0026rsquo;s verify your multi-region setup and see replication in action through the AWS Console.\nReady to Explore: Your Global Tables are already configured and running. Time to see them in action!\n\r"
},
{
	"uri": "http://localhost:1313/3-global-tables-setup/3.2-verify-global-setup/",
	"title": "3.2 Verify Global Setup",
	"tags": [],
	"description": "",
	"content": "Verify Global Setup âœ… Step-by-step verification of your Global Tables configuration through AWS Console\nOverview Your CloudFormation template has automatically configured Global Tables between US-East-1 and EU-West-1. Let\u0026rsquo;s verify everything is working correctly before we start testing replication.\nStep 1: Access Primary Region Navigate to US-East-1  Open AWS Console: Ensure you\u0026rsquo;re logged in Check Region: Top-right corner should show \u0026ldquo;N. Virginia\u0026rdquo; Switch if needed: Click region dropdown â†’ \u0026ldquo;US East (N. Virginia)\u0026rdquo;  Screenshot Location: Add screenshot of AWS Console with region selector showing US East (N. Virginia)\n\rFind Your DynamoDB Table  Services: Navigate to DynamoDB service Tables: Click \u0026ldquo;Tables\u0026rdquo; in left sidebar Locate: Find demo-ecommerce-freetier Status: Verify table shows \u0026ldquo;Active\u0026rdquo;  Screenshot Location: Add screenshot of DynamoDB Tables list showing demo-ecommerce-freetier with Active status\n\rStep 2: Check Global Tables Configuration Access Global Tables Tab  Click: Table name demo-ecommerce-freetier Navigate: Click \u0026ldquo;Global tables\u0026rdquo; tab Review: Global Tables configuration  Screenshot Location: Add screenshot of table overview with Global tables tab highlighted\n\rVerify Global Tables Status Expected Configuration:\n   Region Status Role Health     us-east-1 Active Primary âœ… Healthy   eu-west-1 Active Replica âœ… Healthy    Key indicators to verify:\n Replication Status: \u0026ldquo;Healthy\u0026rdquo; or \u0026ldquo;Active\u0026rdquo; Last Replication: Recent timestamp Pending Updates: Should be 0  Screenshot Location: Add screenshot of Global tables configuration showing both regions with healthy status\n\rStep 3: Verify Secondary Region Switch to EU-West-1  Region Selector: Click region dropdown (top-right) Select: \u0026ldquo;Europe (Ireland)\u0026rdquo; - eu-west-1 Wait: Allow console to switch regions  Screenshot Location: Add screenshot of region selector dropdown with Europe (Ireland) highlighted\n\rCheck Replica Table  Navigate: DynamoDB â†’ Tables Find: Same table name demo-ecommerce-freetier Verify: Table exists and shows \u0026ldquo;Active\u0026rdquo; Check: Global tables tab shows replica status  Expected in EU-West-1:\n Table Name: demo-ecommerce-freetier (identical) Status: Active Role: Replica table Primary Region: us-east-1  Screenshot Location: Add screenshot of EU region showing the replica table with same name\n\rStep 4: Compare Table Schemas Verify Schema Consistency Both regions should have identical table schema:\nPrimary Keys:\n Partition Key: PK (String) Sort Key: SK (String)  Global Secondary Indexes:\n GSI1: GSI1PK (String), GSI1SK (String) GSI2: GSI2PK (String), GSI2SK (String)  Settings:\n Read Capacity: 5 units (provisioned) Write Capacity: 5 units (provisioned) Point-in-time Recovery: Enabled DynamoDB Streams: Enabled  Screenshot Location: Add screenshot comparing table schema between US and EU regions\n\rStep 5: Check Current Data Verify Existing Data Replication If you\u0026rsquo;ve completed Module 2, check that your existing data appears in both regions:\nSwitch to US-East-1:\n Go to: Items tab Count items: Note the number of items  Switch to EU-West-1:\n Go to: Items tab Compare count: Should match US region exactly  If item counts don\u0026rsquo;t match:\n Wait 1-2 minutes for replication Refresh the browser page Check Global Tables health status  Screenshot Location: Add screenshot showing identical item counts in both regions\n\rStep 6: Verify Stream Configuration Check DynamoDB Streams In US-East-1:\n Table Overview: Go to table details Exports and streams: Click tab Stream details: Verify settings  Expected Stream Configuration:\n Stream Status: Enabled Stream view type: New and old images Stream ARN: Should be present Shard count: 1 or more active shards  Screenshot Location: Add screenshot of DynamoDB Streams configuration showing enabled status\n\rStep 7: Health Check Dashboard Monitor Replication Health Access Monitoring:\n Metrics Tab: Click in table view Global Tables metrics: Look for replication metrics Key metrics:  Replication latency Pending replication count Failed replication events    Healthy Indicators:\n Replication Latency: \u0026lt; 2 seconds average Pending Count: 0 or very low Error Rate: 0%  Screenshot Location: Add screenshot of metrics dashboard showing healthy replication metrics\n\rStep 8: Network Connectivity Test Test Cross-Region Communication Simple connectivity verification:\n Create test item in US-East-1 Wait 30 seconds Check EU-West-1 for the item Delete test item from either region  Test Item Example:\n{ \u0026#34;PK\u0026#34;: \u0026#34;TEST#connectivity\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;VERIFICATION\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-08-11T15:00:00Z\u0026#34;, \u0026#34;test_purpose\u0026#34;: \u0026#34;global_tables_verification\u0026#34; } \rScreenshot Location: Add screenshot of test item appearing in both regions\n\rTroubleshooting Common Issues Issue: Table Not Found in EU-West-1 Possible Causes:\n Wrong region selected CloudFormation deployment incomplete Global Tables setup failed  Solutions:\n Double-check region in top-right corner Verify CloudFormation stack completed successfully Check IAM permissions for cross-region access  Issue: Replication Status Unhealthy Check These Items:\n Network connectivity between regions DynamoDB Streams enabled on source table IAM permissions for Global Tables service Table capacity not throttling  Issue: Item Counts Don\u0026rsquo;t Match Troubleshooting Steps:\n Wait longer (up to 2 minutes) Refresh browser page Check for errors in CloudWatch logs Verify no throttling in metrics  Verification Checklist Before proceeding to multi-region operations:\n Both regions accessible through AWS Console Table exists in both us-east-1 and eu-west-1 Global Tables status shows \u0026ldquo;Healthy\u0026rdquo; or \u0026ldquo;Active\u0026rdquo; Schema identical between regions DynamoDB Streams enabled Existing data replicated (if any) Test connectivity working Monitoring metrics available  Verification Complete: Your Global Tables setup is healthy and ready for multi-region operations!\n\rNext Steps With Global Tables verified and healthy, you\u0026rsquo;re ready to experience multi-region operations. In the next section, we\u0026rsquo;ll create data in one region and watch it automatically appear in another!\nPro Tip: Keep both region tabs open in your browser (US-East-1 and EU-West-1) to easily switch between them during exercises.\n\r"
},
{
	"uri": "http://localhost:1313/3-global-tables-setup/3.3-multi-region-operations/",
	"title": "3.3 Multi-Region Operations",
	"tags": [],
	"description": "",
	"content": "Multi-Region Operations ğŸŒ Hands-on practice with cross-region read/write operations and replication testing\nOverview Now that your Global Tables are verified, let\u0026rsquo;s experience multi-region operations firsthand. You\u0026rsquo;ll create data in one region, verify it replicates to another, and test conflict resolution scenarios.\nExercise 1: Write to Primary, Read from Replica Step 1: Create Global User in US-East-1 Ensure you\u0026rsquo;re in US-East-1:\n Check region: Top-right should show \u0026ldquo;N. Virginia\u0026rdquo; Navigate: DynamoDB â†’ Tables â†’ demo-ecommerce-freetier Go to: Items tab Click: \u0026ldquo;Create item\u0026rdquo;  Screenshot Location: Add screenshot of US-East-1 region with Create item dialog open\n\rUser Creation Template Switch to JSON view and create:\n{ \u0026#34;PK\u0026#34;: \u0026#34;USER#global-user-[your-name]\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;PROFILE\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;USER#[your-name]@global.com\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;PROFILE\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;global-user-[your-name]\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[Your Name] Global\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;[your-name]@global.com\u0026#34;, \u0026#34;region_created\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;created_timestamp\u0026#34;: \u0026#34;2025-08-11T15:30:00Z\u0026#34;, \u0026#34;test_type\u0026#34;: \u0026#34;global_replication\u0026#34;, \u0026#34;workshop_session\u0026#34;: \u0026#34;module3\u0026#34; } Important: Replace [your-name] with your actual name to make items unique.\nScreenshot Location: Add screenshot of JSON editor with global user data being entered\n\rStep 2: Note Creation Time Record the details:\n Click \u0026ldquo;Create item\u0026rdquo; Note the time: Record when you clicked create Take screenshot: Of the created item  Screenshot Location: Add screenshot showing successfully created item in US region\n\rStep 3: Switch to EU-West-1 Change regions:\n Region selector: Click dropdown (top-right) Select: \u0026ldquo;Europe (Ireland)\u0026rdquo; Wait: For region switch to complete Navigate: DynamoDB â†’ Tables â†’ demo-ecommerce-freetier  Screenshot Location: Add screenshot of region selector with Europe (Ireland) selected\n\rStep 4: Query for Replicated Data Search for your user:\n Items tab: Navigate to items view Click: \u0026ldquo;Query\u0026rdquo; button Configure query:  Partition key (PK): USER#global-user-[your-name] Sort key (SK): PROFILE   Click: \u0026ldquo;Run\u0026rdquo;  Screenshot Location: Add screenshot of query setup in EU region looking for the US-created user\n\rStep 5: Verify Replication Expected results:\n If immediate: Item appears right away If delayed: Wait 30-60 seconds and try again Replication time: Note how long it took  Verify the data:\n All attributes: Should match exactly region_created: Should still show \u0026ldquo;us-east-1\u0026rdquo; Timestamps: Should be identical  Screenshot Location: Add screenshot showing the replicated item in EU region with identical data\n\rExercise 2: Write from Replica, Read from Primary Step 1: Create Product in EU-West-1 Stay in EU-West-1 and create a product:\n{ \u0026#34;PK\u0026#34;: \u0026#34;PRODUCT#eu-product-[unique-id]\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;DETAILS\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;CATEGORY#eu-electronics\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;PRODUCT#eu-product-[unique-id]\u0026#34;, \u0026#34;GSI2PK\u0026#34;: \u0026#34;PRICE#200-500\u0026#34;, \u0026#34;GSI2SK\u0026#34;: \u0026#34;PRODUCT#eu-product-[unique-id]\u0026#34;, \u0026#34;product_id\u0026#34;: \u0026#34;eu-product-[unique-id]\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;European Smartphone\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Created in EU region\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;eu-electronics\u0026#34;, \u0026#34;price\u0026#34;: 299, \u0026#34;stock\u0026#34;: 50, \u0026#34;region_created\u0026#34;: \u0026#34;eu-west-1\u0026#34;, \u0026#34;created_timestamp\u0026#34;: \u0026#34;2025-08-11T15:35:00Z\u0026#34;, \u0026#34;test_type\u0026#34;: \u0026#34;reverse_replication\u0026#34; } \rScreenshot Location: Add screenshot of product creation in EU region\n\rStep 2: Switch Back to US-East-1 Return to primary region:\n Region selector: \u0026ldquo;US East (N. Virginia)\u0026rdquo; Navigate: DynamoDB â†’ Tables â†’ Items Query for product:  PK: PRODUCT#eu-product-[unique-id] SK: DETAILS    Screenshot Location: Add screenshot of US region query looking for EU-created product\n\rStep 3: Verify Reverse Replication Check the results:\n Product appears: In US region region_created: Still shows \u0026ldquo;eu-west-1\u0026rdquo; All data intact: Exact copy from EU  This demonstrates bi-directional replication - you can write to any region!\nScreenshot Location: Add screenshot showing EU-created product now visible in US region\n\rExercise 3: Conflict Resolution Testing Step 1: Create Base Order In US-East-1, create an order:\n{ \u0026#34;PK\u0026#34;: \u0026#34;USER#global-user-[your-name]\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;ORDER#conflict-test\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;ORDER#conflict-test\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;DETAILS\u0026#34;, \u0026#34;GSI2PK\u0026#34;: \u0026#34;STATUS#pending\u0026#34;, \u0026#34;GSI2SK\u0026#34;: \u0026#34;ORDER#conflict-test\u0026#34;, \u0026#34;order_id\u0026#34;: \u0026#34;conflict-test\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;global-user-[your-name]\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;total_amount\u0026#34;: 100, \u0026#34;last_updated_region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;conflict_test\u0026#34;: true } Wait 2 minutes for replication to complete.\nScreenshot Location: Add screenshot of conflict test order creation in US region\n\rStep 2: Simultaneous Updates (Advanced) If working with a partner:\n Partner A: Update order in US-East-1 Partner B: Update same order in EU-West-1 Both execute: Within 10 seconds of each other  US Update (Partner A):\n{ \u0026#34;total_amount\u0026#34;: 150, \u0026#34;last_updated_region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;update_timestamp\u0026#34;: \u0026#34;2025-08-11T15:40:00Z\u0026#34; } EU Update (Partner B):\n{ \u0026#34;total_amount\u0026#34;: 200, \u0026#34;last_updated_region\u0026#34;: \u0026#34;eu-west-1\u0026#34;, \u0026#34;update_timestamp\u0026#34;: \u0026#34;2025-08-11T15:40:05Z\u0026#34; } \rScreenshot Location: Add screenshot showing edit dialog for conflict testing\n\rStep 3: Observe Conflict Resolution After 2-3 minutes:\n Check both regions: Query the same order Compare results: Which update won? Understand why: Later timestamp wins  Expected outcome: EU update wins because timestamp 15:40:05 \u0026gt; 15:40:00\nScreenshot Location: Add screenshot showing final conflict resolution result in both regions\n\rExercise 4: Query Patterns Across Regions Step 1: Category Query in EU Test GSI queries work across regions:\n Stay in EU-West-1 Query Index: GSI1 GSI1PK: CATEGORY#eu-electronics Run query  Expected: Shows products created in EU region\nScreenshot Location: Add screenshot of GSI category query in EU region\n\rStep 2: Status Query in US Test cross-region status queries:\n Switch to US-East-1 Query Index: GSI2 GSI2PK: STATUS#pending Run query  Expected: Shows orders from both regions with pending status\nScreenshot Location: Add screenshot of GSI status query showing orders from multiple regions\n\rExercise 5: Replication Timing Analysis Step 1: Measure Replication Speed Create timestamped items:\n Record start time: Note exact time before creation Create item: In one region Switch regions: Immediately Query repeatedly: Until item appears Calculate delay: End time - start time  Test Item Template:\n{ \u0026#34;PK\u0026#34;: \u0026#34;TEST#timing-[timestamp]\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;REPLICATION\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;[exact-timestamp]\u0026#34;, \u0026#34;source_region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;test_purpose\u0026#34;: \u0026#34;timing_analysis\u0026#34; } \rScreenshot Location: Add screenshot showing timing test item creation with precise timestamps\n\rStep 2: Document Results Record your findings:\n Fastest replication: ___ seconds Slowest replication: ___ seconds Average time: ___ seconds Consistency: Usually \u0026lt; 2 seconds  Real-World Scenarios Scenario 1: Global User Login Simulate global application:\n User logs in: US region Profile updated: Last login timestamp User travels: EU region App checks: Profile from EU Verify: Recent login time visible  Scenario 2: Inventory Management Product stock updates:\n Product sold: US region (-1 stock) Same product: EU region query Stock level: Eventually consistent Business logic: Handle temporary inconsistency  Scenario 3: Order Processing Multi-region order flow:\n Order created: EU region Payment processed: US region Status updated: EU region Fulfillment: Reads from nearest region  Performance Monitoring Check Replication Metrics During exercises:\n Monitor: CloudWatch metrics Watch: Replication latency Observe: Pending replication count Track: Error rates (should be 0)  Screenshot Location: Add screenshot of CloudWatch metrics showing replication performance during exercises\n\rTroubleshooting Guide Replication Not Working Common issues:\n Wrong region: Double-check region selection Typos in keys: Exact match required for queries Browser cache: Refresh page Wait longer: Up to 2 minutes possible  Queries Returning Empty Checklist:\n Correct PK/SK: Exact string match Region correct: Item exists in queried region GSI populated: GSI keys included in item Query type: Using Query, not Scan  Conflict Resolution Unexpected Understanding:\n Timestamp precision: Millisecond level Clock synchronization: AWS handles timing Application design: Plan for overwrites  Important: If you experience issues, check the Global Tables health status in the console and verify network connectivity.\n\rExercise Summary By completing these exercises, you\u0026rsquo;ve experienced:\n âœ… Cross-region replication in both directions âœ… Eventual consistency timing âœ… Conflict resolution with Last Writer Wins âœ… Query patterns working across regions âœ… Real-world scenarios and timing analysis  Multi-Region Mastery: You now understand how Global Tables enables truly global applications with local performance!\n\rNext Steps With hands-on Global Tables experience complete, let\u0026rsquo;s monitor the replication performance and understand the metrics that help you operate global applications in production.\n"
},
{
	"uri": "http://localhost:1313/3-global-tables-setup/3.4-replication-monitoring/",
	"title": "3.4 Replication Monitoring",
	"tags": [],
	"description": "",
	"content": "CloudWatch Metrics for Global Tables ğŸ” Monitor replication performance and troubleshoot Global Tables issues\nOverview Understanding Global Tables performance is crucial for production applications. You\u0026rsquo;ll learn to monitor replication latency, track errors, and set up alerts for your global application.\nKey Metrics to Monitor Essential metrics for Global Tables health:\n ReplicationLatency: Time for writes to replicate PendingReplicationCount: Unsynced items awaiting replication ReplicationMetrics: Success/failure rates ConsumedReads/Writes: Per-region capacity usage  Exercise 1: View Basic Replication Metrics Step 1: Access CloudWatch Navigate to monitoring:\n AWS Console: Search \u0026ldquo;CloudWatch\u0026rdquo; Left sidebar: Click \u0026ldquo;Metrics\u0026rdquo; Browse metrics: Click \u0026ldquo;Browse\u0026rdquo; tab Find DynamoDB: Click \u0026ldquo;DynamoDB\u0026rdquo; tile  Screenshot Location: Add screenshot of CloudWatch console with DynamoDB metrics selected\n\rStep 2: Locate Global Tables Metrics Find replication metrics:\n Metric categories: Look for \u0026ldquo;Global Table Metrics\u0026rdquo; Table selection: Choose demo-ecommerce-freetier Region pairs: See US-East-1 â†’ EU-West-1 metrics Click checkbox: Select metrics you want to view  Screenshot Location: Add screenshot showing Global Table metrics selection interface\n\rStep 3: Create Replication Dashboard Build monitoring dashboard:\n Select metrics:  ReplicationLatency for both region pairs PendingReplicationCount for both directions   Time range: Set to \u0026ldquo;Last 1 hour\u0026rdquo; Refresh: Set to \u0026ldquo;1 minute\u0026rdquo; Add to dashboard: Click \u0026ldquo;Add to dashboard\u0026rdquo;  Screenshot Location: Add screenshot of CloudWatch dashboard showing replication latency graphs\n\rExercise 2: Generate Load and Monitor Step 1: Batch Create Items Create monitoring load with this script pattern:\n# PowerShell script to create multiple items for ($i=1; $i -le 20; $i++) { # Create item in US-East-1 echo \u0026#34;Creating item $i\u0026#34; # Use AWS CLI or console to create items } Manual approach:\n US-East-1: Create 10 products rapidly EU-West-1: Create 10 users rapidly Both regions: Create orders referencing both Monitor: Watch metrics update  Screenshot Location: Add screenshot showing multiple items being created in rapid succession\n\rStep 2: Watch Replication Latency Observe real-time metrics:\n Refresh CloudWatch: Every 30 seconds Latency spikes: Look for increased replication time Pending count: Should spike then decrease Normal range: Usually \u0026lt; 2 seconds  Expected patterns:\n Initial spike: High pending count Gradual decline: Items replicate Return to baseline: All items synced  Screenshot Location: Add screenshot of CloudWatch graph showing replication latency spike and recovery\n\rStep 3: Cross-Region Verification Check replication completion:\n Count items: Query in each region Compare totals: Should be identical Random sampling: Verify specific items exist Timing analysis: Note total replication time  Exercise 3: Set Up Monitoring Alerts Step 1: Create Latency Alert High replication latency alarm:\n CloudWatch: Go to \u0026ldquo;Alarms\u0026rdquo; â†’ \u0026ldquo;Create alarm\u0026rdquo; Select metric: DynamoDB â†’ Global Table â†’ ReplicationLatency Configuration:  Statistic: Average Period: 5 minutes Threshold: Greater than 10 seconds Datapoints: 2 out of 3    Screenshot Location: Add screenshot of alarm creation interface with replication latency settings\n\rStep 2: Create Pending Count Alert Stuck replication alarm:\n Create new alarm: For PendingReplicationCount Configuration:  Statistic: Maximum Period: 5 minutes Threshold: Greater than 100 items Datapoints: 3 out of 3    Screenshot Location: Add screenshot showing pending replication count alarm configuration\n\rStep 3: Configure Notifications Set up alert actions:\n SNS Topic: Create \u0026ldquo;DynamoDB-GlobalTables-Alerts\u0026rdquo; Email subscription: Add your email Confirm subscription: Check email and confirm Attach to alarms: Add SNS action to both alarms  Screenshot Location: Add screenshot of SNS topic creation and email subscription setup\n\rExercise 4: Regional Performance Analysis Step 1: Compare Regional Metrics Analyze per-region performance:\n Create dashboard: \u0026ldquo;Global Tables Regional Analysis\u0026rdquo; Add metrics for each region:  ConsumedReadCapacityUnits ConsumedWriteCapacityUnits ThrottledRequests SuccessfulRequestLatency    Screenshot Location: Add screenshot of regional performance comparison dashboard\n\rStep 2: Identify Usage Patterns Look for patterns:\n Read distribution: Which regions get more reads? Write distribution: Which regions get more writes? Peak times: When is each region busiest? Throttling: Any capacity issues?  Step 3: Capacity Planning Insights Use metrics for planning:\n Peak usage: Note highest consumption Regional differences: Compare usage patterns Growth trends: Look at usage over time Capacity needs: Plan for scaling  Exercise 5: Troubleshooting Scenarios Scenario 1: High Replication Latency Simulated problem investigation:\n Symptoms: Latency \u0026gt; 30 seconds consistently Check metrics: PendingReplicationCount growing Investigate:  Large item sizes? Network issues? High write volume? Regional capacity throttling?    Screenshot Location: Add screenshot showing troubleshooting dashboard with high latency indicators\n\rScenario 2: Replication Failures Error investigation:\n Metrics to check:  ReplicationLatency (would be missing data points) DynamoDB error logs Failed request counts   Common causes:  Item too large (\u0026gt;400KB) Provisioned capacity exceeded Service limits reached    Scenario 3: Inconsistent Data Data consistency issues:\n Symptoms: Different data in different regions Investigation steps:  Check replication lag Verify conflict resolution Review application write patterns Check for failed replications    Exercise 6: Performance Optimization Step 1: Identify Bottlenecks Analysis checklist:\n Item sizes: Large items replicate slowly Hot partitions: Uneven write distribution Write patterns: Batch vs individual writes Regional capacity: Adequate in all regions  Step 2: Optimization Strategies Performance improvements:\n Reduce item size: Remove unnecessary attributes Batch operations: Group writes when possible Distribute load: Avoid hot partitions Capacity planning: Right-size each region  Step 3: Monitor Improvements Track optimization results:\n Before/after metrics: Compare replication latency Capacity utilization: More even distribution Error rates: Should decrease Application performance: Improved response times  Real-World Monitoring Strategy Production Monitoring Checklist Essential alerts:\n âœ… Replication latency \u0026gt; 10 seconds âœ… Pending replication \u0026gt; 100 items âœ… Failed replications \u0026gt; 0 âœ… Capacity utilization \u0026gt; 80% âœ… Throttling events \u0026gt; 0  Dashboard Organization Recommended dashboards:\n Executive summary: High-level health Operations dashboard: Detailed metrics Regional comparison: Cross-region analysis Troubleshooting: Drill-down views  Alerting Best Practices Alert configuration:\n Severity levels: Critical vs warning Notification timing: Immediate vs batched Escalation paths: On-call rotation Runbook links: Troubleshooting guides  Cost Monitoring Track Global Tables Costs Monitor expenses:\n Cost Explorer: Filter by DynamoDB service Region breakdown: See per-region costs Feature costs: Global Tables premium Data transfer: Cross-region charges  Screenshot Location: Add screenshot of Cost Explorer showing DynamoDB Global Tables costs by region\n\rCost Optimization Reduce expenses:\n Right-size capacity: Avoid over-provisioning Regional placement: Optimize for user proximity Data lifecycle: Archive old data Feature usage: Evaluate Global Tables necessity  Integration with Application Monitoring Application-Level Metrics Track in your application:\n// Example: Track replication lag from application const metrics = { writeRegion: \u0026#39;us-east-1\u0026#39;, writeTimestamp: Date.now(), readRegion: \u0026#39;eu-west-1\u0026#39;, readTimestamp: Date.now(), replicationDelay: readTimestamp - writeTimestamp }; Custom CloudWatch Metrics Send application metrics:\n Custom namespace: \u0026ldquo;MyApp/GlobalTables\u0026rdquo; Key metrics:  Perceived replication lag Cross-region read success rate User experience impact Business KPIs affected    Important: Monitor both AWS-provided metrics and application-specific performance indicators for complete visibility.\n\rExercise Summary You\u0026rsquo;ve now mastered Global Tables monitoring:\n âœ… CloudWatch metrics setup and interpretation âœ… Performance monitoring dashboards âœ… Alerting configuration for proactive management âœ… Troubleshooting common replication issues âœ… Cost monitoring for Global Tables âœ… Integration with application monitoring  Monitoring Mastery: You can now operate Global Tables confidently in production with comprehensive observability!\n\rNext Steps With Global Tables setup and monitoring complete, you\u0026rsquo;re ready to explore advanced DynamoDB patterns including streams processing, which enables real-time data processing and event-driven architectures.\n"
},
{
	"uri": "http://localhost:1313/4-streams-lambda-processing/4.1-stream-configuration/",
	"title": "4.1 Stream Configuration",
	"tags": [],
	"description": "",
	"content": "Enable DynamoDB Streams ğŸ”§ Configure your DynamoDB table to capture every data change\nOverview DynamoDB Streams capture data modification events in your tables. When enabled, streams provide a time-ordered sequence of item-level modifications for up to 24 hours.\nWhat Streams Capture Event Types:\n INSERT: New item added to table MODIFY: Existing item updated REMOVE: Item deleted from table  Stream View Types:\n KEYS_ONLY: Only key attributes of modified item NEW_IMAGE: Entire item after modification OLD_IMAGE: Entire item before modification NEW_AND_OLD_IMAGES: Both before and after images  Exercise 1: Enable Streams on Existing Table Step 1: Navigate to DynamoDB Console Access your table:\n AWS Console: Search \u0026ldquo;DynamoDB\u0026rdquo; Tables: Click \u0026ldquo;Tables\u0026rdquo; in left sidebar Select table: Click demo-ecommerce-freetier Exports and streams: Click \u0026ldquo;Exports and streams\u0026rdquo; tab  Screenshot Location: Add screenshot of DynamoDB console with Exports and streams tab highlighted\n\rStep 2: Configure DynamoDB Stream Enable stream processing:\n DynamoDB stream section: Scroll to \u0026ldquo;DynamoDB stream\u0026rdquo; section Turn on stream: Click \u0026ldquo;Turn on\u0026rdquo; button View type selection: Choose \u0026ldquo;New and old images\u0026rdquo; Confirmation: Click \u0026ldquo;Turn on stream\u0026rdquo;  Screenshot Location: Add screenshot of stream configuration dialog with \u0026ldquo;New and old images\u0026rdquo; selected\n\rStep 3: Verify Stream Configuration Check stream status:\n Stream details: Note the stream ARN appears Status: Should show \u0026ldquo;Active\u0026rdquo; View type: Confirms \u0026ldquo;New and old images\u0026rdquo; Creation time: Shows when stream was enabled  Screenshot Location: Add screenshot showing active stream with ARN and configuration details\n\rExercise 2: Understanding Stream Settings Stream View Type Comparison Choose the right view type for your use case:\n   View Type Use Case Data Captured     KEYS_ONLY Audit logging PK, SK only   NEW_IMAGE Cache updates Item after change   OLD_IMAGE Change tracking Item before change   NEW_AND_OLD_IMAGES Full audit Both versions    Performance Considerations Stream Configuration Impact:\n Storage: NEW_AND_OLD_IMAGES uses most space Lambda payload: Larger payloads with full images Processing time: More data = longer processing Cost: Minimal additional cost for streams  Free Tier Note: DynamoDB Streams are included at no additional charge. Lambda processing stays within Free Tier limits.\n\rExercise 3: Test Stream Functionality Step 1: Create Test Item Generate a stream event:\n Items tab: Go back to \u0026ldquo;Items\u0026rdquo; tab Create item: Click \u0026ldquo;Create item\u0026rdquo; Add test data:  { \u0026#34;PK\u0026#34;: \u0026#34;STREAM#test-item\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;EVENT#001\u0026#34;, \u0026#34;event_type\u0026#34;: \u0026#34;stream_test\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Testing DynamoDB Stream functionality\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-08-11T16:00:00Z\u0026#34;, \u0026#34;test_purpose\u0026#34;: \u0026#34;verify_stream_capture\u0026#34; } Create: Click \u0026ldquo;Create item\u0026rdquo;  Screenshot Location: Add screenshot of item creation dialog with stream test data\n\rStep 2: Monitor Stream Activity Check stream metrics:\n CloudWatch: Open CloudWatch console in new tab Metrics: Navigate to Metrics DynamoDB: Click \u0026ldquo;DynamoDB\u0026rdquo; namespace Stream metrics: Look for stream-related metrics IncomingRecords: Should show 1 new record  Screenshot Location: Add screenshot of CloudWatch showing DynamoDB stream metrics\n\rStep 3: Understand Stream Records Stream record structure (for reference):\n{ \u0026#34;Records\u0026#34;: [ { \u0026#34;eventID\u0026#34;: \u0026#34;12345...\u0026#34;, \u0026#34;eventName\u0026#34;: \u0026#34;INSERT\u0026#34;, \u0026#34;eventVersion\u0026#34;: \u0026#34;1.1\u0026#34;, \u0026#34;eventSource\u0026#34;: \u0026#34;aws:dynamodb\u0026#34;, \u0026#34;awsRegion\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;dynamodb\u0026#34;: { \u0026#34;ApproximateCreationDateTime\u0026#34;: 1642857600, \u0026#34;Keys\u0026#34;: { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;STREAM#test-item\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;EVENT#001\u0026#34;} }, \u0026#34;NewImage\u0026#34;: { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;STREAM#test-item\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;EVENT#001\u0026#34;}, \u0026#34;event_type\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;stream_test\u0026#34;}, \u0026#34;description\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Testing DynamoDB Stream functionality\u0026#34;}, \u0026#34;created_at\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;2025-08-11T16:00:00Z\u0026#34;}, \u0026#34;test_purpose\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;verify_stream_capture\u0026#34;} }, \u0026#34;SequenceNumber\u0026#34;: \u0026#34;123456789\u0026#34;, \u0026#34;SizeBytes\u0026#34;: 245, \u0026#34;StreamViewType\u0026#34;: \u0026#34;NEW_AND_OLD_IMAGES\u0026#34; } } ] } Key components:\n eventName: INSERT (since this is a new item) Keys: Primary key of the changed item NewImage: Complete item data after creation OldImage: Would be empty for INSERT events  Exercise 4: Stream Configuration Best Practices Optimal Configuration for Workshop Recommended settings:\n âœ… View Type: NEW_AND_OLD_IMAGES (comprehensive audit trail) âœ… Retention: 24 hours (default, sufficient for processing) âœ… Shards: Auto-managed by AWS âœ… Processing: Lambda with appropriate batch size  Security Considerations Access control:\n IAM permissions: Lambda needs stream read permissions Encryption: Streams inherit table encryption settings VPC: Streams work within your VPC configuration Monitoring: CloudTrail logs stream access  Cost Optimization Stream cost factors:\n Read requests: No additional charge for stream writes Lambda invocations: Count toward Free Tier Data transfer: Minimal for in-region processing Storage: Stream records retained for 24 hours only  Exercise 5: Advanced Stream Configuration Multiple Consumer Pattern When you need multiple processors:\n Single stream: One DynamoDB stream per table Multiple Lambdas: Each can process the same stream Kinesis Data Streams: For more complex routing Event filtering: Lambda-level filtering  Cross-Region Considerations Global Tables + Streams:\n Each region: Has its own stream Replication events: Generate stream records Filtering: Distinguish app writes from replication Processing: Handle regional differences  Screenshot Location: Add screenshot of Global Tables with streams enabled in multiple regions\n\rTroubleshooting Common Issues Stream Not Appearing Check these items:\n Permissions: Ensure you have DynamoDB full access Region: Verify you\u0026rsquo;re in the correct AWS region Table status: Table must be ACTIVE to enable streams Refresh: Browser refresh may be needed  Stream Configuration Failed Possible causes:\n Table updating: Wait for table to be ACTIVE Permissions: Need dynamodb:EnableStream permission Rate limits: Wait and retry if rate limited Billing: Ensure account is in good standing  Stream Records Missing Debugging steps:\n Stream status: Confirm stream is ACTIVE Write operations: Ensure items are actually changing Time delay: Allow 1-2 minutes for propagation Metrics: Check CloudWatch for IncomingRecords  Configuration Summary By completing this exercise, you have:\n âœ… Enabled DynamoDB Streams on your table âœ… Configured view type for comprehensive change capture âœ… Tested stream functionality with sample data âœ… Understood stream record structure and components âœ… Applied best practices for optimal configuration  Stream Ready: Your DynamoDB table now captures every change and is ready for Lambda processing!\n\rNext Steps With streams configured, you\u0026rsquo;re ready to create Lambda functions that will process these events in real-time. In the next section, we\u0026rsquo;ll build and deploy a Lambda function optimized for DynamoDB stream processing.\n"
},
{
	"uri": "http://localhost:1313/4-streams-lambda-processing/4.2-lambda-function-setup/",
	"title": "4.2 Lambda Function Setup",
	"tags": [],
	"description": "",
	"content": "Create Stream Processing Lambda âš™ï¸ Build a Lambda function to process DynamoDB stream events in real-time\nOverview AWS Lambda provides serverless compute to process DynamoDB stream events. Your function will automatically trigger when items change in your table, enabling real-time processing patterns.\nFunction Requirements Free Tier Optimized Configuration:\n Runtime: Python 3.9 (reliable and well-supported) Memory: 128 MB (minimum for Free Tier) Timeout: 30 seconds (sufficient for stream processing) Concurrent executions: 10 (Free Tier safe)  Exercise 1: Create Lambda Function Step 1: Access Lambda Console Navigate to Lambda service:\n AWS Console: Search \u0026ldquo;Lambda\u0026rdquo; Functions: Click \u0026ldquo;Functions\u0026rdquo; in left sidebar Create function: Click \u0026ldquo;Create function\u0026rdquo; button Author from scratch: Select this option  Screenshot Location: Add screenshot of Lambda console with Create function button highlighted\n\rStep 2: Configure Basic Settings Function configuration:\n Function name: demo-dynamodb-stream-processor Runtime: Select \u0026ldquo;Python 3.9\u0026rdquo; Architecture: Leave as \u0026ldquo;x86_64\u0026rdquo; Permissions: \u0026ldquo;Create a new role with basic Lambda permissions\u0026rdquo; Create function: Click to proceed  Screenshot Location: Add screenshot of Lambda function creation form with specified settings\n\rStep 3: Configure Function Settings Optimize for Free Tier:\n Configuration tab: Click after function creation General configuration: Click \u0026ldquo;Edit\u0026rdquo; Memory: Set to 128 MB Timeout: Set to 30 seconds Save: Click to apply changes  Screenshot Location: Add screenshot of Lambda function configuration settings with memory and timeout values\n\rExercise 2: Add Stream Processing Code Step 1: Replace Function Code Navigate to code editor:\n Code tab: Click to open code editor lambda_function.py: Replace existing code with:  import json import boto3 import logging from datetime import datetime # Configure logging logger = logging.getLogger() logger.setLevel(logging.INFO) def lambda_handler(event, context): \u0026#34;\u0026#34;\u0026#34; Process DynamoDB Stream events Optimized for AWS Free Tier \u0026#34;\u0026#34;\u0026#34; try: processed_records = 0 # Process each record in the batch for record in event[\u0026#39;Records\u0026#39;]: event_name = record[\u0026#39;eventName\u0026#39;] # Process INSERT, MODIFY, REMOVE events if event_name in [\u0026#39;INSERT\u0026#39;, \u0026#39;MODIFY\u0026#39;, \u0026#39;REMOVE\u0026#39;]: process_stream_record(record) processed_records += 1 # Return success response return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;message\u0026#39;: f\u0026#39;Successfully processed {processed_records}records\u0026#39;, \u0026#39;timestamp\u0026#39;: datetime.utcnow().isoformat(), \u0026#39;processed_count\u0026#39;: processed_records }) } except Exception as e: logger.error(f\u0026#34;Error processing stream records: {str(e)}\u0026#34;) # Re-raise for Lambda retry logic raise e def process_stream_record(record): \u0026#34;\u0026#34;\u0026#34; Process individual stream record Add your business logic here \u0026#34;\u0026#34;\u0026#34; event_name = record[\u0026#39;eventName\u0026#39;] # Extract key information if \u0026#39;dynamodb\u0026#39; in record: keys = record[\u0026#39;dynamodb\u0026#39;].get(\u0026#39;Keys\u0026#39;, {}) pk = keys.get(\u0026#39;PK\u0026#39;, {}).get(\u0026#39;S\u0026#39;, \u0026#39;\u0026#39;) sk = keys.get(\u0026#39;SK\u0026#39;, {}).get(\u0026#39;S\u0026#39;, \u0026#39;\u0026#39;) logger.info(f\u0026#34;Processing {event_name}for item: {pk}#{sk}\u0026#34;) # Handle different event types if event_name == \u0026#39;INSERT\u0026#39;: handle_insert_event(record) elif event_name == \u0026#39;MODIFY\u0026#39;: handle_modify_event(record) elif event_name == \u0026#39;REMOVE\u0026#39;: handle_remove_event(record) def handle_insert_event(record): \u0026#34;\u0026#34;\u0026#34; Handle new item creation \u0026#34;\u0026#34;\u0026#34; logger.info(\u0026#34;Processing INSERT event\u0026#34;) # Get new item data new_image = record[\u0026#39;dynamodb\u0026#39;].get(\u0026#39;NewImage\u0026#39;, {}) # Example: Send notification for new user if \u0026#39;USER#\u0026#39; in str(new_image.get(\u0026#39;PK\u0026#39;, {})): logger.info(\u0026#34;New user created - could send welcome email\u0026#34;) # Example: Update inventory for new product elif \u0026#39;PRODUCT#\u0026#39; in str(new_image.get(\u0026#39;PK\u0026#39;, {})): logger.info(\u0026#34;New product created - could update search index\u0026#34;) # Example: Process new order elif \u0026#39;ORDER#\u0026#39; in str(new_image.get(\u0026#39;SK\u0026#39;, {})): logger.info(\u0026#34;New order created - could trigger fulfillment\u0026#34;) def handle_modify_event(record): \u0026#34;\u0026#34;\u0026#34; Handle item updates \u0026#34;\u0026#34;\u0026#34; logger.info(\u0026#34;Processing MODIFY event\u0026#34;) # Get before and after images old_image = record[\u0026#39;dynamodb\u0026#39;].get(\u0026#39;OldImage\u0026#39;, {}) new_image = record[\u0026#39;dynamodb\u0026#39;].get(\u0026#39;NewImage\u0026#39;, {}) # Example: Check for status changes old_status = old_image.get(\u0026#39;status\u0026#39;, {}).get(\u0026#39;S\u0026#39;, \u0026#39;\u0026#39;) new_status = new_image.get(\u0026#39;status\u0026#39;, {}).get(\u0026#39;S\u0026#39;, \u0026#39;\u0026#39;) if old_status != new_status: logger.info(f\u0026#34;Status changed from {old_status}to {new_status}\u0026#34;) # Could trigger notifications, cache updates, etc. # Example: Price change detection old_price = old_image.get(\u0026#39;price\u0026#39;, {}).get(\u0026#39;N\u0026#39;, \u0026#39;0\u0026#39;) new_price = new_image.get(\u0026#39;price\u0026#39;, {}).get(\u0026#39;N\u0026#39;, \u0026#39;0\u0026#39;) if old_price != new_price: logger.info(f\u0026#34;Price changed from {old_price}to {new_price}\u0026#34;) # Could invalidate cache, update recommendations, etc. def handle_remove_event(record): \u0026#34;\u0026#34;\u0026#34; Handle item deletion \u0026#34;\u0026#34;\u0026#34; logger.info(\u0026#34;Processing REMOVE event\u0026#34;) # Get deleted item data old_image = record[\u0026#39;dynamodb\u0026#39;].get(\u0026#39;OldImage\u0026#39;, {}) # Example: Cleanup related data if \u0026#39;USER#\u0026#39; in str(old_image.get(\u0026#39;PK\u0026#39;, {})): logger.info(\u0026#34;User deleted - could cleanup user data\u0026#34;) # Example: Remove from search index elif \u0026#39;PRODUCT#\u0026#39; in str(old_image.get(\u0026#39;PK\u0026#39;, {})): logger.info(\u0026#34;Product deleted - could remove from search\u0026#34;) Deploy: Click \u0026ldquo;Deploy\u0026rdquo; to save the code  Screenshot Location: Add screenshot of Lambda code editor with the stream processing code\n\rStep 2: Test Function Syntax Validate the code:\n Test tab: Click \u0026ldquo;Test\u0026rdquo; tab Create test event: Click \u0026ldquo;Create new event\u0026rdquo; Event template: Select \u0026ldquo;DynamoDB Stream\u0026rdquo; template Event name: test-stream-event Test: Click \u0026ldquo;Test\u0026rdquo; to validate syntax  Screenshot Location: Add screenshot of Lambda test configuration with DynamoDB Stream template\n\rExercise 3: Configure Event Source Mapping Step 1: Add DynamoDB Trigger Connect Lambda to DynamoDB Stream:\n Function overview: In Lambda console Add trigger: Click \u0026ldquo;Add trigger\u0026rdquo; button Trigger configuration:  Source: Select \u0026ldquo;DynamoDB\u0026rdquo; DynamoDB table: Choose demo-ecommerce-freetier Batch size: Set to 10 (Free Tier optimized) Starting position: Select \u0026ldquo;Trim horizon\u0026rdquo;   Add: Click to create trigger  Screenshot Location: Add screenshot of trigger configuration dialog with DynamoDB settings\n\rStep 2: Verify Event Source Mapping Check trigger configuration:\n Function overview: Should show DynamoDB trigger Configuration: Verify settings:  Batch size: 10 records Starting position: Trim horizon Status: Enabled State: Creating â†’ Enabled    Screenshot Location: Add screenshot showing successful DynamoDB trigger configuration\n\rStep 3: Configure IAM Permissions Update Lambda execution role:\n Configuration tab: Click \u0026ldquo;Permissions\u0026rdquo; Execution role: Click role name link IAM console: Opens in new tab Attach policies: Add AWSLambdaDynamoDBExecutionRole Save: Return to Lambda console  Screenshot Location: Add screenshot of IAM role with DynamoDB stream permissions\n\rExercise 4: Test Stream Processing Step 1: Create Test Item Generate stream event:\n DynamoDB console: Open in new tab Items tab: Navigate to your table Create item: Add test data:  { \u0026#34;PK\u0026#34;: \u0026#34;LAMBDA#test-processing\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;EVENT#001\u0026#34;, \u0026#34;event_type\u0026#34;: \u0026#34;lambda_test\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Testing Lambda stream processing\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-08-11T16:30:00Z\u0026#34;, \u0026#34;test_purpose\u0026#34;: \u0026#34;verify_lambda_trigger\u0026#34; } Create: Save the item  Screenshot Location: Add screenshot of DynamoDB item creation for Lambda testing\n\rStep 2: Monitor Lambda Execution Check function invocation:\n Lambda console: Return to your function Monitor tab: Click to view metrics Invocations: Should show 1 new invocation Duration: Typically \u0026lt; 1 second Errors: Should be 0  Screenshot Location: Add screenshot of Lambda monitoring tab showing successful invocation\n\rStep 3: Check Processing Logs View detailed logs:\n CloudWatch logs: Click \u0026ldquo;View CloudWatch logs\u0026rdquo; Log stream: Click latest log stream Log entries: Look for:  START RequestId: [uuid] Processing INSERT for item: LAMBDA#test-processing#EVENT#001 Processing INSERT event Successfully processed 1 records END RequestId: [uuid]    Screenshot Location: Add screenshot of CloudWatch logs showing successful stream processing\n\rExercise 5: Advanced Configuration Error Handling Configuration Configure retry and error handling:\n Event source mapping: Edit your DynamoDB trigger Additional settings:  Retry attempts: 3 (default) Maximum record age: 3600 seconds Split batch on error: Enable Dead letter queue: Configure SNS/SQS (optional)    Performance Optimization Free Tier optimization settings:\n Parallelization factor: 1 (avoid excess concurrency) Batch size: 10 records (balance latency vs cost) Reserved concurrency: 10 (control costs) Provisioned concurrency: 0 (not needed for streams)  Monitoring and Alerting Set up basic monitoring:\n CloudWatch Alarms: Create for:  Function errors \u0026gt; 0 Function duration \u0026gt; 20 seconds Iterator age \u0026gt; 30 seconds   Notifications: SNS topic for alerts Dashboard: Add metrics to CloudWatch dashboard  Screenshot Location: Add screenshot of CloudWatch alarm configuration for Lambda function\n\rFunction Testing Patterns Test Different Event Types Comprehensive testing:\n INSERT: Create new items MODIFY: Update existing items REMOVE: Delete items Batch: Multiple rapid changes  Validation Checklist Verify your setup:\n âœ… Lambda function created with correct runtime âœ… Stream trigger configured with proper permissions âœ… Code deployed and syntax validated âœ… Test successful with sample data âœ… Logs showing processing details âœ… Metrics indicating healthy execution  Troubleshooting Common Issues Lambda Not Triggering Check these items:\n Stream enabled: DynamoDB stream is active Permissions: Lambda has stream read permissions Event source mapping: Trigger is enabled Function state: Lambda is active (not failed)  Processing Errors Debug steps:\n CloudWatch logs: Check for error messages Timeout issues: Increase timeout if needed Memory errors: Monitor memory usage Permissions: Verify all required permissions  Performance Issues Optimization tips:\n Batch size: Adjust based on processing time Memory allocation: Right-size for your workload Cold starts: Consider provisioned concurrency if needed Error handling: Implement proper retry logic  Lambda Ready: Your function is now processing DynamoDB stream events in real-time!\n\rNext Steps With your Lambda function processing stream events, you\u0026rsquo;re ready to practice with real data changes and explore different event-driven patterns. The next section covers hands-on stream processing exercises.\n"
},
{
	"uri": "http://localhost:1313/4-streams-lambda-processing/4.3-event-processing-practice/",
	"title": "4.3 Event Processing Practice",
	"tags": [],
	"description": "",
	"content": "Hands-On Stream Processing ğŸ¯ Practice real-time event processing with various data changes and business scenarios\nOverview Put your stream processing setup to work with realistic scenarios. You\u0026rsquo;ll create, modify, and delete items while monitoring how Lambda processes each event in real-time.\nExercise Setup Preparation checklist:\n âœ… DynamoDB Streams enabled âœ… Lambda function deployed âœ… Event source mapping active âœ… CloudWatch logs accessible  Exercise 1: INSERT Event Processing Step 1: Setup Monitoring Dashboard Open multiple browser tabs:\n Tab 1: DynamoDB Console (Items view) Tab 2: Lambda Console (Monitor tab) Tab 3: CloudWatch Logs (your function\u0026rsquo;s log group) Tab 4: CloudWatch Metrics (for real-time metrics)  Screenshot Location: Add screenshot showing browser tabs setup for monitoring\n\rStep 2: Create User Profile Test user creation event:\n DynamoDB tab: Navigate to Items Create item: Click \u0026ldquo;Create item\u0026rdquo; User data:  { \u0026#34;PK\u0026#34;: \u0026#34;USER#stream-user-001\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;PROFILE\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;USER#john.stream@example.com\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;PROFILE\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;stream-user-001\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Stream\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john.stream@example.com\u0026#34;, \u0026#34;registration_date\u0026#34;: \u0026#34;2025-08-11T16:45:00Z\u0026#34;, \u0026#34;account_type\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;stream_test\u0026#34;: true } Create: Save the item Note time: Record when you clicked create  Screenshot Location: Add screenshot of user creation in DynamoDB with stream test data\n\rStep 3: Monitor INSERT Processing Check Lambda execution:\n Lambda tab: Refresh Monitor tab Invocations: Should increase by 1 Duration: Note processing time Success rate: Should be 100%  Check processing logs:\n CloudWatch Logs tab: Refresh log stream Look for: \u0026ldquo;Processing INSERT for item: USER#stream-user-001#PROFILE\u0026rdquo; Business logic: \u0026ldquo;New user created - could send welcome email\u0026rdquo; Success message: \u0026ldquo;Successfully processed 1 records\u0026rdquo;  Screenshot Location: Add screenshot of CloudWatch logs showing INSERT event processing\n\rStep 4: Create Product Item Test product creation:\n{ \u0026#34;PK\u0026#34;: \u0026#34;PRODUCT#stream-prod-001\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;DETAILS\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;CATEGORY#electronics\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;PRODUCT#stream-prod-001\u0026#34;, \u0026#34;GSI2PK\u0026#34;: \u0026#34;PRICE#100-500\u0026#34;, \u0026#34;GSI2SK\u0026#34;: \u0026#34;PRODUCT#stream-prod-001\u0026#34;, \u0026#34;product_id\u0026#34;: \u0026#34;stream-prod-001\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Stream Test Smartphone\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Test product for stream processing\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;electronics\u0026#34;, \u0026#34;price\u0026#34;: 299, \u0026#34;stock\u0026#34;: 50, \u0026#34;created_date\u0026#34;: \u0026#34;2025-08-11T16:50:00Z\u0026#34;, \u0026#34;stream_test\u0026#34;: true } Expected log output: \u0026ldquo;New product created - could update search index\u0026rdquo;\nScreenshot Location: Add screenshot of product creation and corresponding log entry\n\rStep 5: Create Order Item Test order creation:\n{ \u0026#34;PK\u0026#34;: \u0026#34;USER#stream-user-001\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;ORDER#stream-order-001\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;ORDER#stream-order-001\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;DETAILS\u0026#34;, \u0026#34;GSI2PK\u0026#34;: \u0026#34;STATUS#pending\u0026#34;, \u0026#34;GSI2SK\u0026#34;: \u0026#34;ORDER#stream-order-001\u0026#34;, \u0026#34;order_id\u0026#34;: \u0026#34;stream-order-001\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;stream-user-001\u0026#34;, \u0026#34;product_id\u0026#34;: \u0026#34;stream-prod-001\u0026#34;, \u0026#34;quantity\u0026#34;: 1, \u0026#34;total_amount\u0026#34;: 299, \u0026#34;status\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;order_date\u0026#34;: \u0026#34;2025-08-11T16:55:00Z\u0026#34;, \u0026#34;stream_test\u0026#34;: true } Expected log output: \u0026ldquo;New order created - could trigger fulfillment\u0026rdquo;\nExercise 2: MODIFY Event Processing Step 1: Update Order Status Test status change processing:\n Find order: Query for ORDER#stream-order-001 Edit item: Click \u0026ldquo;Edit\u0026rdquo; button Update status: Change status from \u0026ldquo;pending\u0026rdquo; to \u0026ldquo;processing\u0026rdquo; Add timestamp: Update or add last_modified field Save changes: Click \u0026ldquo;Save\u0026rdquo;  Screenshot Location: Add screenshot of order status update in DynamoDB\n\rStep 2: Monitor MODIFY Processing Check logs for MODIFY event:\n CloudWatch Logs: Look for new entries Event type: \u0026ldquo;Processing MODIFY event\u0026rdquo; Status change: \u0026ldquo;Status changed from pending to processing\u0026rdquo; Business logic: Could trigger notifications  Screenshot Location: Add screenshot of CloudWatch logs showing MODIFY event with status change detection\n\rStep 3: Update Product Price Test price change detection:\n Find product: PRODUCT#stream-prod-001 Edit price: Change from 299 to 279 Save changes: Update the item Check logs: \u0026ldquo;Price changed from 299 to 279\u0026rdquo;  Step 4: Multiple Field Updates Test complex modifications:\n Edit user profile: Update multiple fields:  name: \u0026ldquo;John Stream Updated\u0026rdquo; last_login: \u0026ldquo;2025-08-11T17:00:00Z\u0026rdquo; login_count: 5   Save changes: Apply updates Monitor logs: Check processing details  Exercise 3: REMOVE Event Processing Step 1: Delete Test Product Test deletion processing:\n Find product: PRODUCT#stream-prod-001 Delete item: Click \u0026ldquo;Delete\u0026rdquo; button Confirm deletion: Click \u0026ldquo;Delete\u0026rdquo; in confirmation dialog Note time: Record deletion time  Screenshot Location: Add screenshot of item deletion confirmation dialog\n\rStep 2: Monitor REMOVE Processing Check deletion logs:\n CloudWatch Logs: Look for REMOVE event Event type: \u0026ldquo;Processing REMOVE event\u0026rdquo; Business logic: \u0026ldquo;Product deleted - could remove from search\u0026rdquo; Old image: Shows deleted item data  Screenshot Location: Add screenshot of CloudWatch logs showing REMOVE event processing\n\rStep 3: Cleanup Remaining Items Delete test items:\n Delete order: ORDER#stream-order-001 Delete user: USER#stream-user-001 Monitor each: Check logs for each deletion Verify processing: All events processed successfully  Exercise 4: Batch Processing Analysis Step 1: Create Multiple Items Rapidly Test batch processing:\n Create 5 items quickly (within 30 seconds):  // Item 1 { \u0026#34;PK\u0026#34;: \u0026#34;BATCH#test-001\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;DATA\u0026#34;, \u0026#34;test_type\u0026#34;: \u0026#34;batch_processing\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-08-11T17:10:00Z\u0026#34; } // Item 2 { \u0026#34;PK\u0026#34;: \u0026#34;BATCH#test-002\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;DATA\u0026#34;, \u0026#34;test_type\u0026#34;: \u0026#34;batch_processing\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-08-11T17:10:15Z\u0026#34; } // Continue with test-003, test-004, test-005... Create rapidly: Submit each within seconds of each other  Screenshot Location: Add screenshot showing rapid item creation for batch testing\n\rStep 2: Analyze Batch Processing Check Lambda behavior:\n Monitor tab: Check invocation count Fewer invocations: Should be less than 5 (batching occurred) CloudWatch Logs: Look for entries like \u0026ldquo;Successfully processed 3 records\u0026rdquo; Batch efficiency: Multiple records per invocation  Step 3: Measure Processing Time Performance analysis:\n Duration metrics: Average processing time per batch Throughput: Records processed per second Cost efficiency: Fewer invocations = lower cost Latency: Time from change to processing  Screenshot Location: Add screenshot of Lambda metrics showing batch processing efficiency\n\rExercise 5: Real-World Scenarios Scenario 1: E-commerce Order Flow Simulate complete order lifecycle:\n User signup: Create new user profile Product browsing: Create product views (optional) Add to cart: Create cart items Place order: Create order with \u0026ldquo;pending\u0026rdquo; status Payment: Update order to \u0026ldquo;paid\u0026rdquo; Fulfillment: Update to \u0026ldquo;processing\u0026rdquo; â†’ \u0026ldquo;shipped\u0026rdquo; Delivery: Update to \u0026ldquo;delivered\u0026rdquo;  Monitor each step: Check logs for appropriate business logic triggers\nScenario 2: Inventory Management Test inventory updates:\n Create product: With initial stock level Sale occurs: Decrease stock by purchase quantity Restock: Increase stock when inventory arrives Low stock: Monitor when stock falls below threshold Out of stock: Handle zero inventory scenarios  Scenario 3: User Activity Tracking Profile management scenario:\n Registration: New user creation Profile updates: Name, email, preferences changes Activity logging: Login events, page views Account upgrades: Change account type Account deletion: Remove user data  Exercise 6: Error Handling Testing Step 1: Test Large Item Processing Create oversized item (careful with limits):\n{ \u0026#34;PK\u0026#34;: \u0026#34;LARGE#test-item\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;DATA\u0026#34;, \u0026#34;large_data\u0026#34;: \u0026#34;[Create a string approaching 400KB limit]\u0026#34;, \u0026#34;test_type\u0026#34;: \u0026#34;size_testing\u0026#34; } Monitor: Check if processing handles large payloads appropriately\nStep 2: Rapid Fire Testing Stress test scenario:\n Create 20 items: As fast as possible Monitor throttling: Check for any rate limiting Processing lag: Measure iterator age Error rates: Ensure no processing failures  Step 3: Recovery Testing Test retry behavior:\n Simulate errors: (Advanced) Modify Lambda to fail temporarily Check retries: Verify automatic retry logic Dead letter queue: Monitor failed messages (if configured) Recovery: Ensure processing resumes normally  Performance Metrics Analysis Key Metrics to Track Lambda Function Metrics:\n Invocations: Total function calls Duration: Average execution time (target \u0026lt; 1 second) Errors: Should remain at 0 Throttles: Should be 0 for Free Tier usage  DynamoDB Stream Metrics:\n IncomingRecords: Rate of changes captured IteratorAge: Processing delay (target \u0026lt; 1 second) ReadThrottledEvents: Should be 0  Optimization Insights Processing efficiency:\n Batch size impact: Larger batches = fewer invocations Memory usage: Monitor to optimize allocation Cold starts: First invocation may be slower Consistent performance: Subsequent calls should be fast  Screenshot Location: Add screenshot of comprehensive metrics dashboard showing all key performance indicators\n\rExercise Summary Through these exercises, you\u0026rsquo;ve experienced:\n âœ… INSERT event processing with different item types âœ… MODIFY event handling with change detection âœ… REMOVE event processing with cleanup logic âœ… Batch processing for efficiency optimization âœ… Real-world scenarios for practical application âœ… Performance monitoring and optimization insights  Business Logic Patterns Demonstrated Event-driven actions you\u0026rsquo;ve implemented:\n User registration â†’ Welcome email trigger Product creation â†’ Search index update Order placement â†’ Fulfillment notification Status changes â†’ Customer notifications Price changes â†’ Cache invalidation Item deletion â†’ Cleanup operations  Stream Processing Mastery: You now understand how to build responsive, event-driven applications with DynamoDB Streams and Lambda!\n\rNext Steps With hands-on stream processing experience complete, the final section covers monitoring, debugging, and troubleshooting your stream processing pipeline in production scenarios.\n"
},
{
	"uri": "http://localhost:1313/4-streams-lambda-processing/4.4-monitoring-debugging/",
	"title": "4.4 Monitoring &amp; Debugging",
	"tags": [],
	"description": "",
	"content": "Stream Processing Operations ğŸ“Š Monitor, debug, and optimize your DynamoDB Streams and Lambda processing pipeline\nOverview Production stream processing requires comprehensive monitoring, proactive alerting, and effective debugging capabilities. Learn to operate your event-driven system with confidence.\nMonitoring Strategy Multi-layered approach:\n Infrastructure metrics: Lambda and DynamoDB performance Application metrics: Business logic success rates Cost metrics: Free Tier usage and optimization Error tracking: Failed processing and retry patterns  Exercise 1: Comprehensive Monitoring Setup Step 1: Create Monitoring Dashboard Build CloudWatch dashboard:\n CloudWatch console: Navigate to Dashboards Create dashboard: Click \u0026ldquo;Create dashboard\u0026rdquo; Dashboard name: DynamoDB-Streams-Monitoring Add widgets: Start with blank dashboard  Screenshot Location: Add screenshot of CloudWatch dashboard creation interface\n\rStep 2: Add Lambda Function Metrics Essential Lambda metrics:\n Add widget: Click \u0026ldquo;Add widget\u0026rdquo; Line graph: Select line graph type Metrics: Choose AWS/Lambda namespace Select metrics:  Invocations (demo-dynamodb-stream-processor) Duration (demo-dynamodb-stream-processor) Errors (demo-dynamodb-stream-processor) Throttles (demo-dynamodb-stream-processor)   Period: Set to 5 minutes Add to dashboard: Save widget  Screenshot Location: Add screenshot of Lambda metrics widget configuration\n\rStep 3: Add DynamoDB Stream Metrics Stream-specific monitoring:\n Add widget: New line graph Metrics: AWS/DynamoDB namespace Stream metrics:  IncomingRecords (your table stream) IteratorAge (your table stream) ReadThrottledEvents (your table stream)   Time range: Last 1 hour Refresh: Auto-refresh every 1 minute  Screenshot Location: Add screenshot of DynamoDB stream metrics configuration\n\rStep 4: Add Custom Business Metrics Application-level tracking:\n Custom namespace: \u0026ldquo;DynamoDB-Workshop/StreamProcessing\u0026rdquo; Metrics to track:  Processed INSERT events Processed MODIFY events Processed REMOVE events Business logic success rate Processing latency    Example custom metric code (add to Lambda):\nimport boto3 cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;) def send_custom_metric(metric_name, value, unit=\u0026#39;Count\u0026#39;): \u0026#34;\u0026#34;\u0026#34;Send custom metric to CloudWatch\u0026#34;\u0026#34;\u0026#34; try: cloudwatch.put_metric_data( Namespace=\u0026#39;DynamoDB-Workshop/StreamProcessing\u0026#39;, MetricData=[ { \u0026#39;MetricName\u0026#39;: metric_name, \u0026#39;Value\u0026#39;: value, \u0026#39;Unit\u0026#39;: unit, \u0026#39;Timestamp\u0026#39;: datetime.utcnow() } ] ) except Exception as e: logger.error(f\u0026#34;Failed to send metric {metric_name}: {e}\u0026#34;) # Add to your event handlers: def handle_insert_event(record): logger.info(\u0026#34;Processing INSERT event\u0026#34;) # Your business logic here... send_custom_metric(\u0026#39;INSERT_Events_Processed\u0026#39;, 1) Exercise 2: Real-Time Alerting Step 1: Create Lambda Error Alert Alert for processing failures:\n CloudWatch: Navigate to Alarms Create alarm: Click \u0026ldquo;Create alarm\u0026rdquo; Select metric: AWS/Lambda â†’ Errors Function: demo-dynamodb-stream-processor Conditions:  Threshold: Greater than 0 Period: 5 minutes Datapoints: 1 out of 1   Actions: Create SNS topic for notifications  Screenshot Location: Add screenshot of Lambda error alarm configuration\n\rStep 2: Create High Latency Alert Alert for slow processing:\n Create alarm: For Lambda Duration metric Conditions:  Statistic: Average Threshold: Greater than 10 seconds Period: 5 minutes Datapoints: 2 out of 3   Actions: Same SNS topic as error alert  Step 3: Create Stream Lag Alert Alert for processing delays:\n Create alarm: For DynamoDB IteratorAge Conditions:  Threshold: Greater than 30 seconds Period: 5 minutes Datapoints: 2 out of 3   Description: \u0026ldquo;Stream processing falling behind\u0026rdquo;  Screenshot Location: Add screenshot of iterator age alarm showing stream lag monitoring\n\rStep 4: Configure SNS Notifications Set up email alerts:\n SNS console: Create topic \u0026ldquo;StreamProcessing-Alerts\u0026rdquo; Subscription: Add your email address Confirm subscription: Check email and confirm Test notification: Send test message  Exercise 3: Debugging Common Issues Issue 1: Lambda Function Not Triggering Diagnostic checklist:\n Stream status: Verify DynamoDB stream is active Event source mapping: Check if enabled and healthy IAM permissions: Verify Lambda execution role Function state: Ensure Lambda isn\u0026rsquo;t in failed state  Debugging steps:\n DynamoDB console: Check stream status Lambda console: Verify trigger configuration IAM console: Review execution role permissions CloudTrail: Check for permission errors  Screenshot Location: Add screenshot of event source mapping status showing enabled state\n\rIssue 2: Processing Errors in Lambda Error investigation:\n CloudWatch Logs: Check for error messages Common errors:  Timeout errors (increase timeout) Memory errors (increase memory) Permission errors (check IAM) Syntax errors (review code)    Error log example:\n[ERROR] 2025-08-11T17:30:00.000Z Task timed out after 30.00 seconds\rResolution steps:\n Increase timeout: 30s â†’ 60s Add error handling: Try-catch blocks Optimize code: Reduce processing time Monitor memory: Increase if needed  Issue 3: High Iterator Age Stream processing lag:\n Symptoms: IteratorAge \u0026gt; 30 seconds consistently Causes:  High write volume to DynamoDB Lambda function running slowly Insufficient concurrency Error in processing logic    Resolution:\n Optimize Lambda: Reduce processing time Increase concurrency: Allow more parallel executions Batch size tuning: Adjust batch size for efficiency Error handling: Fix processing errors  Screenshot Location: Add screenshot of CloudWatch metrics showing high iterator age and resolution\n\rExercise 4: Performance Optimization Step 1: Analyze Processing Patterns Performance metrics analysis:\n Duration trends: Identify slow processing periods Invocation patterns: Understand batch processing Error patterns: Track failure scenarios Cost patterns: Monitor Free Tier usage  Step 2: Optimize Lambda Configuration Right-sizing your function:\nPerformance Testing Results:\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ Memory (MB) â”‚ Duration â”‚ Cost/Invoke â”‚ Throughput â”‚\râ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\râ”‚ 128 â”‚ 850ms â”‚ Lowest â”‚ Slowest â”‚\râ”‚ 256 â”‚ 425ms â”‚ Medium â”‚ Medium â”‚\râ”‚ 512 â”‚ 215ms â”‚ Higher â”‚ Fastest â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\rOptimal Free Tier configuration:\n Memory: 128 MB (cost effective) Timeout: 30 seconds (sufficient) Batch size: 10 records (balanced) Concurrency: 10 (Free Tier safe)  Step 3: Code Optimization Performance improvements:\n# Optimized stream processing code import json import boto3 import logging from datetime import datetime logger = logging.getLogger() logger.setLevel(logging.INFO) # Reuse connections outside handler cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;) def lambda_handler(event, context): \u0026#34;\u0026#34;\u0026#34; Optimized stream processing \u0026#34;\u0026#34;\u0026#34; start_time = datetime.utcnow() processed_records = 0 errors = 0 try: # Batch process records for record in event[\u0026#39;Records\u0026#39;]: try: process_stream_record(record) processed_records += 1 except Exception as e: logger.error(f\u0026#34;Error processing record: {e}\u0026#34;) errors += 1 # Continue processing other records # Send metrics in batch end_time = datetime.utcnow() processing_duration = (end_time - start_time).total_seconds() send_batch_metrics(processed_records, errors, processing_duration) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;processed\u0026#39;: processed_records, \u0026#39;errors\u0026#39;: errors, \u0026#39;duration_seconds\u0026#39;: processing_duration }) } except Exception as e: logger.error(f\u0026#34;Critical error in lambda_handler: {e}\u0026#34;) raise e def send_batch_metrics(processed, errors, duration): \u0026#34;\u0026#34;\u0026#34;Send multiple metrics in single call\u0026#34;\u0026#34;\u0026#34; try: cloudwatch.put_metric_data( Namespace=\u0026#39;DynamoDB-Workshop/StreamProcessing\u0026#39;, MetricData=[ { \u0026#39;MetricName\u0026#39;: \u0026#39;RecordsProcessed\u0026#39;, \u0026#39;Value\u0026#39;: processed, \u0026#39;Unit\u0026#39;: \u0026#39;Count\u0026#39; }, { \u0026#39;MetricName\u0026#39;: \u0026#39;ProcessingErrors\u0026#39;, \u0026#39;Value\u0026#39;: errors, \u0026#39;Unit\u0026#39;: \u0026#39;Count\u0026#39; }, { \u0026#39;MetricName\u0026#39;: \u0026#39;ProcessingDuration\u0026#39;, \u0026#39;Value\u0026#39;: duration, \u0026#39;Unit\u0026#39;: \u0026#39;Seconds\u0026#39; } ] ) except Exception as e: logger.error(f\u0026#34;Failed to send metrics: {e}\u0026#34;) Exercise 5: Cost Monitoring Step 1: Track Free Tier Usage Monitor AWS Free Tier limits:\n Billing console: Navigate to AWS Billing Free Tier: Check usage against limits Key metrics:  Lambda invocations: 1M/month limit Lambda compute time: 400,000 GB-seconds/month CloudWatch logs: 5 GB/month CloudWatch metrics: 10 custom metrics    Screenshot Location: Add screenshot of AWS Free Tier usage dashboard\n\rStep 2: Cost Optimization Strategies Reduce costs while maintaining functionality:\nCost Optimization Checklist:\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ Area â”‚ Current â”‚ Optimized â”‚\râ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\râ”‚ Lambda Memory â”‚ 256 MB â”‚ 128 MB â”‚\râ”‚ Lambda Timeout â”‚ 60 seconds â”‚ 30 seconds â”‚\râ”‚ Batch Size â”‚ 1 record â”‚ 10 records â”‚\râ”‚ Log Retention â”‚ Never expire â”‚ 7 days â”‚\râ”‚ Custom Metrics â”‚ Unlimited â”‚ Essential only â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\rStep 3: Set Billing Alerts Prevent unexpected charges:\n CloudWatch alarm: For estimated charges Threshold: $1.00 (early warning) Actions: Email notification Monthly budget: Set $5 limit with alerts  Exercise 6: Production Readiness Checklist for Production Deployment Infrastructure readiness:\n âœ… Monitoring: Comprehensive dashboard setup âœ… Alerting: Critical alerts configured âœ… Error handling: Robust error processing âœ… Performance: Optimized for workload âœ… Security: Proper IAM permissions âœ… Cost control: Billing alerts active  Operational Procedures Standard operating procedures:\n Daily monitoring: Check dashboard health Weekly optimization: Review performance metrics Monthly cost review: Analyze spending trends Incident response: Follow alert procedures Capacity planning: Monitor growth trends  Documentation Requirements Essential documentation:\n Architecture diagrams Runbook procedures Alert response guides Performance baselines Cost optimization notes  Screenshot Location: Add screenshot of complete monitoring dashboard with all metrics and health indicators\n\rTroubleshooting Guide Quick Reference Common issues and solutions:\n   Issue Symptoms Quick Fix     No processing No Lambda invocations Check event source mapping   High errors Error rate \u0026gt; 0% Review CloudWatch logs   Slow processing High duration Optimize code/increase memory   High costs Exceeding Free Tier Reduce batch size/frequency   Missing alerts No notifications Verify SNS subscriptions    Advanced Debugging Deep dive investigation:\n X-Ray tracing: Enable for detailed function analysis VPC flow logs: Network connectivity issues CloudTrail: API call investigation Config rules: Compliance monitoring  Exercise Summary You\u0026rsquo;ve mastered stream processing operations:\n âœ… Comprehensive monitoring with dashboards and metrics âœ… Proactive alerting for critical issues âœ… Debugging techniques for common problems âœ… Performance optimization for efficiency âœ… Cost monitoring and Free Tier management âœ… Production readiness with operational procedures  Operations Excellence: You can now confidently operate DynamoDB Streams and Lambda processing in production!\n\rNext Steps With stream processing mastered, you\u0026rsquo;re ready to tackle advanced monitoring and optimization patterns in Module 5, where we\u0026rsquo;ll build comprehensive dashboards for the entire DynamoDB ecosystem.\n"
},
{
	"uri": "http://localhost:1313/5-monitoring-optimization/5.1-cloudwatch-dashboards/",
	"title": "5.1 CloudWatch Dashboards",
	"tags": [],
	"description": "",
	"content": "Visual Monitoring Setup ğŸ“Š Create comprehensive CloudWatch dashboards for real-time DynamoDB monitoring\nOverview CloudWatch dashboards provide visual insights into your DynamoDB performance, capacity utilization, and system health. Build production-ready monitoring that helps you detect issues before they impact users.\nDashboard Strategy Multi-layered monitoring approach:\n Executive Dashboard: High-level health indicators Operations Dashboard: Detailed metrics for daily monitoring Troubleshooting Dashboard: Deep-dive analysis views  Exercise 1: Create Primary Monitoring Dashboard Step 1: Access CloudWatch Console Navigate to dashboard creation:\n AWS Console: Search \u0026ldquo;CloudWatch\u0026rdquo; Dashboards: Click \u0026ldquo;Dashboards\u0026rdquo; in left sidebar Create dashboard: Click \u0026ldquo;Create dashboard\u0026rdquo; button Dashboard name: DynamoDB-Production-Monitoring Create dashboard: Click to proceed  Screenshot Location: Add screenshot of CloudWatch console with Dashboards section highlighted\n\rStep 2: Add DynamoDB Capacity Widget Create capacity utilization monitoring:\n  Add widget: Click \u0026ldquo;Add widget\u0026rdquo; button\n  Widget type: Select \u0026ldquo;Line\u0026rdquo; chart\n  Data source: Choose \u0026ldquo;Metrics\u0026rdquo;\n  Browse metrics:\n AWS/DynamoDB â†’ Table Metrics Table: Select demo-ecommerce-freetier Metrics: Check all these:  ConsumedReadCapacityUnits ConsumedWriteCapacityUnits ProvisionedReadCapacityUnits ProvisionedWriteCapacityUnits      Configure widget:\n Title: \u0026ldquo;DynamoDB Capacity Utilization\u0026rdquo; Period: 5 minutes Statistic: Average Y-axis range: Auto    Screenshot Location: Add screenshot of widget configuration with DynamoDB capacity metrics selected\n\rStep 3: Add Performance Metrics Widget Monitor response times and errors:\n  Add widget: Click \u0026ldquo;Add widget\u0026rdquo; (second widget)\n  Widget type: Line chart\n  Metrics selection:\n AWS/DynamoDB â†’ Table Metrics Table: demo-ecommerce-freetier Select metrics:  SuccessfulRequestLatency (Query operations) SuccessfulRequestLatency (GetItem operations) UserErrors SystemErrors ThrottledRequests      Widget configuration:\n Title: \u0026ldquo;Performance \u0026amp; Error Metrics\u0026rdquo; Period: 5 minutes Y-axis: Split axis (latency vs errors)    Screenshot Location: Add screenshot of performance metrics widget configuration\n\rStep 4: Add Storage and Item Count Widget Track data growth and storage usage:\n  Add widget: Third widget\n  Widget type: Line chart\n  Metrics:\n AWS/DynamoDB â†’ Table Metrics Table: demo-ecommerce-freetier Select:  ItemCount TableSizeBytes      Configuration:\n Title: \u0026ldquo;Storage Utilization\u0026rdquo; Period: 1 hour (slower changing metrics) Y-axis: Split axis (count vs bytes)    Screenshot Location: Add screenshot of storage metrics configuration showing ItemCount and TableSizeBytes\n\rStep 5: Add Lambda Stream Processing Widget Monitor stream processing performance:\n  Add widget: Fourth widget\n  Metrics:\n AWS/Lambda â†’ Function Metrics Function: demo-dynamodb-stream-processor Select:  Invocations Duration Errors Throttles      Configuration:\n Title: \u0026ldquo;Stream Processing Health\u0026rdquo; Period: 5 minutes    Screenshot Location: Add screenshot of Lambda metrics widget showing stream processing performance\n\rExercise 2: Global Tables Monitoring (if applicable) Step 1: Add Replication Metrics Monitor cross-region replication:\n  Add widget: Fifth widget\n  Metrics:\n AWS/DynamoDB â†’ Global Table Metrics Source/Destination: Your regions Select:  ReplicationLatency PendingReplicationCount      Configuration:\n Title: \u0026ldquo;Global Tables Replication\u0026rdquo; Period: 5 minutes    Screenshot Location: Add screenshot of Global Tables replication metrics configuration\n\rExercise 3: Dashboard Customization Step 1: Organize Widget Layout Optimize dashboard layout:\n Resize widgets: Drag corners to adjust size Arrange layout:  Top row: Capacity + Performance (most critical) Second row: Storage + Lambda processing Third row: Global Tables (if applicable)   Widget spacing: Leave room for annotations  Screenshot Location: Add screenshot of final dashboard layout with all widgets organized\n\rStep 2: Configure Time Range and Auto-Refresh Set operational parameters:\n Time range: Set to \u0026ldquo;Last 3 hours\u0026rdquo; Auto-refresh: Enable 1-minute refresh Time zone: Set to your local time zone Save dashboard: Click \u0026ldquo;Save dashboard\u0026rdquo;  Step 3: Add Dashboard Annotations Add context and documentation:\n Add widget: Text widget Widget content:  # DynamoDB Production Dashboard ## Key Metrics: - **Capacity**: Target \u0026lt;80% utilization - **Latency**: Target \u0026lt;50ms average - **Errors**: Target 0% error rate - **Storage**: Monitor 25GB Free Tier limit ## Alert Thresholds: - High capacity: \u0026gt;4 RCU/WCU (80% of 5) - High latency: \u0026gt;100ms sustained - Any errors: \u0026gt;0 errors per period Last Updated: [Date] \rScreenshot Location: Add screenshot of text widget with dashboard documentation\n\rExercise 4: Specialized Dashboards Step 1: Create Cost Monitoring Dashboard Track Free Tier usage:\n New dashboard: \u0026ldquo;DynamoDB-Cost-Analysis\u0026rdquo; Add widgets:  Billing metrics: EstimatedCharges Usage metrics: Free Tier consumption Capacity trends: Weekly capacity usage Storage growth: Data size over time    Step 2: Create Troubleshooting Dashboard Deep-dive analysis dashboard:\n Dashboard name: \u0026ldquo;DynamoDB-Troubleshooting\u0026rdquo; Detailed metrics:  Per-operation latency: Broken down by operation type Hot partition detection: ConsumedCapacity by partition Error analysis: Error types and frequencies Throttling patterns: When and why throttling occurs    Screenshot Location: Add screenshot of troubleshooting dashboard with detailed operational metrics\n\rExercise 5: Dashboard Sharing and Access Step 1: Configure Dashboard Permissions Set up team access:\n Dashboard actions: Click \u0026ldquo;Actions\u0026rdquo; menu Share dashboard: Select sharing options Access controls:  Public: No (keep private) Account access: Specific IAM users/roles Read-only: Recommended for most users    Step 2: Create Dashboard URLs Generate shareable links:\n Get shareable URL: Copy dashboard URL Embed options: For inclusion in other tools Export options: PNG/PDF for reports  Screenshot Location: Add screenshot of dashboard sharing configuration options\n\rExercise 6: Dashboard Maintenance Step 1: Set Up Dashboard Alerts Monitor dashboard health:\n Missing data alerts: When metrics stop reporting Dashboard access logs: Who is viewing dashboards Widget performance: Load times and responsiveness  Step 2: Regular Review Process Dashboard optimization schedule:\n Daily: Quick health check of all dashboards Weekly: Review metric relevance and thresholds Monthly: Optimize layout and add new metrics Quarterly: Archive unused dashboards  Dashboard Best Practices Widget Organization Effective dashboard design:\nDashboard Layout Strategy:\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ Critical Metrics â”‚ Performance Trends â”‚\râ”‚ (Capacity, Errors) â”‚ (Latency, Success) â”‚\râ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\râ”‚ Resource Usage â”‚ Stream Processing â”‚\râ”‚ (Storage, Items) â”‚ (Lambda, Events) â”‚\râ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\râ”‚ Global Tables \u0026amp; Replication (if applicable)â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\rColor Coding Strategy Visual consistency:\n Green: Normal/healthy metrics Yellow: Warning thresholds approached Red: Critical issues requiring attention Blue: Informational/baseline metrics  Metric Selection Guidelines Choose the right metrics:\n   Purpose Primary Metrics Secondary Metrics     Health Errors, Throttles Success rate, Availability   Performance Latency, Duration Throughput, Concurrency   Capacity Consumed vs Provisioned Utilization percentage   Cost Storage size, Operations Free Tier consumption    Troubleshooting Dashboard Issues Common Problems Dashboard not loading:\n Check region: Verify correct AWS region Permissions: Ensure CloudWatch read access Browser cache: Clear cache and reload Service status: Check AWS status page  Missing metrics data:\n Time range: Adjust time window Metric delay: Wait for data propagation Resource activity: Generate activity to create metrics Metric retention: Check data retention periods  Performance issues:\n Too many widgets: Reduce widget count Long time ranges: Shorten time periods High resolution: Use lower resolution for overview Auto-refresh: Reduce refresh frequency  Free Tier Note: CloudWatch provides 10 custom metrics and 3 dashboards free. Plan your dashboard strategy accordingly.\n\rExercise Summary You\u0026rsquo;ve created comprehensive monitoring dashboards:\n âœ… Primary dashboard with essential DynamoDB metrics âœ… Performance monitoring for latency and errors âœ… Capacity tracking to prevent throttling âœ… Storage monitoring for cost management âœ… Stream processing health visibility âœ… Specialized dashboards for cost and troubleshooting  Dashboard Mastery: You now have production-grade visual monitoring that provides complete visibility into your DynamoDB system!\n\rNext Steps With dashboards providing visual monitoring, the next step is setting up proactive alerting to detect issues before they impact users. We\u0026rsquo;ll configure intelligent alarms and notification systems.\n"
},
{
	"uri": "http://localhost:1313/5-monitoring-optimization/5.2-alerting-notifications/",
	"title": "5.2 Alerting &amp; Notifications",
	"tags": [],
	"description": "",
	"content": "Proactive Issue Detection ğŸš¨ Set up intelligent CloudWatch alarms and notification systems for proactive monitoring\nOverview Proactive alerting prevents small issues from becoming big problems. Configure smart alarms that notify you before users experience problems, enabling quick response and maintaining system reliability.\nAlerting Strategy Multi-tiered alerting approach:\n Critical: Immediate attention required (system down) Warning: Issues developing (capacity approaching limits) Informational: Trends worth monitoring (usage patterns)  Alert Design Principles Effective alerting characteristics:\n Actionable: Every alert should have a clear response Timely: Detect issues before user impact Accurate: Minimize false positives Prioritized: Critical alerts stand out  Exercise 1: SNS Topic Setup Step 1: Create Notification Topic Set up central notification system:\n AWS Console: Search \u0026ldquo;SNS\u0026rdquo; (Simple Notification Service) Topics: Click \u0026ldquo;Topics\u0026rdquo; in left sidebar Create topic: Click \u0026ldquo;Create topic\u0026rdquo; Topic configuration:  Type: Standard Name: DynamoDB-Alerts Display name: \u0026ldquo;DynamoDB Production Alerts\u0026rdquo;   Create topic: Click to create  Screenshot Location: Add screenshot of SNS topic creation with DynamoDB-Alerts configuration\n\rStep 2: Add Email Subscription Configure email notifications:\n Topic details: Open your created topic Subscriptions: Click \u0026ldquo;Create subscription\u0026rdquo; Subscription configuration:  Protocol: Email Endpoint: Your email address   Create subscription: Click to create Confirm subscription: Check email and click confirmation link  Screenshot Location: Add screenshot of email subscription configuration and confirmation process\n\rStep 3: Test Notification System Verify notification setup:\n Topic actions: Click \u0026ldquo;Publish message\u0026rdquo; Test message:  Subject: \u0026ldquo;DynamoDB Alert System Test\u0026rdquo; Message: \u0026ldquo;This is a test of the DynamoDB alerting system. Please ignore.\u0026rdquo;   Publish: Send test message Verify: Check email reception  Screenshot Location: Add screenshot of test message publication and email notification received\n\rExercise 2: Critical Capacity Alerts Step 1: High Read Capacity Alarm Prevent read throttling:\n  CloudWatch: Navigate to CloudWatch console\n  Alarms: Click \u0026ldquo;Alarms\u0026rdquo; â†’ \u0026ldquo;All alarms\u0026rdquo;\n  Create alarm: Click \u0026ldquo;Create alarm\u0026rdquo;\n  Select metric:\n AWS/DynamoDB â†’ Table Metrics Table: demo-ecommerce-freetier Metric: ConsumedReadCapacityUnits    Define conditions:\n Statistic: Average Period: 5 minutes Threshold type: Static Condition: Greater than 4 (80% of 5 RCU limit) Datapoints to alarm: 2 out of 3    Screenshot Location: Add screenshot of read capacity alarm configuration with threshold settings\n\rStep 2: Configure Alarm Actions Set up notification actions:\n Configure actions:  Alarm state trigger: In alarm SNS topic: Select DynamoDB-Alerts OK state trigger: In alarm (recovery notification)   Add description:  Name: \u0026ldquo;DynamoDB-HighReadCapacity\u0026rdquo; Description: \u0026ldquo;Read capacity utilization exceeds 80% threshold\u0026rdquo;    Step 3: High Write Capacity Alarm Prevent write throttling:\n Create alarm: Follow similar process Metric: ConsumedWriteCapacityUnits Threshold: Greater than 4 (80% of 5 WCU) Name: \u0026ldquo;DynamoDB-HighWriteCapacity\u0026rdquo;  Screenshot Location: Add screenshot of write capacity alarm configuration\n\rExercise 3: Performance and Error Alerts Step 1: High Latency Alert Monitor response time degradation:\n  Create alarm: New alarm\n  Metric:\n AWS/DynamoDB â†’ Table Metrics Table: demo-ecommerce-freetier Metric: SuccessfulRequestLatency (Query operations)    Conditions:\n Statistic: Average Period: 5 minutes Threshold: Greater than 100 milliseconds Datapoints: 3 out of 3 (avoid false alarms)    Configuration:\n Name: \u0026ldquo;DynamoDB-HighLatency\u0026rdquo; Description: \u0026ldquo;Query latency exceeds 100ms threshold\u0026rdquo;    Screenshot Location: Add screenshot of latency alarm showing threshold and datapoint configuration\n\rStep 2: Error Rate Alert Detect application errors:\n  Create alarm: Error monitoring\n  Metric: UserErrors\n  Conditions:\n Statistic: Sum Period: 5 minutes Threshold: Greater than or equal to 1 Datapoints: 1 out of 1 (immediate alert)    Name: \u0026ldquo;DynamoDB-UserErrors\u0026rdquo;\n  Step 3: Throttling Alert Critical throttling detection:\n  Metric: ThrottledRequests\n  Conditions:\n Threshold: Greater than or equal to 1 Period: 1 minute Immediate: 1 out of 1    Name: \u0026ldquo;DynamoDB-Throttling-CRITICAL\u0026rdquo;\n  Screenshot Location: Add screenshot of throttling alarm configuration with critical priority\n\rExercise 4: Lambda Stream Processing Alerts Step 1: Lambda Error Alert Monitor stream processing failures:\n  Create alarm: Lambda function monitoring\n  Metric:\n AWS/Lambda â†’ Function Metrics Function: demo-dynamodb-stream-processor Metric: Errors    Conditions:\n Threshold: Greater than or equal to 1 Period: 5 minutes Datapoints: 1 out of 1    Name: \u0026ldquo;Lambda-StreamProcessor-Errors\u0026rdquo;\n  Screenshot Location: Add screenshot of Lambda error alarm configuration\n\rStep 2: Lambda Duration Alert Detect processing performance issues:\n  Metric: Duration\n  Conditions:\n Statistic: Average Threshold: Greater than 20000 milliseconds (20 seconds) Period: 5 minutes    Name: \u0026ldquo;Lambda-StreamProcessor-SlowExecution\u0026rdquo;\n  Step 3: Iterator Age Alert (Advanced) Monitor stream processing lag:\n  Metric: DynamoDB Streams IteratorAge\n  Conditions:\n Threshold: Greater than 30000 milliseconds (30 seconds) Period: 5 minutes    Name: \u0026ldquo;DynamoDB-StreamLag\u0026rdquo;\n  Exercise 5: Cost and Storage Alerts Step 1: Storage Size Alert Free Tier limit monitoring:\n  Create alarm: Storage monitoring\n  Metric: TableSizeBytes\n  Conditions:\n Threshold: Greater than 21474836480 (20GB - 80% of Free Tier) Period: 1 hour Datapoints: 2 out of 2    Name: \u0026ldquo;DynamoDB-StorageWarning\u0026rdquo;\n  Screenshot Location: Add screenshot of storage size alarm with Free Tier threshold\n\rStep 2: Estimated Charges Alert Billing protection:\n  Create alarm: Billing monitoring\n  Metric:\n AWS/Billing â†’ Total Estimated Charge Currency: USD ServiceName: AmazonDynamoDB    Conditions:\n Threshold: Greater than 1.00 USD Period: 6 hours    Name: \u0026ldquo;DynamoDB-UnexpectedCharges\u0026rdquo;\n  Exercise 6: Advanced Alerting Patterns Step 1: Composite Alarms Multi-metric alerting:\n Create composite alarm: Combine multiple conditions Rule expression: ALARM(\u0026quot;DynamoDB-HighReadCapacity\u0026quot;) OR ALARM(\u0026quot;DynamoDB-HighWriteCapacity\u0026quot;)\r Name: \u0026ldquo;DynamoDB-CapacityIssues\u0026rdquo;  Step 2: Anomaly Detection ML-based alerting:\n Create alarm: With anomaly detection Metric: ConsumedReadCapacityUnits Anomaly detection: Enable anomaly detection Threshold: Outside of 2 standard deviations  Screenshot Location: Add screenshot of anomaly detection alarm configuration\n\rStep 3: Alarm Scheduling Time-based alert suppression:\n Maintenance windows: Suppress non-critical alerts Business hours: Different thresholds for business vs off-hours Weekend adjustments: Relaxed thresholds for expected low usage  Exercise 7: Alert Testing and Validation Step 1: Generate Test Conditions Trigger alarms safely:\n High capacity: Create multiple items rapidly Latency test: Use complex queries Error simulation: Query non-existent items  Step 2: Verify Alert Flow End-to-end testing:\n Trigger alarm: Generate alert condition Check alarm state: Verify alarm triggers Confirm notification: Check email delivery Recovery test: Verify OK state notifications  Screenshot Location: Add screenshot of alarm in ALARM state with notification sent\n\rStep 3: Response Time Measurement Alert performance metrics:\n Detection time: Condition to alarm state Notification time: Alarm to email delivery Total time: End-to-end alerting latency  Alert Management Best Practices Alarm Organization Structured approach:\nAlarm Naming Convention:\r[Service]-[Resource]-[Metric]-[Severity]\rExamples:\r- DynamoDB-Table-HighCapacity-WARNING\r- Lambda-StreamProcessor-Errors-CRITICAL\r- DynamoDB-Storage-Approaching-INFO\rEscalation Strategy Alert priority levels:\n   Priority Response Time Examples     CRITICAL Immediate Throttling, System errors   HIGH 15 minutes High latency, Capacity warnings   MEDIUM 1 hour Storage growth, Usage patterns   LOW Next business day Optimization opportunities    Alert Fatigue Prevention Maintain alerting effectiveness:\n Tune thresholds: Adjust based on actual patterns Group related alerts: Avoid duplicate notifications Suppress during maintenance: Planned outage handling Regular review: Remove obsolete alerts  Troubleshooting Alerts Common Alert Issues Alerts not triggering:\n Threshold values: Check if realistic for your usage Datapoint configuration: Too strict requirements Metric delay: CloudWatch metric propagation lag Period settings: Too short for metric collection  False positive alerts:\n Threshold tuning: Adjust based on normal patterns Datapoint requirements: Require multiple breaches Anomaly detection: Use ML-based thresholds Composite alarms: Combine multiple indicators  Missing notifications:\n SNS subscription: Verify email confirmation Email filtering: Check spam/junk folders Topic permissions: Ensure CloudWatch can publish Subscription limits: SNS subscription quotas  Free Tier Limits: CloudWatch allows 10 alarms in the Free Tier. Plan your most critical alerts carefully.\n\rExercise Summary You\u0026rsquo;ve built a comprehensive alerting system:\n âœ… SNS notification system for centralized alerting âœ… Capacity alerts to prevent throttling âœ… Performance monitoring for latency and errors âœ… Stream processing alerts for Lambda monitoring âœ… Cost protection with billing alerts âœ… Advanced patterns with composite alarms  Alerting Excellence: Your system now detects issues proactively and notifies you before users are impacted!\n\rNext Steps With proactive alerting in place, the next step is analyzing costs and optimizing your Free Tier usage to maximize value while minimizing expenses.\n"
},
{
	"uri": "http://localhost:1313/5-monitoring-optimization/5.3-cost-analysis-optimization/",
	"title": "5.3 Cost Analysis &amp; Optimization",
	"tags": [],
	"description": "",
	"content": "Free Tier Management \u0026amp; Cost Control ğŸ’° Analyze AWS costs, optimize Free Tier usage, and implement cost control strategies\nOverview Effective cost management ensures you maximize AWS Free Tier benefits while building production-ready skills. Learn to monitor spending, optimize resource usage, and scale efficiently without unexpected charges.\nCost Analysis Strategy Multi-dimensional cost tracking:\n Service-level: DynamoDB, Lambda, CloudWatch costs Resource-level: Per-table, per-function expenses Time-based: Daily, weekly, monthly trends Feature-level: Global Tables, Streams, Backups  Free Tier Optimization Goals Maximize value within limits:\n Stay within Free Tier: Avoid any charges Learn production patterns: Real-world techniques Scale-ready design: Prepared for growth Cost visibility: Understand pricing model  Exercise 1: AWS Cost Explorer Analysis Step 1: Access Billing Dashboard Navigate to cost analysis tools:\n AWS Console: Click account name (top-right) Billing and Cost Management: Select from dropdown Cost Explorer: Click \u0026ldquo;Cost Explorer\u0026rdquo; in left sidebar Launch Cost Explorer: Click to open (may take a few minutes first time)  Screenshot Location: Add screenshot of AWS Console billing dropdown and Cost Explorer access\n\rStep 2: Analyze DynamoDB Costs Review current DynamoDB expenses:\n Time range: Set to \u0026ldquo;Last 3 months\u0026rdquo; Group by: Service Filter services: Add filter for \u0026ldquo;DynamoDB\u0026rdquo; View type: Daily costs Expected result: $0.00 for all periods (Free Tier)  Screenshot Location: Add screenshot of Cost Explorer showing DynamoDB costs at $0.00\n\rStep 3: Service Breakdown Analysis Detailed cost breakdown:\n Group by: Service and Operation Services: Include DynamoDB, Lambda, CloudWatch Analysis:  DynamoDB: Read/write operations, storage Lambda: Invocations, duration CloudWatch: Metrics, logs, alarms   Verify: All services within Free Tier limits  Screenshot Location: Add screenshot of detailed service cost breakdown with operations\n\rExercise 2: Free Tier Usage Monitoring Step 1: Access Free Tier Dashboard Check Free Tier consumption:\n Billing console: Navigate to main billing page Free Tier: Click \u0026ldquo;Free Tier\u0026rdquo; tab Service filters: Select relevant services Review usage:  DynamoDB: Read/write capacity usage Lambda: Invocations and compute time CloudWatch: Metrics and alarms    Screenshot Location: Add screenshot of Free Tier dashboard showing DynamoDB usage percentages\n\rStep 2: Analyze Usage Patterns DynamoDB Free Tier consumption:\nCurrent usage tracking:\nFree Tier Utilization Analysis:\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ Resource â”‚ Free Tier Limit â”‚ Current Usage â”‚\râ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\râ”‚ Read Capacity â”‚ 25 RCU â”‚ ~15 RCU (60%) â”‚\râ”‚ Write Capacity â”‚ 25 WCU â”‚ ~15 WCU (60%) â”‚\râ”‚ Storage â”‚ 25 GB â”‚ \u0026lt;0.1 GB (0.4%) â”‚\râ”‚ Lambda Invocations â”‚ 1M requests â”‚ \u0026lt;1K (0.1%) â”‚\râ”‚ CloudWatch Metrics â”‚ 10 metrics â”‚ 8 metrics (80%) â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\rStep 3: Set Usage Alerts Prevent Free Tier overages:\n Billing preferences: Navigate to preferences Billing alerts: Enable \u0026ldquo;Receive Free Tier Usage Alerts\u0026rdquo; Email address: Confirm notification email Usage thresholds: Set alerts at 80% of limits  Screenshot Location: Add screenshot of Free Tier alert configuration\n\rExercise 3: DynamoDB Cost Optimization Step 1: Capacity Utilization Analysis Optimize read/write capacity:\n  CloudWatch: Navigate to DynamoDB metrics\n  Capacity analysis:\n Peak usage: Identify highest consumption periods Average usage: Calculate typical utilization Efficiency: Consumed vs provisioned ratio    Optimization opportunities:\n Current setup: 5 RCU/WCU per table + GSIs Utilization: ~40-60% average usage Optimization: Well-sized for Free Tier    Screenshot Location: Add screenshot of CloudWatch showing DynamoDB capacity utilization patterns\n\rStep 2: Storage Optimization Minimize storage costs:\n  Data lifecycle analysis:\n Current storage: Check table size metrics Growth rate: Monitor size increase over time Data patterns: Identify large or unused items    Optimization strategies:\n Item design: Remove unnecessary attributes Data archiving: Move old data to S3 Compression: Efficient data formats    Step 3: Query Optimization Improve read efficiency:\n  Query analysis:\n Scan vs Query: Prefer Query operations Projection: Use GSI projections effectively Batch operations: Group requests when possible    Cost impact:\n Query: 1 RCU per 4KB read Scan: Consumes RCU for entire scan GSI queries: Use projected attributes    Screenshot Location: Add screenshot showing query performance metrics and RCU consumption\n\rExercise 4: Lambda Cost Optimization Step 1: Function Performance Analysis Optimize Lambda costs:\n  CloudWatch: Review Lambda metrics\n  Performance analysis:\n Duration: Target \u0026lt;1 second execution Memory: 128MB optimal for Free Tier Invocations: Monitor request frequency    Cost breakdown:\n Requests: 1M free per month Compute time: 400,000 GB-seconds free Current usage: \u0026lt;1% of limits    Screenshot Location: Add screenshot of Lambda cost analysis showing Free Tier usage\n\rStep 2: Code Optimization Improve function efficiency:\n# Optimized Lambda function for cost efficiency import json import boto3 import logging # Reuse connections (outside handler) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(\u0026#39;demo-ecommerce-freetier\u0026#39;) logger = logging.getLogger() logger.setLevel(logging.INFO) def lambda_handler(event, context): \u0026#34;\u0026#34;\u0026#34; Cost-optimized stream processing \u0026#34;\u0026#34;\u0026#34; try: # Process in batches for efficiency processed = 0 for record in event[\u0026#39;Records\u0026#39;]: # Minimal processing for Free Tier optimization process_efficiently(record) processed += 1 return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;processed\u0026#39;: processed } except Exception as e: logger.error(f\u0026#34;Error: {e}\u0026#34;) raise def process_efficiently(record): \u0026#34;\u0026#34;\u0026#34; Streamlined processing to minimize duration \u0026#34;\u0026#34;\u0026#34; # Essential processing only event_name = record[\u0026#39;eventName\u0026#39;] logger.info(f\u0026#34;Processed {event_name}event\u0026#34;) # Additional business logic as needed Step 3: Monitoring and Alerting Costs CloudWatch cost management:\n  Metrics optimization:\n Custom metrics: Use sparingly (10 free) Log retention: Set appropriate retention periods Dashboard widgets: Limit to essential views    Cost tracking:\n Logs: 5GB free per month Metrics: 10 custom metrics free Alarms: 10 alarms free    Exercise 5: Scaling Cost Projections Step 1: Growth Scenario Planning Project costs for scaling:\n  Current baseline: Document Free Tier usage\n  Growth scenarios:\n 10x traffic: Capacity and cost impact 100x traffic: Migration to on-demand billing Global expansion: Multi-region costs    Cost modeling:\n  Scaling Cost Projections:\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ Traffic Level â”‚ RCU/WCU â”‚ Monthly Costâ”‚ Billing Modeâ”‚\râ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\râ”‚ Current (Free) â”‚ 15/15 â”‚ $0.00 â”‚ Provisioned â”‚\râ”‚ 2x Growth â”‚ 30/30 â”‚ ~$15 â”‚ Provisioned â”‚\râ”‚ 10x Growth â”‚ 150/150 â”‚ ~$75 â”‚ On-Demand â”‚\râ”‚ 100x Growth â”‚ 1500/1500 â”‚ ~$750 â”‚ On-Demand â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\rScreenshot Location: Add screenshot of cost projection analysis in Cost Explorer\n\rStep 2: Optimization Strategies for Scale Cost-effective scaling approaches:\n  Provisioned vs On-Demand:\n Predictable traffic: Provisioned capacity Variable traffic: On-demand billing Hybrid approach: Mix both modes    Auto-scaling configuration:\n Target utilization: 70% capacity Scale-out: Quick scaling up Scale-in: Conservative scaling down    Reserved capacity (for large scale):\n Commitment discounts: Up to 76% savings Predictable workloads: Best for steady traffic    Exercise 6: Cost Governance Setup Step 1: Budget Creation Set spending limits:\n  AWS Budgets: Navigate to Budgets service\n  Create budget: Click \u0026ldquo;Create budget\u0026rdquo;\n  Budget configuration:\n Budget type: Cost budget Time range: Monthly Budget amount: $5.00 (safety buffer) Filters: DynamoDB service    Alert thresholds:\n 80% threshold: Warning alert 100% threshold: Critical alert    Screenshot Location: Add screenshot of budget creation with DynamoDB-specific filters\n\rStep 2: Cost Anomaly Detection Automated cost monitoring:\n  Cost Anomaly Detection: Enable service\n  Detection configuration:\n Services: DynamoDB, Lambda, CloudWatch Sensitivity: High (detect small anomalies) Notifications: Email alerts    Machine learning: AWS analyzes patterns automatically\n  Step 3: Resource Tagging Strategy Cost allocation and tracking:\n  Tagging strategy:\nTag Structure:\r- Environment: workshop/dev/prod\r- Project: dynamodb-learning\r- Owner: [your-name]\r- Cost-Center: education\r  Tag-based budgets: Create budgets by tag values\n  Cost allocation: Track costs by project/owner\n  Exercise 7: Optimization Recommendations Step 1: Right-Sizing Analysis Capacity optimization:\n  Current efficiency:\n Table utilization: 40-60% average GSI utilization: 20-40% average Lambda efficiency: \u0026lt;1 second execution    Optimization actions:\n Maintain current sizing: Well-optimized for Free Tier Monitor growth: Scale when approaching limits Query optimization: Continue efficient patterns    Step 2: Feature Optimization Service feature cost analysis:\n  Global Tables:\n Cost: Same as single-region within Free Tier Value: Multi-region availability Recommendation: Keep enabled for learning    DynamoDB Streams:\n Cost: Free feature Value: Real-time processing capability Recommendation: Continue using    Backups:\n Continuous backups: Free On-demand backups: First 10GB free monthly    Screenshot Location: Add screenshot of feature-by-feature cost analysis\n\rStep 3: Long-term Strategy Scaling preparation:\n Monitoring foundation: Already established Cost visibility: Comprehensive tracking in place Optimization patterns: Learned efficient practices Scaling readiness: Understand cost implications  Cost Optimization Best Practices Resource Efficiency Maximize Free Tier value:\n Batch operations: Group requests for efficiency Efficient queries: Use Query instead of Scan Right-size capacity: Match provisioning to usage Data lifecycle: Archive old data appropriately  Monitoring and Alerting Cost control automation:\n Usage alerts: Set at 80% of Free Tier limits Budget notifications: Multiple threshold levels Anomaly detection: Automated unusual usage detection Regular reviews: Monthly cost analysis  Scaling Economics Cost-effective growth:\n Gradual scaling: Increase capacity incrementally Performance testing: Validate before scaling Alternative billing: Consider on-demand for variable loads Reserved capacity: For predictable, large-scale workloads  Cost Safety: Always monitor Free Tier usage closely and set up billing alerts to prevent unexpected charges.\n\rExercise Summary You\u0026rsquo;ve mastered cost analysis and optimization:\n âœ… Cost Explorer analysis for detailed spending insights âœ… Free Tier monitoring to maximize benefits âœ… Resource optimization for efficient utilization âœ… Scaling projections for growth planning âœ… Cost governance with budgets and alerts âœ… Best practices for long-term cost management  Cost Mastery: You can now operate DynamoDB efficiently within Free Tier limits while preparing for cost-effective scaling!\n\rNext Steps With cost optimization mastered, the final section covers performance tuning techniques to maximize efficiency and prepare your system for production-scale operations.\n"
},
{
	"uri": "http://localhost:1313/5-monitoring-optimization/5.4-performance-tuning/",
	"title": "5.4 Performance Tuning",
	"tags": [],
	"description": "",
	"content": "System Performance Optimization âš¡ Advanced techniques for maximizing DynamoDB performance and efficiency\nOverview Performance tuning transforms good systems into great ones. Learn advanced optimization techniques that improve response times, increase throughput, and prepare your system for production-scale performance requirements.\nPerformance Optimization Framework Multi-dimensional approach:\n Data Model: Optimize table and index design Access Patterns: Efficient query and scan strategies Capacity Management: Right-size provisioned throughput Application Patterns: Implement best practices in code  Performance Metrics to Optimize Key performance indicators:\n Latency: Target \u0026lt;50ms for critical operations Throughput: Maximize operations per second Efficiency: Optimize RCU/WCU consumption Consistency: Balance performance with data consistency  Exercise 1: Query Performance Analysis Step 1: Baseline Performance Measurement Establish current performance metrics:\n  CloudWatch: Navigate to DynamoDB metrics\n  Performance analysis:\n SuccessfulRequestLatency: Current average latency ConsumedReadCapacityUnits: Read efficiency ConsumedWriteCapacityUnits: Write efficiency ItemCount: Data volume impact    Baseline documentation:\nCurrent Performance Baseline:\r- Average Query Latency: ~15-25ms\r- Average GetItem Latency: ~5-10ms\r- Read Capacity Efficiency: 40-60%\r- Write Capacity Efficiency: 30-50%\r  Screenshot Location: Add screenshot of CloudWatch showing current performance baseline metrics\n\rStep 2: Query Pattern Analysis Evaluate access pattern efficiency:\n DynamoDB console: Navigate to table items Test different access patterns:  Pattern 1: Efficient Query (GSI1)\nGSI1PK = \u0026quot;USER#john@example.com\u0026quot;\rGSI1SK begins_with \u0026quot;ORDER#\u0026quot;\rPattern 2: Inefficient Scan\nScan entire table for items where status = \u0026quot;pending\u0026quot;\rPattern 3: Optimized Query (GSI2)\nGSI2PK = \u0026quot;STATUS#pending\u0026quot;\rGSI2SK begins_with \u0026quot;ORDER#\u0026quot;\rPerformance comparison:  Query vs Scan: 10x+ performance difference GSI usage: 5x more efficient than table scan Key design: Well-designed keys = better performance    Screenshot Location: Add screenshot comparing query performance metrics between efficient and inefficient patterns\n\rStep 3: Index Utilization Optimization Maximize GSI efficiency:\n  GSI performance analysis:\n GSI1 (User-centric): High utilization, efficient queries GSI2 (Status-based): Medium utilization, good for reporting GSI3 (Category-based): Low utilization, specialized queries    Optimization opportunities:\n Projection optimization: Include frequently accessed attributes Capacity allocation: Right-size GSI capacity independently Query patterns: Design application to favor GSI queries    Exercise 2: Capacity Optimization Step 1: Capacity Utilization Analysis Analyze current capacity patterns:\n  CloudWatch metrics: Review capacity consumption\n  Pattern identification:\n Peak usage times: When capacity spikes occur Utilization efficiency: Consumed vs provisioned ratio Growth trends: How usage changes over time    Current optimization status:\nCapacity Analysis:\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ Resource â”‚ Provisioned â”‚ Peak Used â”‚ Efficiency â”‚\râ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\râ”‚ Table RCU â”‚ 5 â”‚ 3 â”‚ 60% â”‚\râ”‚ Table WCU â”‚ 5 â”‚ 4 â”‚ 80% â”‚\râ”‚ GSI1 RCU â”‚ 5 â”‚ 2 â”‚ 40% â”‚\râ”‚ GSI1 WCU â”‚ 5 â”‚ 1 â”‚ 20% â”‚\râ”‚ GSI2 RCU â”‚ 5 â”‚ 1 â”‚ 20% â”‚\râ”‚ GSI2 WCU â”‚ 5 â”‚ 2 â”‚ 40% â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r  Screenshot Location: Add screenshot of capacity utilization analysis showing current efficiency ratios\n\rStep 2: Hot Partition Detection Identify and resolve hot partitions:\n  Partition analysis:\n Access patterns: Even distribution across partition keys Key design: Avoid sequential or predictable keys Load distribution: Monitor for uneven access    Hot partition indicators:\n Throttling on specific keys: Some items throttled, others not Uneven capacity consumption: Some partitions overloaded Performance variance: Inconsistent response times    Resolution strategies:\n Key design: Use composite keys for distribution Write sharding: Add random suffix to keys Time-based partitioning: Distribute by time periods    Step 3: Burst Capacity Management Optimize burst capacity usage:\n  Understanding burst capacity:\n Automatic scaling: DynamoDB provides burst capacity Duration: Available for short periods Best practices: Don\u0026rsquo;t rely on burst for sustained traffic    Optimization strategies:\n Smooth traffic: Avoid sudden spikes Batch operations: Group requests efficiently Rate limiting: Control application request rates    Screenshot Location: Add screenshot showing burst capacity utilization patterns\n\rExercise 3: Application-Level Optimizations Step 1: Connection and SDK Optimization Optimize AWS SDK usage:\nimport boto3 from boto3.dynamodb.conditions import Key, Attr import time from botocore.config import Config # Optimized DynamoDB client configuration config = Config( retries={ \u0026#39;max_attempts\u0026#39;: 3, \u0026#39;mode\u0026#39;: \u0026#39;adaptive\u0026#39; }, max_pool_connections=50 ) # Reuse clients (important for performance) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;, config=config) table = dynamodb.Table(\u0026#39;demo-ecommerce-freetier\u0026#39;) class PerformanceOptimizer: def __init__(self, table): self.table = table def efficient_batch_get(self, keys): \u0026#34;\u0026#34;\u0026#34; Optimized batch get operation \u0026#34;\u0026#34;\u0026#34; try: response = self.table.batch_get_item( RequestItems={ self.table.table_name: { \u0026#39;Keys\u0026#39;: keys } } ) return response[\u0026#39;Responses\u0026#39;][self.table.table_name] except Exception as e: print(f\u0026#34;Batch get error: {e}\u0026#34;) return [] def efficient_query_with_pagination(self, pk_value, limit=100): \u0026#34;\u0026#34;\u0026#34; Paginated query with performance optimization \u0026#34;\u0026#34;\u0026#34; results = [] last_key = None while True: query_params = { \u0026#39;KeyConditionExpression\u0026#39;: Key(\u0026#39;PK\u0026#39;).eq(pk_value), \u0026#39;Limit\u0026#39;: limit } if last_key: query_params[\u0026#39;ExclusiveStartKey\u0026#39;] = last_key response = self.table.query(**query_params) results.extend(response[\u0026#39;Items\u0026#39;]) last_key = response.get(\u0026#39;LastEvaluatedKey\u0026#39;) if not last_key: break return results def batch_write_optimized(self, items): \u0026#34;\u0026#34;\u0026#34; Optimized batch write with retry logic \u0026#34;\u0026#34;\u0026#34; with self.table.batch_writer() as batch: for item in items: batch.put_item(Item=item) return len(items) \rScreenshot Location: Add screenshot of performance monitoring showing optimized vs unoptimized operations\n\rStep 2: Caching Strategy Implementation Implement intelligent caching:\n  Caching patterns:\n Read-through: Cache miss triggers database read Write-through: Update cache on every write Write-behind: Async cache updates    Cache implementation (conceptual):\n  import time from typing import Dict, Any, Optional class DynamoDBCache: def __init__(self, ttl_seconds=300): self.cache = {} self.ttl = ttl_seconds def get_with_cache(self, pk, sk): \u0026#34;\u0026#34;\u0026#34; Get item with caching layer \u0026#34;\u0026#34;\u0026#34; cache_key = f\u0026#34;{pk}#{sk}\u0026#34; # Check cache first if cache_key in self.cache: cached_item, timestamp = self.cache[cache_key] if time.time() - timestamp \u0026lt; self.ttl: return cached_item # Cache miss - fetch from DynamoDB response = table.get_item( Key={\u0026#39;PK\u0026#39;: pk, \u0026#39;SK\u0026#39;: sk} ) item = response.get(\u0026#39;Item\u0026#39;) if item: # Store in cache self.cache[cache_key] = (item, time.time()) return item def invalidate_cache(self, pk, sk): \u0026#34;\u0026#34;\u0026#34; Remove item from cache on update \u0026#34;\u0026#34;\u0026#34; cache_key = f\u0026#34;{pk}#{sk}\u0026#34; self.cache.pop(cache_key, None) Step 3: Connection Pooling and Retry Logic Optimize connection management:\n Connection pooling: Reuse connections across requests Retry strategies: Implement exponential backoff Circuit breaker: Fail fast when service unavailable Timeout optimization: Set appropriate timeouts  Exercise 4: Advanced Query Optimization Step 1: Projection Optimization Minimize data transfer:\n Projection expressions: Fetch only needed attributes GSI projections: Include frequently accessed attributes Network efficiency: Reduce payload sizes  Example optimizations:\n# Unoptimized: Fetch entire item response = table.get_item( Key={\u0026#39;PK\u0026#39;: \u0026#39;USER#123\u0026#39;, \u0026#39;SK\u0026#39;: \u0026#39;PROFILE\u0026#39;} ) # Optimized: Fetch only needed attributes response = table.get_item( Key={\u0026#39;PK\u0026#39;: \u0026#39;USER#123\u0026#39;, \u0026#39;SK\u0026#39;: \u0026#39;PROFILE\u0026#39;}, ProjectionExpression=\u0026#39;#name, email, last_login\u0026#39;, ExpressionAttributeNames={\u0026#39;#name\u0026#39;: \u0026#39;name\u0026#39;} ) # GSI Query optimization response = table.query( IndexName=\u0026#39;GSI1\u0026#39;, KeyConditionExpression=Key(\u0026#39;GSI1PK\u0026#39;).eq(\u0026#39;USER#john@example.com\u0026#39;), ProjectionExpression=\u0026#39;order_id, status, total_amount\u0026#39; ) \rScreenshot Location: Add screenshot showing network efficiency improvements with projection expressions\n\rStep 2: Filter Expression Optimization Efficient filtering strategies:\n Server-side filtering: Use FilterExpression appropriately Key condition vs filter: Understand RCU consumption Composite key design: Move filters to key conditions  Optimization examples:\n# Less efficient: Filter after query response = table.query( KeyConditionExpression=Key(\u0026#39;PK\u0026#39;).eq(\u0026#39;USER#123\u0026#39;), FilterExpression=Attr(\u0026#39;status\u0026#39;).eq(\u0026#39;active\u0026#39;) ) # More efficient: Include status in key design response = table.query( IndexName=\u0026#39;GSI2\u0026#39;, KeyConditionExpression=Key(\u0026#39;GSI2PK\u0026#39;).eq(\u0026#39;STATUS#active\u0026#39;) \u0026amp; Key(\u0026#39;GSI2SK\u0026#39;).begins_with(\u0026#39;USER#123\u0026#39;) ) Step 3: Parallel Processing Implement parallel scan/query:\n Parallel scan: Divide table into segments Concurrent queries: Process multiple partitions Thread pooling: Optimize concurrent operations  import concurrent.futures import boto3 def parallel_scan_segments(table_name, total_segments=4): \u0026#34;\u0026#34;\u0026#34; Parallel scan implementation \u0026#34;\u0026#34;\u0026#34; dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(table_name) def scan_segment(segment): response = table.scan( Segment=segment, TotalSegments=total_segments ) return response[\u0026#39;Items\u0026#39;] # Execute segments in parallel with concurrent.futures.ThreadPoolExecutor(max_workers=total_segments) as executor: futures = [executor.submit(scan_segment, i) for i in range(total_segments)] results = [] for future in concurrent.futures.as_completed(futures): results.extend(future.result()) return results Exercise 5: Performance Testing and Validation Step 1: Load Testing Setup Performance testing strategy:\n  Test scenarios:\n Single item operations: GetItem, PutItem performance Batch operations: BatchGetItem, BatchWriteItem efficiency Query operations: Various access patterns Mixed workloads: Realistic usage patterns    Test implementation:\n  import time import statistics from concurrent.futures import ThreadPoolExecutor class PerformanceTester: def __init__(self, table): self.table = table self.results = [] def test_get_item_performance(self, test_keys, iterations=100): \u0026#34;\u0026#34;\u0026#34; Test GetItem performance \u0026#34;\u0026#34;\u0026#34; latencies = [] for _ in range(iterations): start_time = time.time() # Perform GetItem operation response = self.table.get_item( Key=test_keys[0] # Use first test key ) end_time = time.time() latency = (end_time - start_time) * 1000 # Convert to ms latencies.append(latency) return { \u0026#39;avg_latency\u0026#39;: statistics.mean(latencies), \u0026#39;median_latency\u0026#39;: statistics.median(latencies), \u0026#39;p95_latency\u0026#39;: self.percentile(latencies, 95), \u0026#39;min_latency\u0026#39;: min(latencies), \u0026#39;max_latency\u0026#39;: max(latencies) } def percentile(self, data, percentile): \u0026#34;\u0026#34;\u0026#34;Calculate percentile\u0026#34;\u0026#34;\u0026#34; size = len(data) return sorted(data)[int(size * percentile / 100)] \rScreenshot Location: Add screenshot of performance test results showing latency distribution\n\rStep 2: Benchmark Analysis Performance benchmark results:\nPerformance Test Results:\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ Operation Type â”‚ Avg Latency â”‚ P95 Latency â”‚ Throughput â”‚\râ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\râ”‚ GetItem â”‚ 8ms â”‚ 15ms â”‚ 125 ops/sec â”‚\râ”‚ PutItem â”‚ 12ms â”‚ 25ms â”‚ 83 ops/sec â”‚\râ”‚ Query (Simple) â”‚ 15ms â”‚ 30ms â”‚ 67 ops/sec â”‚\râ”‚ Query (GSI) â”‚ 18ms â”‚ 35ms â”‚ 56 ops/sec â”‚\râ”‚ BatchGetItem (10) â”‚ 25ms â”‚ 50ms â”‚ 400 items/s â”‚\râ”‚ BatchWriteItem (10) â”‚ 35ms â”‚ 70ms â”‚ 285 items/s â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\rStep 3: Performance Optimization Validation Measure optimization impact:\n Before/after comparison: Document improvements A/B testing: Compare optimization approaches Production validation: Test with realistic loads  Exercise 6: Production Readiness Step 1: Performance SLA Definition Define performance targets:\n  Response time SLAs:\n GetItem: \u0026lt;10ms average, \u0026lt;25ms P95 Query: \u0026lt;20ms average, \u0026lt;50ms P95 Batch operations: \u0026lt;100ms average    Throughput requirements:\n Read operations: 100 ops/second sustained Write operations: 50 ops/second sustained Mixed workload: 150 total ops/second    Availability targets:\n Uptime: 99.9% availability Error rate: \u0026lt;0.1% of operations Recovery time: \u0026lt;5 minutes for issues    Screenshot Location: Add screenshot of performance SLA dashboard with target thresholds\n\rStep 2: Monitoring and Alerting for Performance Performance monitoring setup:\n  Performance alerts:\n High latency: P95 \u0026gt; 50ms for 5 minutes Low throughput: \u0026lt;50% expected throughput Error rate spike: \u0026gt;1% error rate    Performance dashboard: Real-time performance monitoring\n  Capacity planning: Proactive scaling based on trends\n  Step 3: Performance Optimization Runbook Operational procedures:\nPerformance Issue Response:\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ Issue Type â”‚ Immediate Action â”‚ Investigation â”‚\râ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\râ”‚ High Latency â”‚ Check capacity â”‚ Analyze queries â”‚\râ”‚ Throttling â”‚ Scale capacity â”‚ Review patterns â”‚\râ”‚ Hot Partitions â”‚ Implement sharding â”‚ Key design â”‚\râ”‚ Memory Errors â”‚ Optimize queries â”‚ Data size â”‚\râ”‚ Timeout Issues â”‚ Increase timeouts â”‚ Network check â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\rPerformance Optimization Best Practices Design Patterns Proven optimization techniques:\n Access pattern optimization: Design keys for efficient queries Batch operation usage: Group operations for efficiency Connection reuse: Minimize connection overhead Caching strategies: Implement intelligent caching layers  Monitoring and Measurement Continuous performance improvement:\n Baseline establishment: Document current performance Regular testing: Automated performance validation Trend analysis: Monitor performance over time Proactive optimization: Address issues before impact  Scaling Considerations Performance at scale:\n Partition distribution: Ensure even load distribution Capacity planning: Scale before hitting limits Regional optimization: Place resources near users Auto-scaling: Implement reactive scaling policies  Performance Excellence: Your DynamoDB system is now optimized for production-scale performance with comprehensive monitoring and optimization strategies!\n\rExercise Summary You\u0026rsquo;ve mastered performance optimization:\n âœ… Query optimization for maximum efficiency âœ… Capacity tuning for optimal resource utilization âœ… Application patterns for high-performance access âœ… Performance testing with comprehensive validation âœ… Production readiness with SLAs and monitoring âœ… Best practices for continuous optimization  Module 5 Complete Congratulations! You\u0026rsquo;ve built a comprehensive monitoring and optimization framework that transforms your DynamoDB system into a production-ready, highly-optimized solution. You now have the skills to operate, monitor, and optimize DynamoDB systems at enterprise scale.\n"
},
{
	"uri": "http://localhost:1313/6-advanced-patterns/6.1-batch-operations/",
	"title": "6.1 Batch Operations",
	"tags": [],
	"description": "",
	"content": "Efficient Multi-Item Processing âš¡ Master batch operations to process multiple items with single API calls for maximum efficiency\nOverview Batch operations are the key to efficient DynamoDB usage. Instead of making individual API calls for each item, batch operations allow you to process up to 25 items in a single request, dramatically reducing latency and improving performance.\nBatch Operation Types Available batch operations:\n BatchWriteItem: Insert or delete up to 25 items BatchGetItem: Retrieve up to 100 items Reduced API calls: Fewer round trips to DynamoDB Lower latency: Bulk processing efficiency  Performance Comparison Single Operations (Inefficient):\r- 100 items = 100 API calls\r- Higher latency per operation\r- More request units consumed\rBatch Operations (Efficient):\r- 100 items = 4 API calls (25 items each)\r- Lower overall latency\r- Optimal request unit usage\rExercise 1: Batch Write Operations Step 1: Create Batch Product Data Prepare batch data for multiple product insertion:\n AWS CloudShell: Open AWS CloudShell from the AWS Console Create batch file: Generate JSON for multiple products  Create batch-products.json file:\ncat \u0026gt; batch-products.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; { \u0026#34;demo-ecommerce-freetier\u0026#34;: [ { \u0026#34;PutRequest\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-laptop-001\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;}, \u0026#34;GSI1PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;CATEGORY#electronics\u0026#34;}, \u0026#34;GSI1SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-laptop-001\u0026#34;}, \u0026#34;name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Gaming Laptop Pro\u0026#34;}, \u0026#34;price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1299\u0026#34;}, \u0026#34;stock\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;15\u0026#34;}, \u0026#34;category\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;electronics\u0026#34;}, \u0026#34;brand\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;TechPro\u0026#34;} } } }, { \u0026#34;PutRequest\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-mouse-002\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;}, \u0026#34;GSI1PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;CATEGORY#electronics\u0026#34;}, \u0026#34;GSI1SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-mouse-002\u0026#34;}, \u0026#34;name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Wireless Gaming Mouse\u0026#34;}, \u0026#34;price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;79\u0026#34;}, \u0026#34;stock\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;50\u0026#34;}, \u0026#34;category\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;electronics\u0026#34;}, \u0026#34;brand\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;GamerGear\u0026#34;} } } }, { \u0026#34;PutRequest\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-keyboard-003\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;}, \u0026#34;GSI1PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;CATEGORY#electronics\u0026#34;}, \u0026#34;GSI1SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-keyboard-003\u0026#34;}, \u0026#34;name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Mechanical Keyboard RGB\u0026#34;}, \u0026#34;price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;159\u0026#34;}, \u0026#34;stock\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;25\u0026#34;}, \u0026#34;category\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;electronics\u0026#34;}, \u0026#34;brand\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;KeyMaster\u0026#34;} } } } ] } EOF \rScreenshot Location: Add screenshot of CloudShell with the batch-products.json file created\n\rStep 2: Execute Batch Write Perform batch write operation:\n Execute batch write command:  aws dynamodb batch-write-item --request-items file://batch-products.json Verify successful execution:  Check for no error messages Note the response showing processed items All 3 items created with single API call    Screenshot Location: Add screenshot showing successful batch write response in CloudShell\n\rStep 3: Verify Batch Creation Confirm items were created in DynamoDB Console:\n DynamoDB Console: Navigate to DynamoDB service Select table: Click on demo-ecommerce-freetier table View items: Click \u0026ldquo;Explore table items\u0026rdquo; Verify batch items: Look for the 3 newly created products  Items to verify:\n PRODUCT#batch-laptop-001: Gaming Laptop Pro PRODUCT#batch-mouse-002: Wireless Gaming Mouse PRODUCT#batch-keyboard-003: Mechanical Keyboard RGB  Screenshot Location: Add screenshot of DynamoDB console showing the 3 batch-created items\n\rExercise 2: Batch Read Operations Step 1: Create Batch Get Request Prepare batch read operation:\n Create batch-get.json file:  cat \u0026gt; batch-get.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; { \u0026#34;demo-ecommerce-freetier\u0026#34;: { \u0026#34;Keys\u0026#34;: [ { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-laptop-001\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;} }, { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-mouse-002\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;} }, { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-keyboard-003\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;} } ] } } EOF Step 2: Execute Batch Get Retrieve multiple items with single API call:\n Execute batch get command:  aws dynamodb batch-get-item --request-items file://batch-get.json Analyze response:  All 3 items returned in single response Complete item data for each product Much faster than 3 separate GetItem calls    Screenshot Location: Add screenshot of batch get response showing all 3 items retrieved at once\n\rStep 3: Performance Comparison Compare batch vs individual operations:\nSingle GetItem (for comparison):\n# Single item retrieval (less efficient) aws dynamodb get-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#batch-laptop-001\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; Performance analysis:\n Batch operation: 1 API call for 3 items Individual operations: 3 API calls for 3 items Efficiency gain: 3x fewer API calls Latency reduction: Significant improvement  Performance Benefit: Batch operations reduce API calls by up to 25x for writes and 100x for reads, dramatically improving application performance!\n\rExercise 3: Advanced Batch Patterns Step 1: Mixed Batch Operations Combine PutRequest and DeleteRequest in single batch:\n Create mixed-batch.json:  cat \u0026gt; mixed-batch.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; { \u0026#34;demo-ecommerce-freetier\u0026#34;: [ { \u0026#34;PutRequest\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-headset-004\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;}, \u0026#34;GSI1PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;CATEGORY#electronics\u0026#34;}, \u0026#34;GSI1SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-headset-004\u0026#34;}, \u0026#34;name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Wireless Headset Pro\u0026#34;}, \u0026#34;price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;199\u0026#34;}, \u0026#34;stock\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;20\u0026#34;}, \u0026#34;category\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;electronics\u0026#34;} } } }, { \u0026#34;DeleteRequest\u0026#34;: { \u0026#34;Key\u0026#34;: { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-mouse-002\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;} } } } ] } EOF Execute mixed batch operation:  aws dynamodb batch-write-item --request-items file://mixed-batch.json \rScreenshot Location: Add screenshot showing mixed batch operation execution (add + delete)\n\rStep 2: Error Handling in Batch Operations Understanding batch operation limitations:\n Create oversized batch (to demonstrate limits):  # Create a batch with more than 25 items (will be split automatically by AWS CLI) cat \u0026gt; large-batch.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; { \u0026#34;demo-ecommerce-freetier\u0026#34;: [ EOF # Add multiple items to demonstrate batch limits for i in {1..30}; do cat \u0026gt;\u0026gt; large-batch.json \u0026lt;\u0026lt; EOF { \u0026#34;PutRequest\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-test-$i\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;}, \u0026#34;name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Test Product $i\u0026#34;}, \u0026#34;price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;10\u0026#34;} } } }$([ $i -lt 30 ] \u0026amp;\u0026amp; echo \u0026#34;,\u0026#34;) EOF done cat \u0026gt;\u0026gt; large-batch.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; ] } EOF Execute and observe AWS CLI handling:  aws dynamodb batch-write-item --request-items file://large-batch.json Batch operation best practices:\n Maximum 25 items per BatchWriteItem request Maximum 100 items per BatchGetItem request Handle UnprocessedItems in response for retries Implement exponential backoff for failed items  Step 3: Batch Operations with Projections Optimize batch reads with projection expressions:\n Create projection batch get:  cat \u0026gt; batch-get-projection.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; { \u0026#34;demo-ecommerce-freetier\u0026#34;: { \u0026#34;Keys\u0026#34;: [ { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-laptop-001\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;} }, { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-keyboard-003\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;} }, { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-headset-004\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;} } ], \u0026#34;ProjectionExpression\u0026#34;: \u0026#34;PK, SK, #n, price, stock\u0026#34;, \u0026#34;ExpressionAttributeNames\u0026#34;: { \u0026#34;#n\u0026#34;: \u0026#34;name\u0026#34; } } } EOF Execute optimized batch get:  aws dynamodb batch-get-item --request-items file://batch-get-projection.json Benefits of projection in batch operations:\n Reduced data transfer: Only fetch needed attributes Lower RCU consumption: Pay only for retrieved data Faster response times: Less data to process Network efficiency: Smaller payload sizes  Screenshot Location: Add screenshot comparing full item vs projected item responses\n\rExercise 4: Production Batch Patterns Step 1: Batch Processing with Error Handling Implement robust batch processing:\nimport boto3 import json import time from botocore.exceptions import ClientError class BatchProcessor: def __init__(self, table_name): self.dynamodb = boto3.client(\u0026#39;dynamodb\u0026#39;) self.table_name = table_name def batch_write_with_retry(self, items, max_retries=3): \u0026#34;\u0026#34;\u0026#34; Batch write with automatic retry for unprocessed items \u0026#34;\u0026#34;\u0026#34; request_items = {self.table_name: items} for attempt in range(max_retries): try: response = self.dynamodb.batch_write_item( RequestItems=request_items ) # Check for unprocessed items unprocessed = response.get(\u0026#39;UnprocessedItems\u0026#39;, {}) if not unprocessed: return True # All items processed successfully # Retry unprocessed items with exponential backoff wait_time = (2 ** attempt) * 0.1 time.sleep(wait_time) request_items = unprocessed except ClientError as e: print(f\u0026#34;Batch write error (attempt {attempt + 1}): {e}\u0026#34;) if attempt == max_retries - 1: raise time.sleep(2 ** attempt) return False # Some items remained unprocessed def batch_get_with_projection(self, keys, projection_expression=None): \u0026#34;\u0026#34;\u0026#34; Batch get with optional projection \u0026#34;\u0026#34;\u0026#34; request_items = { self.table_name: {\u0026#39;Keys\u0026#39;: keys} } if projection_expression: request_items[self.table_name][\u0026#39;ProjectionExpression\u0026#39;] = projection_expression response = self.dynamodb.batch_get_item(RequestItems=request_items) return response.get(\u0026#39;Responses\u0026#39;, {}).get(self.table_name, []) Step 2: Batch Operation Monitoring Monitor batch operation performance:\n  CloudWatch metrics to monitor:\n UserErrors: Failed batch operations SystemErrors: DynamoDB-side issues ThrottledRequests: Capacity exceeded ConsumedReadCapacityUnits: RCU usage ConsumedWriteCapacityUnits: WCU usage    Performance optimization:\n Batch size tuning: Use maximum batch sizes Parallel processing: Multiple concurrent batches Error handling: Implement retry logic Monitoring: Track success rates and latency    Screenshot Location: Add screenshot of CloudWatch metrics showing batch operation performance\n\rStep 3: Cost Optimization with Batching Maximize Free Tier efficiency:\nCost comparison:\nIndividual Operations:\r- 100 items Ã— 1 WCU each = 100 WCU\r- 100 separate API calls\r- Higher latency\rBatch Operations: - 100 items Ã· 25 per batch = 4 API calls\r- Same 100 WCU total\r- 25x fewer API calls\r- Lower latency\rFree Tier benefits:\n Efficient RCU/WCU usage: Same capacity consumption Reduced API overhead: Fewer requests Better throughput: More items processed per second Improved user experience: Faster application response  Batch Operations Best Practices Efficiency Guidelines Maximize batch effectiveness:\n Use maximum batch sizes: 25 items for writes, 100 for reads Group related operations: Batch similar operations together Implement retry logic: Handle UnprocessedItems properly Monitor performance: Track batch success rates  Error Handling Robust batch processing:\n Exponential backoff: Gradual retry delays Partial success handling: Process successful items Logging and monitoring: Track failed operations Graceful degradation: Fall back to individual operations  Performance Optimization Batch operation tuning:\n Parallel batches: Process multiple batches concurrently Projection expressions: Reduce data transfer Capacity planning: Ensure adequate throughput Connection pooling: Reuse DynamoDB connections  Batch Mastery: You\u0026rsquo;ve mastered batch operations! You can now process multiple items efficiently, reduce API calls by up to 25x, and optimize for both performance and cost.\n\rExercise Summary You\u0026rsquo;ve learned batch operation techniques:\n âœ… Batch write operations for efficient data insertion âœ… Batch read operations for fast data retrieval âœ… Mixed batch operations combining puts and deletes âœ… Error handling patterns for robust processing âœ… Performance optimization with projections âœ… Production patterns for enterprise applications  Next Steps Continue to 6.2 Conditional Updates to learn data integrity patterns and prevent race conditions in concurrent environments.\n"
},
{
	"uri": "http://localhost:1313/6-advanced-patterns/6.2-conditional-updates/",
	"title": "6.2 Conditional Updates",
	"tags": [],
	"description": "",
	"content": "Data Integrity and Race Condition Prevention ğŸ›¡ï¸ Master conditional updates to ensure data integrity and prevent race conditions in concurrent environments\nOverview Conditional updates are essential for maintaining data integrity in multi-user applications. They prevent race conditions, ensure business rules are enforced, and protect against data corruption when multiple clients access the same data simultaneously.\nThe Race Condition Problem Scenario: Two users buying the last item:\nWithout Conditions (Dangerous):\rUser A reads: stock = 1\rUser B reads: stock = 1 User A updates: stock = 0 âœ…\rUser B updates: stock = 0 âŒ (Should fail!)\rResult: Both succeed (oversold!)\rWith Conditions (Safe):\rUser A reads: stock = 1\rUser B reads: stock = 1\rUser A updates: stock = 0 (condition: stock \u0026gt;= 1) âœ…\rUser B updates: stock = 0 (condition: stock \u0026gt;= 1) âŒ FAILS\rResult: Only first succeeds âœ…\rConditional Expression Types Available condition types:\n attribute_exists(path): Item/attribute must exist attribute_not_exists(path): Item/attribute must not exist Comparison operators: =, \u0026lt;\u0026gt;, \u0026lt;, \u0026lt;=, \u0026gt;, \u0026gt;= BETWEEN and IN: Range and membership tests Functions: begins_with(), contains(), size()  Exercise 1: Basic Conditional Updates Step 1: Setup Test Product Create a product for conditional update testing:\n AWS CloudShell: Open CloudShell from AWS Console Create test product:  aws dynamodb put-item \\  --table-name demo-ecommerce-freetier \\  --item \u0026#39;{ \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#conditional-test\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;}, \u0026#34;GSI1PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;CATEGORY#test\u0026#34;}, \u0026#34;GSI1SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#conditional-test\u0026#34;}, \u0026#34;name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Conditional Test Product\u0026#34;}, \u0026#34;price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;100\u0026#34;}, \u0026#34;stock\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;5\u0026#34;}, \u0026#34;version\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;} }\u0026#39; Verify creation: Check item exists in DynamoDB Console  Screenshot Location: Add screenshot showing the test product created in DynamoDB console\n\rStep 2: Price Update with Condition Update price only if current price matches expected value:\n Successful conditional update:  aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET price = :new_price, last_updated = :timestamp\u0026#34; \\  --condition-expression \u0026#34;price = :current_price\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:new_price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;90\u0026#34;}, \u0026#34;:current_price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;100\u0026#34;}, \u0026#34;:timestamp\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;2025-08-12T10:00:00Z\u0026#34;} }\u0026#39; Verify price change: Check updated price in console  Screenshot Location: Add screenshot showing successful price update with condition\n\rStep 3: Failed Conditional Update Attempt update with wrong condition (should fail):\n Try update with incorrect current price:  aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET price = :new_price\u0026#34; \\  --condition-expression \u0026#34;price = :wrong_price\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:new_price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;80\u0026#34;}, \u0026#34;:wrong_price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;100\u0026#34;} }\u0026#39; Expected result: ConditionalCheckFailedException error Verify price unchanged: Price should still be 90  Screenshot Location: Add screenshot showing ConditionalCheckFailedException error in CloudShell\n\rExercise 2: Inventory Management with Conditions Step 1: Stock Decrement with Safety Check Implement safe inventory decrement:\n Purchase 2 items (should succeed):  aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET stock = stock - :quantity\u0026#34; \\  --condition-expression \u0026#34;stock \u0026gt;= :quantity\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:quantity\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;2\u0026#34;} }\u0026#39; Verify stock reduction: Stock should be 3 (was 5, bought 2)  Screenshot Location: Add screenshot showing stock reduced from 5 to 3\n\rStep 2: Prevent Overselling Attempt to buy more items than available:\n Try to purchase 5 items (should fail):  aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET stock = stock - :quantity\u0026#34; \\  --condition-expression \u0026#34;stock \u0026gt;= :quantity\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:quantity\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;5\u0026#34;} }\u0026#39; Expected result: ConditionalCheckFailedException (only 3 in stock) Verify stock unchanged: Stock should still be 3  This prevents overselling! ğŸ›¡ï¸\nScreenshot Location: Add screenshot showing failed purchase attempt due to insufficient stock\n\rStep 3: Advanced Inventory Patterns Implement complex inventory logic:\n Conditional stock update with minimum threshold:  aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET stock = stock - :quantity, last_sold = :timestamp\u0026#34; \\  --condition-expression \u0026#34;stock \u0026gt;= :quantity AND stock \u0026gt; :min_threshold\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:quantity\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;}, \u0026#34;:min_threshold\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;}, \u0026#34;:timestamp\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;2025-08-12T10:05:00Z\u0026#34;} }\u0026#39; Business rule: Keep at least 1 item in stock (reserve inventory)  Exercise 3: Optimistic Locking Pattern Step 1: Version-Based Updates Implement optimistic locking with version numbers:\n Read current version: Note the version number (should be 1)  aws dynamodb get-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --projection-expression \u0026#34;version\u0026#34; Update with version check:  aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET price = :new_price, version = version + :inc\u0026#34; \\  --condition-expression \u0026#34;version = :expected_version\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:new_price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;85\u0026#34;}, \u0026#34;:inc\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;}, \u0026#34;:expected_version\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;} }\u0026#39; Verify version increment: Version should now be 2  Screenshot Location: Add screenshot showing version incremented from 1 to 2\n\rStep 2: Simulate Concurrent Update Conflict Demonstrate optimistic locking protection:\n Attempt update with old version (should fail):  aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET price = :new_price, version = version + :inc\u0026#34; \\  --condition-expression \u0026#34;version = :old_version\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:new_price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;75\u0026#34;}, \u0026#34;:inc\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;}, \u0026#34;:old_version\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;} }\u0026#39; Expected result: ConditionalCheckFailedException (version is now 2, not 1) Protection achieved: Prevents overwriting changes from other users  Screenshot Location: Add screenshot showing version conflict error\n\rStep 3: Proper Version Update Flow Correct optimistic locking workflow:\n Read current item with version:  aws dynamodb get-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --projection-expression \u0026#34;#v, price, #n\u0026#34; \\  --expression-attribute-names \u0026#39;{ \u0026#34;#v\u0026#34;: \u0026#34;version\u0026#34;, \u0026#34;#n\u0026#34;: \u0026#34;name\u0026#34; }\u0026#39; Update with current version (should succeed):  aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET price = :new_price, version = version + :inc\u0026#34; \\  --condition-expression \u0026#34;version = :current_version\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:new_price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;88\u0026#34;}, \u0026#34;:inc\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;}, \u0026#34;:current_version\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;2\u0026#34;} }\u0026#39; Version now 3: Successful update with proper version control  Exercise 4: Advanced Conditional Patterns Step 1: Attribute Existence Conditions Create items only if they don\u0026rsquo;t exist:\n Create user profile (should succeed):  aws dynamodb put-item \\  --table-name demo-ecommerce-freetier \\  --item \u0026#39;{ \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;USER#conditional-user\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PROFILE\u0026#34;}, \u0026#34;email\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;user@example.com\u0026#34;}, \u0026#34;created_at\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;2025-08-12T10:10:00Z\u0026#34;} }\u0026#39; \\  --condition-expression \u0026#34;attribute_not_exists(PK)\u0026#34; Try to create same user again (should fail):  aws dynamodb put-item \\  --table-name demo-ecommerce-freetier \\  --item \u0026#39;{ \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;USER#conditional-user\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PROFILE\u0026#34;}, \u0026#34;email\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;different@example.com\u0026#34;}, \u0026#34;created_at\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;2025-08-12T10:15:00Z\u0026#34;} }\u0026#39; \\  --condition-expression \u0026#34;attribute_not_exists(PK)\u0026#34; Result: Second attempt fails (prevents duplicate users)\nScreenshot Location: Add screenshot showing duplicate prevention with attribute_not_exists\n\rStep 2: Complex Conditional Logic Combine multiple conditions with AND/OR:\n Update product with complex business rules:  aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET discount_price = :discount\u0026#34; \\  --condition-expression \u0026#34;price \u0026gt; :min_price AND stock \u0026gt; :min_stock\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:discount\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;70\u0026#34;}, \u0026#34;:min_price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;50\u0026#34;}, \u0026#34;:min_stock\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;0\u0026#34;} }\u0026#39; Business rule: Only apply discount if price \u0026gt; $50 AND stock \u0026gt; 0  Step 3: Function-Based Conditions Use DynamoDB condition functions:\n Update if attribute contains specific value:  aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET featured = :featured\u0026#34; \\  --condition-expression \u0026#34;contains(#n, :keyword)\u0026#34; \\  --expression-attribute-names \u0026#39;{\u0026#34;#n\u0026#34;: \u0026#34;name\u0026#34;}\u0026#39; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:featured\u0026#34;: {\u0026#34;BOOL\u0026#34;: true}, \u0026#34;:keyword\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Test\u0026#34;} }\u0026#39; Condition: Only feature products with \u0026ldquo;Test\u0026rdquo; in the name  Screenshot Location: Add screenshot showing function-based condition success\n\rExercise 5: Production Conditional Patterns Step 1: E-commerce Order Processing Implement order creation with inventory check:\n# Create order only if product has sufficient stock aws dynamodb put-item \\  --table-name demo-ecommerce-freetier \\  --item \u0026#39;{ \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;ORDER#order-001\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;}, \u0026#34;GSI1PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;USER#conditional-user\u0026#34;}, \u0026#34;GSI1SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;ORDER#order-001\u0026#34;}, \u0026#34;product_id\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#conditional-test\u0026#34;}, \u0026#34;quantity\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;}, \u0026#34;status\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;pending\u0026#34;}, \u0026#34;created_at\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;2025-08-12T10:20:00Z\u0026#34;} }\u0026#39; \\  --condition-expression \u0026#34;attribute_not_exists(PK)\u0026#34; # Simultaneously decrement stock aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET stock = stock - :quantity, reserved_stock = if_not_exists(reserved_stock, :zero) + :quantity\u0026#34; \\  --condition-expression \u0026#34;stock \u0026gt;= :quantity\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:quantity\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;}, \u0026#34;:zero\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;0\u0026#34;} }\u0026#39; Step 2: Error Handling for Conditions Handle conditional failures gracefully:\nimport boto3 from botocore.exceptions import ClientError def safe_inventory_decrement(product_id, quantity): \u0026#34;\u0026#34;\u0026#34; Safely decrement inventory with proper error handling \u0026#34;\u0026#34;\u0026#34; dynamodb = boto3.client(\u0026#39;dynamodb\u0026#39;) try: response = dynamodb.update_item( TableName=\u0026#39;demo-ecommerce-freetier\u0026#39;, Key={ \u0026#39;PK\u0026#39;: {\u0026#39;S\u0026#39;: f\u0026#39;PRODUCT#{product_id}\u0026#39;}, \u0026#39;SK\u0026#39;: {\u0026#39;S\u0026#39;: \u0026#39;DETAILS\u0026#39;} }, UpdateExpression=\u0026#39;SET stock = stock - :quantity\u0026#39;, ConditionExpression=\u0026#39;stock \u0026gt;= :quantity\u0026#39;, ExpressionAttributeValues={ \u0026#39;:quantity\u0026#39;: {\u0026#39;N\u0026#39;: str(quantity)} }, ReturnValues=\u0026#39;ALL_NEW\u0026#39; ) return {\u0026#39;success\u0026#39;: True, \u0026#39;item\u0026#39;: response[\u0026#39;Attributes\u0026#39;]} except ClientError as e: if e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] == \u0026#39;ConditionalCheckFailedException\u0026#39;: return {\u0026#39;success\u0026#39;: False, \u0026#39;error\u0026#39;: \u0026#39;Insufficient stock\u0026#39;} else: return {\u0026#39;success\u0026#39;: False, \u0026#39;error\u0026#39;: f\u0026#39;Database error: {e}\u0026#39;} # Usage example result = safe_inventory_decrement(\u0026#39;conditional-test\u0026#39;, 1) if result[\u0026#39;success\u0026#39;]: print(\u0026#34;Stock decremented successfully\u0026#34;) else: print(f\u0026#34;Failed: {result[\u0026#39;error\u0026#39;]}\u0026#34;) Step 3: Performance Monitoring Monitor conditional update patterns:\n  CloudWatch metrics to track:\n ConditionalCheckFailedException: Failed condition rate SuccessfulRequestLatency: Update performance ThrottledRequests: Capacity issues ItemCollectionMetrics: Item size growth    Optimize conditional performance:\n Minimize condition complexity: Simple conditions perform better Use efficient operators: = is faster than contains() Avoid unnecessary conditions: Only check what\u0026rsquo;s essential Monitor failure rates: High failure rates indicate design issues    Screenshot Location: Add screenshot of CloudWatch showing conditional update metrics\n\rConditional Updates Best Practices Design Guidelines Effective condition design:\n Keep conditions simple: Complex conditions impact performance Use appropriate operators: Choose the most efficient operator Minimize attribute references: Fewer attributes = better performance Plan for failures: Design application logic to handle condition failures  Common Patterns Production-ready conditional patterns:\nOptimistic Locking # Always increment version on updates UPDATE SET data = :new_data, version = version + 1 WHERE version = :expected_version Inventory Management # Prevent overselling UPDATE SET stock = stock - :quantity WHERE stock \u0026gt;= :quantity Unique Constraints # Prevent duplicates PUT item WHERE attribute_not_exists(primary_key) Business Rules # Enforce business logic UPDATE SET status = :new_status WHERE current_status = :expected_status AND user_role = :admin Error Handling Robust conditional update handling:\n Expect failures: Conditions are designed to fail sometimes Retry logic: Implement appropriate retry strategies User feedback: Provide meaningful error messages Logging: Track condition failures for analysis  Data Integrity Mastery: You\u0026rsquo;ve mastered conditional updates! Your applications can now handle concurrent access safely, prevent race conditions, and maintain data integrity at scale.\n\rExercise Summary You\u0026rsquo;ve implemented conditional update patterns:\n âœ… Basic conditional updates with price and stock management âœ… Inventory safety checks preventing overselling âœ… Optimistic locking with version control âœ… Attribute existence conditions for duplicate prevention âœ… Complex conditional logic with multiple criteria âœ… Production error handling for robust applications  Next Steps Continue to 6.3 Advanced Query Techniques to learn performance optimization strategies and advanced querying patterns.\n"
},
{
	"uri": "http://localhost:1313/6-advanced-patterns/6.3-advanced-query-techniques/",
	"title": "6.3 Advanced Query Techniques",
	"tags": [],
	"description": "",
	"content": "Performance Optimization and Cost Efficiency âš¡ Master advanced querying techniques to optimize performance, reduce costs, and improve application responsiveness\nOverview Advanced query techniques are crucial for building high-performance, cost-effective DynamoDB applications. Learn how to minimize RCU consumption, reduce data transfer, and optimize query patterns for production workloads.\nQuery Optimization Fundamentals Key optimization areas:\n Projection Expressions: Fetch only needed attributes Filter Expressions: Refine results server-side Pagination: Handle large result sets efficiently Index Optimization: Choose the right index for each query Parallel Processing: Scale query throughput  Performance vs Cost Balance Query Optimization Impact:\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ Technique â”‚ Performance Gain â”‚ Cost Reduction â”‚\râ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\râ”‚ Projection â”‚ Faster response â”‚ 50-80% RCU â”‚\râ”‚ Proper indexing â”‚ 10x faster queries â”‚ Avoid scans â”‚\râ”‚ Pagination â”‚ Consistent latency â”‚ Memory control â”‚\râ”‚ Parallel queries â”‚ Higher throughput â”‚ Better scaling â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\rExercise 1: Projection Expression Optimization Step 1: Full Item Query (Baseline) Compare full item retrieval vs optimized projection:\n AWS CloudShell: Open CloudShell from AWS Console Query without projection (retrieves all attributes):  aws dynamodb query \\  --table-name demo-ecommerce-freetier \\  --key-condition-expression \u0026#34;PK = :pk\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:pk\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#conditional-test\u0026#34;} }\u0026#39; Note response size: Large payload with all attributes  Screenshot Location: Add screenshot showing full item query with complete response payload\n\rStep 2: Optimized Projection Query Fetch only essential attributes:\n Query with projection expression:  aws dynamodb query \\  --table-name demo-ecommerce-freetier \\  --key-condition-expression \u0026#34;PK = :pk\u0026#34; \\  --projection-expression \u0026#34;PK, SK, #n, price, stock\u0026#34; \\  --expression-attribute-names \u0026#39;{\u0026#34;#n\u0026#34;: \u0026#34;name\u0026#34;}\u0026#39; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:pk\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#conditional-test\u0026#34;} }\u0026#39; Compare response sizes:  Full query: All attributes returned Projected query: Only specified attributes RCU savings: Proportional to size reduction    Screenshot Location: Add screenshot comparing full vs projected query responses\n\rStep 3: GSI Projection Optimization Optimize GSI queries with smart projections:\n Query GSI with projection:  aws dynamodb query \\  --table-name demo-ecommerce-freetier \\  --index-name GSI1 \\  --key-condition-expression \u0026#34;GSI1PK = :gsi1pk\u0026#34; \\  --projection-expression \u0026#34;GSI1PK, GSI1SK, #n, price\u0026#34; \\  --expression-attribute-names \u0026#39;{\u0026#34;#n\u0026#34;: \u0026#34;name\u0026#34;}\u0026#39; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:gsi1pk\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;CATEGORY#electronics\u0026#34;} }\u0026#39; Performance benefits:  Faster queries: Less data transfer Lower RCU costs: Pay only for retrieved data Better user experience: Quicker page loads    Screenshot Location: Add screenshot showing GSI query with projection optimization\n\rExercise 2: Filter Expression Techniques Step 1: Basic Filter Expressions Apply server-side filtering to refine results:\n Query with price range filter:  aws dynamodb query \\  --table-name demo-ecommerce-freetier \\  --index-name GSI1 \\  --key-condition-expression \u0026#34;GSI1PK = :gsi1pk\u0026#34; \\  --filter-expression \u0026#34;price BETWEEN :min_price AND :max_price\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:gsi1pk\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;CATEGORY#electronics\u0026#34;}, \u0026#34;:min_price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;50\u0026#34;}, \u0026#34;:max_price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;200\u0026#34;} }\u0026#39; Important: Filters don\u0026rsquo;t reduce RCU (applied after reading) Use case: When you need subset of results  Screenshot Location: Add screenshot showing filtered query results within price range\n\rStep 2: Complex Filter Conditions Combine multiple filter criteria:\n Multi-condition filter:  aws dynamodb query \\  --table-name demo-ecommerce-freetier \\  --index-name GSI1 \\  --key-condition-expression \u0026#34;GSI1PK = :gsi1pk\u0026#34; \\  --filter-expression \u0026#34;price \u0026lt; :max_price AND attribute_exists(stock) AND stock \u0026gt; :min_stock\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:gsi1pk\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;CATEGORY#electronics\u0026#34;}, \u0026#34;:max_price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;300\u0026#34;}, \u0026#34;:min_stock\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;0\u0026#34;} }\u0026#39; Filter logic: Price \u0026lt; $300 AND has stock attribute AND stock \u0026gt; 0  Step 3: Function-Based Filters Use DynamoDB filter functions:\n Contains function filter:  aws dynamodb query \\  --table-name demo-ecommerce-freetier \\  --index-name GSI1 \\  --key-condition-expression \u0026#34;GSI1PK = :gsi1pk\u0026#34; \\  --filter-expression \u0026#34;contains(#n, :keyword)\u0026#34; \\  --expression-attribute-names \u0026#39;{\u0026#34;#n\u0026#34;: \u0026#34;name\u0026#34;}\u0026#39; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:gsi1pk\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;CATEGORY#electronics\u0026#34;}, \u0026#34;:keyword\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Gaming\u0026#34;} }\u0026#39; Use case: Search for products with \u0026ldquo;Gaming\u0026rdquo; in the name  Screenshot Location: Add screenshot showing function-based filter results\n\rExercise 3: Pagination and Large Result Sets Step 1: Pagination Setup Handle large result sets with pagination:\n First page query:  aws dynamodb query \\  --table-name demo-ecommerce-freetier \\  --index-name GSI1 \\  --key-condition-expression \u0026#34;GSI1PK = :gsi1pk\u0026#34; \\  --limit 2 \\  --expression-attribute-values \u0026#39;{ \u0026#34;:gsi1pk\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;CATEGORY#electronics\u0026#34;} }\u0026#39; Note LastEvaluatedKey: Used for next page Limit parameter: Controls page size  Screenshot Location: Add screenshot showing first page with LastEvaluatedKey\n\rStep 2: Next Page Retrieval Continue pagination with ExclusiveStartKey:\n Second page query (use LastEvaluatedKey from previous response):  aws dynamodb query \\  --table-name demo-ecommerce-freetier \\  --index-name GSI1 \\  --key-condition-expression \u0026#34;GSI1PK = :gsi1pk\u0026#34; \\  --limit 2 \\  --exclusive-start-key \u0026#39;{\u0026#34;GSI1PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;CATEGORY#electronics\u0026#34;},\u0026#34;GSI1SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#batch-keyboard-003\u0026#34;},\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#batch-keyboard-003\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:gsi1pk\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;CATEGORY#electronics\u0026#34;} }\u0026#39; Pagination pattern: Continue until no LastEvaluatedKey returned  Step 3: Efficient Pagination Implementation Production pagination code pattern:\nimport boto3 def paginated_query(table_name, index_name, key_condition, page_size=10): \u0026#34;\u0026#34;\u0026#34; Efficiently paginate through large query results \u0026#34;\u0026#34;\u0026#34; dynamodb = boto3.client(\u0026#39;dynamodb\u0026#39;) paginator = dynamodb.get_paginator(\u0026#39;query\u0026#39;) page_iterator = paginator.paginate( TableName=table_name, IndexName=index_name, KeyConditionExpression=key_condition, PaginationConfig={\u0026#39;PageSize\u0026#39;: page_size} ) all_items = [] for page in page_iterator: items = page.get(\u0026#39;Items\u0026#39;, []) all_items.extend(items) print(f\u0026#34;Retrieved {len(items)}items from this page\u0026#34;) return all_items # Usage example items = paginated_query( \u0026#39;demo-ecommerce-freetier\u0026#39;, \u0026#39;GSI1\u0026#39;, \u0026#39;GSI1PK = :pk\u0026#39;, page_size=5 ) Exercise 4: Parallel Query Processing Step 1: Single vs Parallel Scan Comparison Compare sequential vs parallel processing:\n Standard scan (single segment):  aws dynamodb scan \\  --table-name demo-ecommerce-freetier \\  --filter-expression \u0026#34;attribute_exists(price)\u0026#34; Note scan time: Baseline performance measurement  Screenshot Location: Add screenshot showing single scan execution time\n\rStep 2: Parallel Scan Implementation Use parallel scan for faster processing:\n Parallel scan segment 1:  aws dynamodb scan \\  --table-name demo-ecommerce-freetier \\  --filter-expression \u0026#34;attribute_exists(price)\u0026#34; \\  --segment 0 \\  --total-segments 2 Parallel scan segment 2 (run simultaneously):  aws dynamodb scan \\  --table-name demo-ecommerce-freetier \\  --filter-expression \u0026#34;attribute_exists(price)\u0026#34; \\  --segment 1 \\  --total-segments 2 Performance gain: 2x throughput with 2 segments  Step 3: Production Parallel Processing Implement robust parallel query system:\nimport boto3 import concurrent.futures from typing import List, Dict, Any class ParallelQueryProcessor: def __init__(self, table_name: str): self.table_name = table_name self.dynamodb = boto3.client(\u0026#39;dynamodb\u0026#39;) def parallel_scan(self, total_segments: int = 4, filter_expression: str = None): \u0026#34;\u0026#34;\u0026#34; Execute parallel scan across multiple segments \u0026#34;\u0026#34;\u0026#34; def scan_segment(segment: int) -\u0026gt; List[Dict]: params = { \u0026#39;TableName\u0026#39;: self.table_name, \u0026#39;Segment\u0026#39;: segment, \u0026#39;TotalSegments\u0026#39;: total_segments } if filter_expression: params[\u0026#39;FilterExpression\u0026#39;] = filter_expression response = self.dynamodb.scan(**params) return response.get(\u0026#39;Items\u0026#39;, []) # Execute segments in parallel with concurrent.futures.ThreadPoolExecutor(max_workers=total_segments) as executor: futures = [ executor.submit(scan_segment, i) for i in range(total_segments) ] results = [] for future in concurrent.futures.as_completed(futures): segment_results = future.result() results.extend(segment_results) print(f\u0026#34;Segment completed: {len(segment_results)}items\u0026#34;) return results def parallel_query_by_partition(self, partition_keys: List[str]): \u0026#34;\u0026#34;\u0026#34; Execute parallel queries across different partitions \u0026#34;\u0026#34;\u0026#34; def query_partition(pk: str) -\u0026gt; List[Dict]: response = self.dynamodb.query( TableName=self.table_name, KeyConditionExpression=\u0026#39;PK = :pk\u0026#39;, ExpressionAttributeValues={\u0026#39;:pk\u0026#39;: {\u0026#39;S\u0026#39;: pk}} ) return response.get(\u0026#39;Items\u0026#39;, []) with concurrent.futures.ThreadPoolExecutor(max_workers=len(partition_keys)) as executor: futures = [ executor.submit(query_partition, pk) for pk in partition_keys ] results = [] for future in concurrent.futures.as_completed(futures): partition_results = future.result() results.extend(partition_results) return results # Usage example processor = ParallelQueryProcessor(\u0026#39;demo-ecommerce-freetier\u0026#39;) # Parallel scan with 4 segments items = processor.parallel_scan(total_segments=4) print(f\u0026#34;Total items retrieved: {len(items)}\u0026#34;) # Parallel queries across partitions partitions = [\u0026#39;PRODUCT#batch-laptop-001\u0026#39;, \u0026#39;PRODUCT#batch-mouse-002\u0026#39;] items = processor.parallel_query_by_partition(partitions) Exercise 5: Index Strategy Optimization Step 1: Query Pattern Analysis Analyze and optimize access patterns:\n Inefficient table scan:  aws dynamodb scan \\  --table-name demo-ecommerce-freetier \\  --filter-expression \u0026#34;#s = :status\u0026#34; \\  --expression-attribute-names \u0026#39;{\u0026#34;#s\u0026#34;: \u0026#34;status\u0026#34;}\u0026#39; \\  --expression-attribute-values \u0026#39;{\u0026#34;:status\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;pending\u0026#34;}}\u0026#39; Performance issues: Scans entire table (expensive)  Screenshot Location: Add screenshot showing expensive scan operation\n\rStep 2: GSI Optimization Use appropriate GSI for efficient queries:\n Create status-based GSI (if not exists):  # Query using optimized GSI pattern instead aws dynamodb query \\  --table-name demo-ecommerce-freetier \\  --index-name GSI2 \\  --key-condition-expression \u0026#34;GSI2PK = :status\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:status\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;STATUS#pending\u0026#34;} }\u0026#39; Performance improvement: Direct key access vs full scan  Step 3: Sparse Index Patterns Implement sparse indexes for efficient filtering:\n Query sparse index (only items with specific attributes):  aws dynamodb query \\  --table-name demo-ecommerce-freetier \\  --index-name GSI1 \\  --key-condition-expression \u0026#34;GSI1PK = :category AND begins_with(GSI1SK, :prefix)\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:category\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;CATEGORY#electronics\u0026#34;}, \u0026#34;:prefix\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#\u0026#34;} }\u0026#39; Sparse index benefit: Only indexed items appear in results  Screenshot Location: Add screenshot showing efficient GSI query results\n\rExercise 6: Performance Monitoring and Optimization Step 1: Query Performance Metrics Monitor query performance in CloudWatch:\n  Key metrics to track:\n SuccessfulRequestLatency: Query response time ConsumedReadCapacityUnits: RCU usage per query ThrottledRequests: Capacity exceeded events ItemCount: Number of items processed    DynamoDB Console: Navigate to Metrics tab for performance data\n  Screenshot Location: Add screenshot of CloudWatch metrics showing query performance\n\rStep 2: Query Cost Analysis Understand and optimize query costs:\n RCU consumption analysis:  # Compare RCU usage between operations aws dynamodb query \\  --table-name demo-ecommerce-freetier \\  --key-condition-expression \u0026#34;PK = :pk\u0026#34; \\  --projection-expression \u0026#34;PK, SK\u0026#34; \\  --expression-attribute-values \u0026#39;{\u0026#34;:pk\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#conditional-test\u0026#34;}}\u0026#39; \\  --return-consumed-capacity TOTAL Optimization strategies:  Use projections: Reduce data transfer Efficient key design: Minimize query scope Proper indexing: Avoid scans Batch operations: Reduce API calls    Step 3: Production Query Patterns Implement enterprise-grade query optimization:\nimport boto3 import time from typing import Dict, List, Optional class OptimizedQueryManager: def __init__(self, table_name: str): self.table_name = table_name self.dynamodb = boto3.client(\u0026#39;dynamodb\u0026#39;) self.performance_stats = {} def optimized_query( self, key_condition: str, projection: Optional[str] = None, filter_expr: Optional[str] = None, index_name: Optional[str] = None ) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Execute optimized query with performance tracking \u0026#34;\u0026#34;\u0026#34; start_time = time.time() params = { \u0026#39;TableName\u0026#39;: self.table_name, \u0026#39;KeyConditionExpression\u0026#39;: key_condition, \u0026#39;ReturnConsumedCapacity\u0026#39;: \u0026#39;TOTAL\u0026#39; } if projection: params[\u0026#39;ProjectionExpression\u0026#39;] = projection if filter_expr: params[\u0026#39;FilterExpression\u0026#39;] = filter_expr if index_name: params[\u0026#39;IndexName\u0026#39;] = index_name try: response = self.dynamodb.query(**params) # Track performance metrics duration = time.time() - start_time consumed_rcu = response.get(\u0026#39;ConsumedCapacity\u0026#39;, {}).get(\u0026#39;CapacityUnits\u0026#39;, 0) item_count = response.get(\u0026#39;Count\u0026#39;, 0) self.performance_stats = { \u0026#39;duration_ms\u0026#39;: round(duration * 1000, 2), \u0026#39;consumed_rcu\u0026#39;: consumed_rcu, \u0026#39;items_returned\u0026#39;: item_count, \u0026#39;efficiency\u0026#39;: round(item_count / max(consumed_rcu, 1), 2) } return { \u0026#39;items\u0026#39;: response.get(\u0026#39;Items\u0026#39;, []), \u0026#39;performance\u0026#39;: self.performance_stats, \u0026#39;last_key\u0026#39;: response.get(\u0026#39;LastEvaluatedKey\u0026#39;) } except Exception as e: return {\u0026#39;error\u0026#39;: str(e), \u0026#39;performance\u0026#39;: None} def compare_query_strategies(self, strategies: List[Dict]) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Compare multiple query strategies for performance \u0026#34;\u0026#34;\u0026#34; results = {} for strategy in strategies: name = strategy.get(\u0026#39;name\u0026#39;, \u0026#39;unnamed\u0026#39;) result = self.optimized_query(**strategy.get(\u0026#39;params\u0026#39;, {})) results[name] = result.get(\u0026#39;performance\u0026#39;) return results # Usage example query_manager = OptimizedQueryManager(\u0026#39;demo-ecommerce-freetier\u0026#39;) # Compare different query approaches strategies = [ { \u0026#39;name\u0026#39;: \u0026#39;full_scan\u0026#39;, \u0026#39;params\u0026#39;: { \u0026#39;key_condition\u0026#39;: \u0026#39;PK = :pk\u0026#39;, } }, { \u0026#39;name\u0026#39;: \u0026#39;projected_query\u0026#39;, \u0026#39;params\u0026#39;: { \u0026#39;key_condition\u0026#39;: \u0026#39;PK = :pk\u0026#39;, \u0026#39;projection\u0026#39;: \u0026#39;PK, SK, name, price\u0026#39; } }, { \u0026#39;name\u0026#39;: \u0026#39;gsi_query\u0026#39;, \u0026#39;params\u0026#39;: { \u0026#39;key_condition\u0026#39;: \u0026#39;GSI1PK = :gsi1pk\u0026#39;, \u0026#39;index_name\u0026#39;: \u0026#39;GSI1\u0026#39;, \u0026#39;projection\u0026#39;: \u0026#39;name, price\u0026#39; } } ] performance_comparison = query_manager.compare_query_strategies(strategies) Advanced Query Best Practices Optimization Guidelines Query performance optimization:\n Use projections: Fetch only needed attributes (50-80% RCU savings) Choose right index: GSI vs table scan performance difference is 10x+ Implement pagination: Handle large result sets efficiently Monitor RCU usage: Track consumption patterns  Cost Efficiency Free Tier optimization strategies:\n Projection expressions: Minimize data transfer Efficient key design: Reduce query scope Batch related queries: Group similar operations Use sparse indexes: Index only relevant items  Performance Patterns Production-ready query patterns:\nHigh-Performance Queries # Efficient GSI query with projection QUERY IndexName=GSI1 WHERE GSI1PK = :category PROJECT name, price, stock Cost-Optimized Queries # Minimal data transfer QUERY WHERE PK = :product PROJECT PK, SK, name Scalable Pagination # Consistent page sizes QUERY LIMIT 50 CONTINUE WITH LastEvaluatedKey \rQuery Optimization Mastery: You\u0026rsquo;ve mastered advanced query techniques! Your applications can now achieve optimal performance, minimize costs, and scale efficiently.\n\rExercise Summary You\u0026rsquo;ve mastered advanced query optimization:\n âœ… Projection expressions for 50-80% RCU reduction âœ… Filter expressions for precise result refinement âœ… Pagination patterns for large dataset handling âœ… Parallel processing for improved throughput âœ… Index optimization for 10x+ performance gains âœ… Performance monitoring for continuous improvement  Next Steps Continue to 6.4 Production Patterns to learn enterprise-grade implementation patterns and best practices for production DynamoDB applications.\n"
},
{
	"uri": "http://localhost:1313/6-advanced-patterns/6.4-production-patterns/",
	"title": "6.4 Production Patterns",
	"tags": [],
	"description": "",
	"content": "Enterprise-Grade Implementation Patterns ğŸ¢ Master production-ready patterns and best practices for enterprise DynamoDB applications\nOverview Production patterns represent the culmination of DynamoDB expertiseâ€”proven techniques used by companies serving millions of users. Learn how to implement robust, scalable, and maintainable DynamoDB solutions that perform reliably under real-world conditions.\nEnterprise Requirements Production-grade characteristics:\n Reliability: 99.9%+ uptime with graceful error handling Performance: Consistent sub-50ms response times Scalability: Handle traffic spikes without degradation Maintainability: Clean, documented, and testable code Cost Efficiency: Optimize for operational costs  Pattern Categories Production Pattern Types:\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ Pattern Type â”‚ Use Case â”‚ Benefit â”‚\râ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\râ”‚ Data Access Layer â”‚ Clean separation â”‚ Maintainable â”‚\râ”‚ Caching Strategy â”‚ Performance boost â”‚ Lower latency â”‚\râ”‚ Error Handling â”‚ Robust operations â”‚ Better UX â”‚\râ”‚ Connection Pool â”‚ Resource efficiency â”‚ Cost savings â”‚\râ”‚ Monitoring â”‚ Operational insight â”‚ Proactive fixes â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\rExercise 1: Data Access Layer Pattern Step 1: Repository Pattern Implementation Create a clean data access abstraction:\n AWS CloudShell: Open CloudShell from AWS Console Create repository class structure:  import boto3 import json from typing import Dict, List, Optional, Any from botocore.exceptions import ClientError from datetime import datetime import uuid class DynamoDBRepository: \u0026#34;\u0026#34;\u0026#34; Enterprise-grade DynamoDB repository with error handling and logging \u0026#34;\u0026#34;\u0026#34; def __init__(self, table_name: str, region: str = \u0026#39;us-east-1\u0026#39;): self.table_name = table_name self.dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;, region_name=region) self.table = self.dynamodb.Table(table_name) self.client = boto3.client(\u0026#39;dynamodb\u0026#39;, region_name=region) def create_item(self, item: Dict[str, Any], condition: Optional[str] = None) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Create item with optional condition check \u0026#34;\u0026#34;\u0026#34; try: # Add metadata item[\u0026#39;created_at\u0026#39;] = datetime.utcnow().isoformat() item[\u0026#39;updated_at\u0026#39;] = item[\u0026#39;created_at\u0026#39;] put_params = {\u0026#39;Item\u0026#39;: item} if condition: put_params[\u0026#39;ConditionExpression\u0026#39;] = condition self.table.put_item(**put_params) return { \u0026#39;success\u0026#39;: True, \u0026#39;item\u0026#39;: item, \u0026#39;operation\u0026#39;: \u0026#39;create\u0026#39; } except ClientError as e: return self._handle_error(e, \u0026#39;create_item\u0026#39;, item) def get_item(self, key: Dict[str, str], projection: Optional[str] = None) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Get single item with optional projection \u0026#34;\u0026#34;\u0026#34; try: get_params = {\u0026#39;Key\u0026#39;: key} if projection: get_params[\u0026#39;ProjectionExpression\u0026#39;] = projection response = self.table.get_item(**get_params) if \u0026#39;Item\u0026#39; in response: return { \u0026#39;success\u0026#39;: True, \u0026#39;item\u0026#39;: response[\u0026#39;Item\u0026#39;], \u0026#39;found\u0026#39;: True } else: return { \u0026#39;success\u0026#39;: True, \u0026#39;item\u0026#39;: None, \u0026#39;found\u0026#39;: False } except ClientError as e: return self._handle_error(e, \u0026#39;get_item\u0026#39;, key) def update_item(self, key: Dict[str, str], updates: Dict[str, Any], condition: Optional[str] = None) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Update item with automatic versioning and timestamp \u0026#34;\u0026#34;\u0026#34; try: # Build update expression update_expr = \u0026#34;SET updated_at = :updated_at\u0026#34; expr_values = {\u0026#39;:updated_at\u0026#39;: datetime.utcnow().isoformat()} for attr, value in updates.items(): update_expr += f\u0026#34;, {attr}= :{attr}\u0026#34; expr_values[f\u0026#39;:{attr}\u0026#39;] = value # Add version increment if version exists update_expr += \u0026#34;, version = if_not_exists(version, :zero) + :inc\u0026#34; expr_values[\u0026#39;:zero\u0026#39;] = 0 expr_values[\u0026#39;:inc\u0026#39;] = 1 update_params = { \u0026#39;Key\u0026#39;: key, \u0026#39;UpdateExpression\u0026#39;: update_expr, \u0026#39;ExpressionAttributeValues\u0026#39;: expr_values, \u0026#39;ReturnValues\u0026#39;: \u0026#39;ALL_NEW\u0026#39; } if condition: update_params[\u0026#39;ConditionExpression\u0026#39;] = condition response = self.table.update_item(**update_params) return { \u0026#39;success\u0026#39;: True, \u0026#39;item\u0026#39;: response[\u0026#39;Attributes\u0026#39;], \u0026#39;operation\u0026#39;: \u0026#39;update\u0026#39; } except ClientError as e: return self._handle_error(e, \u0026#39;update_item\u0026#39;, {\u0026#39;key\u0026#39;: key, \u0026#39;updates\u0026#39;: updates}) def query_items(self, key_condition: str, index_name: Optional[str] = None, filter_expr: Optional[str] = None, projection: Optional[str] = None, limit: Optional[int] = None) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Query items with comprehensive parameters \u0026#34;\u0026#34;\u0026#34; try: query_params = {\u0026#39;KeyConditionExpression\u0026#39;: key_condition} if index_name: query_params[\u0026#39;IndexName\u0026#39;] = index_name if filter_expr: query_params[\u0026#39;FilterExpression\u0026#39;] = filter_expr if projection: query_params[\u0026#39;ProjectionExpression\u0026#39;] = projection if limit: query_params[\u0026#39;Limit\u0026#39;] = limit response = self.table.query(**query_params) return { \u0026#39;success\u0026#39;: True, \u0026#39;items\u0026#39;: response[\u0026#39;Items\u0026#39;], \u0026#39;count\u0026#39;: response[\u0026#39;Count\u0026#39;], \u0026#39;last_key\u0026#39;: response.get(\u0026#39;LastEvaluatedKey\u0026#39;) } except ClientError as e: return self._handle_error(e, \u0026#39;query_items\u0026#39;, query_params) def _handle_error(self, error: ClientError, operation: str, context: Any) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Centralized error handling with logging \u0026#34;\u0026#34;\u0026#34; error_code = error.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] error_message = error.response[\u0026#39;Error\u0026#39;][\u0026#39;Message\u0026#39;] # Log error (in production, use proper logging) print(f\u0026#34;DynamoDB Error in {operation}: {error_code}- {error_message}\u0026#34;) print(f\u0026#34;Context: {json.dumps(context, default=str)}\u0026#34;) return { \u0026#39;success\u0026#39;: False, \u0026#39;error_code\u0026#39;: error_code, \u0026#39;error_message\u0026#39;: error_message, \u0026#39;operation\u0026#39;: operation } # Usage example class ProductRepository(DynamoDBRepository): \u0026#34;\u0026#34;\u0026#34; Product-specific repository with business logic \u0026#34;\u0026#34;\u0026#34; def create_product(self, product_data: Dict[str, Any]) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Create product with business validation \u0026#34;\u0026#34;\u0026#34; # Generate unique product ID product_id = f\u0026#34;PRODUCT#{uuid.uuid4().hex[:8]}\u0026#34; item = { \u0026#39;PK\u0026#39;: product_id, \u0026#39;SK\u0026#39;: \u0026#39;DETAILS\u0026#39;, \u0026#39;GSI1PK\u0026#39;: f\u0026#34;CATEGORY#{product_data[\u0026#39;category\u0026#39;]}\u0026#34;, \u0026#39;GSI1SK\u0026#39;: product_id, **product_data } # Ensure product doesn\u0026#39;t already exist condition = \u0026#34;attribute_not_exists(PK)\u0026#34; return self.create_item(item, condition) def get_product(self, product_id: str) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Get product by ID with error handling \u0026#34;\u0026#34;\u0026#34; key = {\u0026#39;PK\u0026#39;: product_id, \u0026#39;SK\u0026#39;: \u0026#39;DETAILS\u0026#39;} return self.get_item(key) def update_product_stock(self, product_id: str, quantity_change: int) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Update product stock with safety checks \u0026#34;\u0026#34;\u0026#34; key = {\u0026#39;PK\u0026#39;: product_id, \u0026#39;SK\u0026#39;: \u0026#39;DETAILS\u0026#39;} if quantity_change \u0026lt; 0: # Decreasing stock - ensure sufficient quantity updates = {\u0026#39;stock\u0026#39;: f\u0026#39;stock - {abs(quantity_change)}\u0026#39;} condition = f\u0026#34;stock \u0026gt;= {abs(quantity_change)}\u0026#34; else: # Increasing stock updates = {\u0026#39;stock\u0026#39;: f\u0026#39;stock + {quantity_change}\u0026#39;} condition = None return self.update_item(key, updates, condition) def get_products_by_category(self, category: str, limit: int = 20) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Get products by category using GSI \u0026#34;\u0026#34;\u0026#34; key_condition = f\u0026#34;GSI1PK = :category\u0026#34; return self.query_items( key_condition=key_condition, index_name=\u0026#39;GSI1\u0026#39;, projection=\u0026#39;PK, SK, name, price, stock\u0026#39;, limit=limit ) \rScreenshot Location: Add screenshot showing repository pattern code implementation in CloudShell\n\rStep 2: Repository Usage Examples Implement business operations using the repository:\n Create test products using repository:  # Initialize repository product_repo = ProductRepository(\u0026#39;demo-ecommerce-freetier\u0026#39;) # Create products laptop_result = product_repo.create_product({ \u0026#39;name\u0026#39;: \u0026#39;Enterprise Laptop\u0026#39;, \u0026#39;price\u0026#39;: 1499, \u0026#39;stock\u0026#39;: 10, \u0026#39;category\u0026#39;: \u0026#39;electronics\u0026#39;, \u0026#39;brand\u0026#39;: \u0026#39;TechCorp\u0026#39; }) mouse_result = product_repo.create_product({ \u0026#39;name\u0026#39;: \u0026#39;Wireless Mouse Pro\u0026#39;, \u0026#39;price\u0026#39;: 89, \u0026#39;stock\u0026#39;: 50, \u0026#39;category\u0026#39;: \u0026#39;electronics\u0026#39;, \u0026#39;brand\u0026#39;: \u0026#39;AccessoryCorp\u0026#39; }) print(\u0026#34;Laptop creation:\u0026#34;, laptop_result[\u0026#39;success\u0026#39;]) print(\u0026#34;Mouse creation:\u0026#34;, mouse_result[\u0026#39;success\u0026#39;]) Business operations with error handling:  # Safe stock update if laptop_result[\u0026#39;success\u0026#39;]: product_id = laptop_result[\u0026#39;item\u0026#39;][\u0026#39;PK\u0026#39;] # Successful stock decrease stock_result = product_repo.update_product_stock(product_id, -2) print(\u0026#34;Stock update:\u0026#34;, stock_result[\u0026#39;success\u0026#39;]) # Attempt overselling (should fail) oversell_result = product_repo.update_product_stock(product_id, -100) print(\u0026#34;Oversell prevented:\u0026#34;, not oversell_result[\u0026#39;success\u0026#39;]) \rScreenshot Location: Add screenshot showing repository business operations execution\n\rExercise 2: Caching Strategy Pattern Step 1: Application-Level Caching Implement intelligent caching for performance:\nimport time from typing import Dict, Any, Optional from threading import Lock class DynamoDBCacheManager: \u0026#34;\u0026#34;\u0026#34; Production-ready caching layer for DynamoDB \u0026#34;\u0026#34;\u0026#34; def __init__(self, repository: DynamoDBRepository, ttl_seconds: int = 300): self.repository = repository self.ttl = ttl_seconds self.cache = {} self.cache_stats = {\u0026#39;hits\u0026#39;: 0, \u0026#39;misses\u0026#39;: 0, \u0026#39;evictions\u0026#39;: 0} self.lock = Lock() def get_item_cached(self, key: Dict[str, str], projection: Optional[str] = None) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Get item with caching layer \u0026#34;\u0026#34;\u0026#34; cache_key = self._generate_cache_key(key, projection) with self.lock: # Check cache first if cache_key in self.cache: cached_item, timestamp = self.cache[cache_key] if time.time() - timestamp \u0026lt; self.ttl: self.cache_stats[\u0026#39;hits\u0026#39;] += 1 return { \u0026#39;success\u0026#39;: True, \u0026#39;item\u0026#39;: cached_item, \u0026#39;from_cache\u0026#39;: True } else: # Cache expired del self.cache[cache_key] self.cache_stats[\u0026#39;evictions\u0026#39;] += 1 # Cache miss - fetch from DynamoDB self.cache_stats[\u0026#39;misses\u0026#39;] += 1 result = self.repository.get_item(key, projection) if result[\u0026#39;success\u0026#39;] and result.get(\u0026#39;found\u0026#39;): with self.lock: self.cache[cache_key] = (result[\u0026#39;item\u0026#39;], time.time()) result[\u0026#39;from_cache\u0026#39;] = False return result def invalidate_cache(self, key: Dict[str, str], projection: Optional[str] = None): \u0026#34;\u0026#34;\u0026#34; Remove item from cache when updated \u0026#34;\u0026#34;\u0026#34; cache_key = self._generate_cache_key(key, projection) with self.lock: self.cache.pop(cache_key, None) def update_item_with_cache(self, key: Dict[str, str], updates: Dict[str, Any], condition: Optional[str] = None) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Update item and invalidate cache \u0026#34;\u0026#34;\u0026#34; result = self.repository.update_item(key, updates, condition) if result[\u0026#39;success\u0026#39;]: self.invalidate_cache(key) return result def get_cache_stats(self) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Get cache performance statistics \u0026#34;\u0026#34;\u0026#34; total_requests = self.cache_stats[\u0026#39;hits\u0026#39;] + self.cache_stats[\u0026#39;misses\u0026#39;] hit_rate = self.cache_stats[\u0026#39;hits\u0026#39;] / max(total_requests, 1) * 100 return { **self.cache_stats, \u0026#39;hit_rate_percent\u0026#39;: round(hit_rate, 2), \u0026#39;cache_size\u0026#39;: len(self.cache) } def _generate_cache_key(self, key: Dict[str, str], projection: Optional[str]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Generate consistent cache key \u0026#34;\u0026#34;\u0026#34; key_str = json.dumps(key, sort_keys=True) projection_str = projection or \u0026#39;full\u0026#39; return f\u0026#34;{key_str}:{projection_str}\u0026#34; # Enhanced product repository with caching class CachedProductRepository(ProductRepository): \u0026#34;\u0026#34;\u0026#34; Product repository with integrated caching \u0026#34;\u0026#34;\u0026#34; def __init__(self, table_name: str, cache_ttl: int = 300): super().__init__(table_name) self.cache_manager = DynamoDBCacheManager(self, cache_ttl) def get_product_cached(self, product_id: str) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Get product with caching \u0026#34;\u0026#34;\u0026#34; key = {\u0026#39;PK\u0026#39;: product_id, \u0026#39;SK\u0026#39;: \u0026#39;DETAILS\u0026#39;} return self.cache_manager.get_item_cached(key) def update_product_with_cache_invalidation(self, product_id: str, updates: Dict[str, Any]) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Update product and invalidate cache \u0026#34;\u0026#34;\u0026#34; key = {\u0026#39;PK\u0026#39;: product_id, \u0026#39;SK\u0026#39;: \u0026#39;DETAILS\u0026#39;} return self.cache_manager.update_item_with_cache(key, updates) Step 2: Cache Performance Testing Test caching effectiveness:\n# Initialize cached repository cached_repo = CachedProductRepository(\u0026#39;demo-ecommerce-freetier\u0026#39;, cache_ttl=60) # Performance comparison import time def test_cache_performance(product_id: str, iterations: int = 10): \u0026#34;\u0026#34;\u0026#34; Compare cached vs non-cached performance \u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Testing cache performance with {iterations}iterations...\u0026#34;) # Test cached access start_time = time.time() for i in range(iterations): result = cached_repo.get_product_cached(product_id) cached_duration = time.time() - start_time # Test direct access (non-cached) start_time = time.time() for i in range(iterations): result = cached_repo.get_product(product_id) direct_duration = time.time() - start_time # Cache statistics stats = cached_repo.cache_manager.get_cache_stats() print(f\u0026#34;Cached access: {cached_duration:.3f}s\u0026#34;) print(f\u0026#34;Direct access: {direct_duration:.3f}s\u0026#34;) print(f\u0026#34;Performance improvement: {direct_duration/cached_duration:.1f}x\u0026#34;) print(f\u0026#34;Cache hit rate: {stats[\u0026#39;hit_rate_percent\u0026#39;]}%\u0026#34;) return stats # Run performance test if laptop_result[\u0026#39;success\u0026#39;]: stats = test_cache_performance(laptop_result[\u0026#39;item\u0026#39;][\u0026#39;PK\u0026#39;]) \rScreenshot Location: Add screenshot showing cache performance comparison results\n\rExercise 3: Error Handling and Resilience Step 1: Comprehensive Error Handling Implement production-grade error handling:\nimport time import random from enum import Enum from typing import Dict, Any, Callable class DynamoDBErrorType(Enum): THROTTLING = \u0026#34;ProvisionedThroughputExceededException\u0026#34; CONDITIONAL_FAILED = \u0026#34;ConditionalCheckFailedException\u0026#34; ITEM_NOT_FOUND = \u0026#34;ResourceNotFoundException\u0026#34; VALIDATION_ERROR = \u0026#34;ValidationException\u0026#34; NETWORK_ERROR = \u0026#34;NetworkError\u0026#34; class ResilientDynamoDBClient: \u0026#34;\u0026#34;\u0026#34; Production resilient DynamoDB client with retry logic \u0026#34;\u0026#34;\u0026#34; def __init__(self, repository: DynamoDBRepository, max_retries: int = 3): self.repository = repository self.max_retries = max_retries self.retry_stats = {\u0026#39;total_attempts\u0026#39;: 0, \u0026#39;retries\u0026#39;: 0, \u0026#39;failures\u0026#39;: 0} def execute_with_retry(self, operation: Callable, *args, **kwargs) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Execute operation with exponential backoff retry \u0026#34;\u0026#34;\u0026#34; self.retry_stats[\u0026#39;total_attempts\u0026#39;] += 1 for attempt in range(self.max_retries + 1): try: result = operation(*args, **kwargs) if not result[\u0026#39;success\u0026#39;]: error_code = result.get(\u0026#39;error_code\u0026#39;, \u0026#39;\u0026#39;) if self._should_retry(error_code) and attempt \u0026lt; self.max_retries: wait_time = self._calculate_backoff(attempt) print(f\u0026#34;Attempt {attempt + 1}failed, retrying in {wait_time}s...\u0026#34;) time.sleep(wait_time) self.retry_stats[\u0026#39;retries\u0026#39;] += 1 continue else: self.retry_stats[\u0026#39;failures\u0026#39;] += 1 return self._handle_final_error(result, attempt + 1) return result except Exception as e: if attempt \u0026lt; self.max_retries: wait_time = self._calculate_backoff(attempt) print(f\u0026#34;Exception on attempt {attempt + 1}, retrying in {wait_time}s...\u0026#34;) time.sleep(wait_time) self.retry_stats[\u0026#39;retries\u0026#39;] += 1 continue else: self.retry_stats[\u0026#39;failures\u0026#39;] += 1 return { \u0026#39;success\u0026#39;: False, \u0026#39;error_code\u0026#39;: \u0026#39;UnexpectedError\u0026#39;, \u0026#39;error_message\u0026#39;: str(e), \u0026#39;attempts\u0026#39;: attempt + 1 } return {\u0026#39;success\u0026#39;: False, \u0026#39;error_code\u0026#39;: \u0026#39;MaxRetriesExceeded\u0026#39;} def _should_retry(self, error_code: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; Determine if error is retryable \u0026#34;\u0026#34;\u0026#34; retryable_errors = { \u0026#39;ProvisionedThroughputExceededException\u0026#39;, \u0026#39;ThrottlingException\u0026#39;, \u0026#39;RequestLimitExceeded\u0026#39;, \u0026#39;InternalServerError\u0026#39;, \u0026#39;ServiceUnavailable\u0026#39; } return error_code in retryable_errors def _calculate_backoff(self, attempt: int) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34; Calculate exponential backoff with jitter \u0026#34;\u0026#34;\u0026#34; base_wait = 2 ** attempt jitter = random.uniform(0.1, 0.5) return base_wait + jitter def _handle_final_error(self, result: Dict, attempts: int) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Handle final error with context \u0026#34;\u0026#34;\u0026#34; error_code = result.get(\u0026#39;error_code\u0026#39;, \u0026#39;Unknown\u0026#39;) error_context = { \u0026#39;success\u0026#39;: False, \u0026#39;error_code\u0026#39;: error_code, \u0026#39;error_message\u0026#39;: result.get(\u0026#39;error_message\u0026#39;, \u0026#39;Operation failed\u0026#39;), \u0026#39;attempts\u0026#39;: attempts, \u0026#39;retry_exhausted\u0026#39;: attempts \u0026gt; 1, \u0026#39;recommendation\u0026#39;: self._get_error_recommendation(error_code) } return error_context def _get_error_recommendation(self, error_code: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Provide actionable error recommendations \u0026#34;\u0026#34;\u0026#34; recommendations = { \u0026#39;ConditionalCheckFailedException\u0026#39;: \u0026#39;Data was modified by another process. Retry with fresh data.\u0026#39;, \u0026#39;ProvisionedThroughputExceededException\u0026#39;: \u0026#39;Increase table capacity or enable auto-scaling.\u0026#39;, \u0026#39;ValidationException\u0026#39;: \u0026#39;Check request parameters and data types.\u0026#39;, \u0026#39;ResourceNotFoundException\u0026#39;: \u0026#39;Verify table/index exists and spelling is correct.\u0026#39;, \u0026#39;ItemCollectionSizeLimitExceededException\u0026#39;: \u0026#39;Partition has too much data. Consider data redistribution.\u0026#39; } return recommendations.get(error_code, \u0026#39;Review error details and DynamoDB documentation.\u0026#39;) def get_retry_stats(self) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Get retry statistics for monitoring \u0026#34;\u0026#34;\u0026#34; total = self.retry_stats[\u0026#39;total_attempts\u0026#39;] retries = self.retry_stats[\u0026#39;retries\u0026#39;] failures = self.retry_stats[\u0026#39;failures\u0026#39;] return { **self.retry_stats, \u0026#39;retry_rate\u0026#39;: round(retries / max(total, 1) * 100, 2), \u0026#39;failure_rate\u0026#39;: round(failures / max(total, 1) * 100, 2) } # Usage example class ProductService: \u0026#34;\u0026#34;\u0026#34; Business service with resilient DynamoDB operations \u0026#34;\u0026#34;\u0026#34; def __init__(self, table_name: str): self.repository = ProductRepository(table_name) self.resilient_client = ResilientDynamoDBClient(self.repository) def create_product_safely(self, product_data: Dict[str, Any]) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Create product with retry logic \u0026#34;\u0026#34;\u0026#34; return self.resilient_client.execute_with_retry( self.repository.create_product, product_data ) def update_stock_safely(self, product_id: str, quantity_change: int) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Update stock with resilient error handling \u0026#34;\u0026#34;\u0026#34; return self.resilient_client.execute_with_retry( self.repository.update_product_stock, product_id, quantity_change ) Step 2: Circuit Breaker Pattern Implement circuit breaker for service protection:\nimport time from enum import Enum class CircuitState(Enum): CLOSED = \u0026#34;closed\u0026#34; # Normal operation OPEN = \u0026#34;open\u0026#34; # Failing, reject requests HALF_OPEN = \u0026#34;half_open\u0026#34; # Testing if service recovered class CircuitBreaker: \u0026#34;\u0026#34;\u0026#34; Circuit breaker pattern for DynamoDB operations \u0026#34;\u0026#34;\u0026#34; def __init__(self, failure_threshold: int = 5, timeout: int = 60): self.failure_threshold = failure_threshold self.timeout = timeout self.failure_count = 0 self.last_failure_time = None self.state = CircuitState.CLOSED def call(self, operation: Callable, *args, **kwargs) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Execute operation through circuit breaker \u0026#34;\u0026#34;\u0026#34; if self.state == CircuitState.OPEN: if self._should_attempt_reset(): self.state = CircuitState.HALF_OPEN print(\u0026#34;Circuit breaker moving to HALF_OPEN state\u0026#34;) else: return { \u0026#39;success\u0026#39;: False, \u0026#39;error_code\u0026#39;: \u0026#39;CircuitBreakerOpen\u0026#39;, \u0026#39;error_message\u0026#39;: \u0026#39;Service temporarily unavailable\u0026#39; } try: result = operation(*args, **kwargs) if result[\u0026#39;success\u0026#39;]: self._on_success() else: self._on_failure() return result except Exception as e: self._on_failure() return { \u0026#39;success\u0026#39;: False, \u0026#39;error_code\u0026#39;: \u0026#39;ServiceError\u0026#39;, \u0026#39;error_message\u0026#39;: str(e) } def _should_attempt_reset(self) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; Check if enough time has passed to attempt reset \u0026#34;\u0026#34;\u0026#34; return (time.time() - self.last_failure_time) \u0026gt;= self.timeout def _on_success(self): \u0026#34;\u0026#34;\u0026#34; Handle successful operation \u0026#34;\u0026#34;\u0026#34; self.failure_count = 0 self.state = CircuitState.CLOSED def _on_failure(self): \u0026#34;\u0026#34;\u0026#34; Handle failed operation \u0026#34;\u0026#34;\u0026#34; self.failure_count += 1 self.last_failure_time = time.time() if self.failure_count \u0026gt;= self.failure_threshold: self.state = CircuitState.OPEN print(f\u0026#34;Circuit breaker OPEN after {self.failure_count}failures\u0026#34;) def get_state(self) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Get current circuit breaker state \u0026#34;\u0026#34;\u0026#34; return { \u0026#39;state\u0026#39;: self.state.value, \u0026#39;failure_count\u0026#39;: self.failure_count, \u0026#39;failure_threshold\u0026#39;: self.failure_threshold, \u0026#39;last_failure\u0026#39;: self.last_failure_time } \rScreenshot Location: Add screenshot showing circuit breaker implementation and state management\n\rExercise 4: Connection Pool and Resource Management Step 1: Connection Pool Implementation Optimize resource usage with connection pooling:\nimport boto3 from botocore.config import Config import threading from queue import Queue class DynamoDBConnectionPool: \u0026#34;\u0026#34;\u0026#34; Production connection pool for DynamoDB clients \u0026#34;\u0026#34;\u0026#34; def __init__(self, max_connections: int = 20, region: str = \u0026#39;us-east-1\u0026#39;): self.max_connections = max_connections self.region = region self.pool = Queue(maxsize=max_connections) self.lock = threading.Lock() self.active_connections = 0 # Pre-create connections for _ in range(max_connections): client = self._create_client() self.pool.put(client) def _create_client(self): \u0026#34;\u0026#34;\u0026#34; Create optimized DynamoDB client \u0026#34;\u0026#34;\u0026#34; config = Config( retries={\u0026#39;max_attempts\u0026#39;: 3, \u0026#39;mode\u0026#39;: \u0026#39;adaptive\u0026#39;}, max_pool_connections=50, connect_timeout=5, read_timeout=10 ) return boto3.client(\u0026#39;dynamodb\u0026#39;, region_name=self.region, config=config) def get_client(self): \u0026#34;\u0026#34;\u0026#34; Get client from pool (context manager) \u0026#34;\u0026#34;\u0026#34; return ConnectionContextManager(self) def _acquire(self): \u0026#34;\u0026#34;\u0026#34; Acquire client from pool \u0026#34;\u0026#34;\u0026#34; with self.lock: if not self.pool.empty(): client = self.pool.get() self.active_connections += 1 return client elif self.active_connections \u0026lt; self.max_connections: client = self._create_client() self.active_connections += 1 return client else: # Wait for available connection client = self.pool.get(block=True, timeout=30) return client def _release(self, client): \u0026#34;\u0026#34;\u0026#34; Return client to pool \u0026#34;\u0026#34;\u0026#34; with self.lock: self.pool.put(client) self.active_connections -= 1 def get_stats(self) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Get connection pool statistics \u0026#34;\u0026#34;\u0026#34; return { \u0026#39;max_connections\u0026#39;: self.max_connections, \u0026#39;active_connections\u0026#39;: self.active_connections, \u0026#39;available_connections\u0026#39;: self.pool.qsize(), \u0026#39;pool_utilization\u0026#39;: round(self.active_connections / self.max_connections * 100, 2) } class ConnectionContextManager: \u0026#34;\u0026#34;\u0026#34; Context manager for connection pool usage \u0026#34;\u0026#34;\u0026#34; def __init__(self, pool: DynamoDBConnectionPool): self.pool = pool self.client = None def __enter__(self): self.client = self.pool._acquire() return self.client def __exit__(self, exc_type, exc_val, exc_tb): if self.client: self.pool._release(self.client) # Enterprise repository with connection pooling class EnterpriseProductRepository: \u0026#34;\u0026#34;\u0026#34; Enterprise-grade repository with connection pooling \u0026#34;\u0026#34;\u0026#34; def __init__(self, table_name: str, connection_pool: DynamoDBConnectionPool): self.table_name = table_name self.pool = connection_pool def get_product_pooled(self, product_id: str) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Get product using connection pool \u0026#34;\u0026#34;\u0026#34; try: with self.pool.get_client() as client: response = client.get_item( TableName=self.table_name, Key={ \u0026#39;PK\u0026#39;: {\u0026#39;S\u0026#39;: product_id}, \u0026#39;SK\u0026#39;: {\u0026#39;S\u0026#39;: \u0026#39;DETAILS\u0026#39;} } ) if \u0026#39;Item\u0026#39; in response: return { \u0026#39;success\u0026#39;: True, \u0026#39;item\u0026#39;: response[\u0026#39;Item\u0026#39;], \u0026#39;pool_stats\u0026#39;: self.pool.get_stats() } else: return {\u0026#39;success\u0026#39;: False, \u0026#39;error\u0026#39;: \u0026#39;Item not found\u0026#39;} except Exception as e: return {\u0026#39;success\u0026#39;: False, \u0026#39;error\u0026#39;: str(e)} def batch_get_products(self, product_ids: List[str]) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Batch get products with connection pooling \u0026#34;\u0026#34;\u0026#34; try: with self.pool.get_client() as client: keys = [ {\u0026#39;PK\u0026#39;: {\u0026#39;S\u0026#39;: pid}, \u0026#39;SK\u0026#39;: {\u0026#39;S\u0026#39;: \u0026#39;DETAILS\u0026#39;}} for pid in product_ids ] response = client.batch_get_item( RequestItems={ self.table_name: {\u0026#39;Keys\u0026#39;: keys} } ) items = response.get(\u0026#39;Responses\u0026#39;, {}).get(self.table_name, []) return { \u0026#39;success\u0026#39;: True, \u0026#39;items\u0026#39;: items, \u0026#39;count\u0026#39;: len(items), \u0026#39;pool_stats\u0026#39;: self.pool.get_stats() } except Exception as e: return {\u0026#39;success\u0026#39;: False, \u0026#39;error\u0026#39;: str(e)} Step 2: Resource Optimization Testing Test connection pool efficiency:\n# Initialize connection pool connection_pool = DynamoDBConnectionPool(max_connections=10) enterprise_repo = EnterpriseProductRepository(\u0026#39;demo-ecommerce-freetier\u0026#39;, connection_pool) # Test concurrent access import concurrent.futures import time def test_concurrent_access(num_threads: int = 5, requests_per_thread: int = 10): \u0026#34;\u0026#34;\u0026#34; Test connection pool under concurrent load \u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Testing with {num_threads}threads, {requests_per_thread}requests each\u0026#34;) def worker_thread(thread_id: int): \u0026#34;\u0026#34;\u0026#34; Worker thread for testing \u0026#34;\u0026#34;\u0026#34; results = [] for i in range(requests_per_thread): if laptop_result[\u0026#39;success\u0026#39;]: result = enterprise_repo.get_product_pooled(laptop_result[\u0026#39;item\u0026#39;][\u0026#39;PK\u0026#39;]) results.append(result[\u0026#39;success\u0026#39;]) return sum(results) start_time = time.time() with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor: futures = [ executor.submit(worker_thread, i) for i in range(num_threads) ] total_successes = sum(future.result() for future in futures) duration = time.time() - start_time total_requests = num_threads * requests_per_thread print(f\u0026#34;Completed {total_requests}requests in {duration:.2f}s\u0026#34;) print(f\u0026#34;Success rate: {total_successes}/{total_requests}({total_successes/total_requests*100:.1f}%)\u0026#34;) print(f\u0026#34;Throughput: {total_requests/duration:.1f}requests/second\u0026#34;) print(f\u0026#34;Pool stats: {connection_pool.get_stats()}\u0026#34;) # Run concurrent test test_concurrent_access(num_threads=5, requests_per_thread=20) \rScreenshot Location: Add screenshot showing concurrent access test results and pool statistics\n\rExercise 5: Production Monitoring and Observability Step 1: Comprehensive Metrics Collection Implement production monitoring:\nimport time import json from datetime import datetime from typing import Dict, Any from collections import defaultdict class DynamoDBMetricsCollector: \u0026#34;\u0026#34;\u0026#34; Production metrics collection for DynamoDB operations \u0026#34;\u0026#34;\u0026#34; def __init__(self): self.metrics = defaultdict(list) self.operation_counts = defaultdict(int) self.error_counts = defaultdict(int) self.start_time = time.time() def record_operation(self, operation: str, duration: float, success: bool, consumed_capacity: float = 0, error_code: str = None): \u0026#34;\u0026#34;\u0026#34; Record operation metrics \u0026#34;\u0026#34;\u0026#34; timestamp = datetime.utcnow().isoformat() metric = { \u0026#39;timestamp\u0026#39;: timestamp, \u0026#39;operation\u0026#39;: operation, \u0026#39;duration_ms\u0026#39;: round(duration * 1000, 2), \u0026#39;success\u0026#39;: success, \u0026#39;consumed_capacity\u0026#39;: consumed_capacity, \u0026#39;error_code\u0026#39;: error_code } self.metrics[operation].append(metric) self.operation_counts[operation] += 1 if not success and error_code: self.error_counts[error_code] += 1 def get_operation_stats(self, operation: str) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Get statistics for specific operation \u0026#34;\u0026#34;\u0026#34; ops = self.metrics[operation] if not ops: return {\u0026#39;error\u0026#39;: \u0026#39;No data for operation\u0026#39;} durations = [op[\u0026#39;duration_ms\u0026#39;] for op in ops] successes = [op for op in ops if op[\u0026#39;success\u0026#39;]] return { \u0026#39;operation\u0026#39;: operation, \u0026#39;total_requests\u0026#39;: len(ops), \u0026#39;successful_requests\u0026#39;: len(successes), \u0026#39;success_rate\u0026#39;: round(len(successes) / len(ops) * 100, 2), \u0026#39;avg_duration_ms\u0026#39;: round(sum(durations) / len(durations), 2), \u0026#39;min_duration_ms\u0026#39;: min(durations), \u0026#39;max_duration_ms\u0026#39;: max(durations), \u0026#39;p95_duration_ms\u0026#39;: self._percentile(durations, 95), \u0026#39;total_capacity_consumed\u0026#39;: sum(op[\u0026#39;consumed_capacity\u0026#39;] for op in ops) } def get_overall_stats(self) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Get comprehensive system statistics \u0026#34;\u0026#34;\u0026#34; uptime = time.time() - self.start_time total_operations = sum(self.operation_counts.values()) total_errors = sum(self.error_counts.values()) return { \u0026#39;uptime_seconds\u0026#39;: round(uptime, 2), \u0026#39;total_operations\u0026#39;: total_operations, \u0026#39;total_errors\u0026#39;: total_errors, \u0026#39;error_rate\u0026#39;: round(total_errors / max(total_operations, 1) * 100, 2), \u0026#39;operations_per_second\u0026#39;: round(total_operations / uptime, 2), \u0026#39;operation_breakdown\u0026#39;: dict(self.operation_counts), \u0026#39;error_breakdown\u0026#39;: dict(self.error_counts) } def _percentile(self, data: List[float], percentile: int) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34; Calculate percentile value \u0026#34;\u0026#34;\u0026#34; sorted_data = sorted(data) index = int(len(sorted_data) * percentile / 100) return sorted_data[min(index, len(sorted_data) - 1)] def export_metrics(self) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Export metrics in JSON format \u0026#34;\u0026#34;\u0026#34; return json.dumps({ \u0026#39;overall_stats\u0026#39;: self.get_overall_stats(), \u0026#39;operation_stats\u0026#39;: { op: self.get_operation_stats(op) for op in self.metrics.keys() }, \u0026#39;raw_metrics\u0026#39;: dict(self.metrics) }, indent=2) class MonitoredProductRepository(ProductRepository): \u0026#34;\u0026#34;\u0026#34; Product repository with integrated monitoring \u0026#34;\u0026#34;\u0026#34; def __init__(self, table_name: str): super().__init__(table_name) self.metrics = DynamoDBMetricsCollector() def get_product_monitored(self, product_id: str) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Get product with metrics collection \u0026#34;\u0026#34;\u0026#34; start_time = time.time() try: result = self.get_product(product_id) duration = time.time() - start_time self.metrics.record_operation( operation=\u0026#39;get_product\u0026#39;, duration=duration, success=result[\u0026#39;success\u0026#39;], consumed_capacity=0.5, # Estimate for GetItem error_code=result.get(\u0026#39;error_code\u0026#39;) ) return result except Exception as e: duration = time.time() - start_time self.metrics.record_operation( operation=\u0026#39;get_product\u0026#39;, duration=duration, success=False, error_code=\u0026#39;UnexpectedError\u0026#39; ) raise def create_product_monitored(self, product_data: Dict[str, Any]) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Create product with metrics collection \u0026#34;\u0026#34;\u0026#34; start_time = time.time() try: result = self.create_product(product_data) duration = time.time() - start_time self.metrics.record_operation( operation=\u0026#39;create_product\u0026#39;, duration=duration, success=result[\u0026#39;success\u0026#39;], consumed_capacity=1.0, # Estimate for PutItem error_code=result.get(\u0026#39;error_code\u0026#39;) ) return result except Exception as e: duration = time.time() - start_time self.metrics.record_operation( operation=\u0026#39;create_product\u0026#39;, duration=duration, success=False, error_code=\u0026#39;UnexpectedError\u0026#39; ) raise def get_performance_report(self) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Generate comprehensive performance report \u0026#34;\u0026#34;\u0026#34; return { \u0026#39;timestamp\u0026#39;: datetime.utcnow().isoformat(), \u0026#39;repository_metrics\u0026#39;: self.metrics.get_overall_stats(), \u0026#39;operation_details\u0026#39;: { \u0026#39;get_product\u0026#39;: self.metrics.get_operation_stats(\u0026#39;get_product\u0026#39;), \u0026#39;create_product\u0026#39;: self.metrics.get_operation_stats(\u0026#39;create_product\u0026#39;) } } Step 2: Performance Monitoring Demo Demonstrate production monitoring capabilities:\n# Initialize monitored repository monitored_repo = MonitoredProductRepository(\u0026#39;demo-ecommerce-freetier\u0026#39;) # Generate sample operations for monitoring def simulate_production_traffic(): \u0026#34;\u0026#34;\u0026#34; Simulate production traffic patterns \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Simulating production traffic...\u0026#34;) # Create several products for i in range(5): result = monitored_repo.create_product_monitored({ \u0026#39;name\u0026#39;: f\u0026#39;Monitored Product {i+1}\u0026#39;, \u0026#39;price\u0026#39;: 100 + i * 10, \u0026#39;stock\u0026#39;: 20, \u0026#39;category\u0026#39;: \u0026#39;monitoring-test\u0026#39; }) if result[\u0026#39;success\u0026#39;]: product_id = result[\u0026#39;item\u0026#39;][\u0026#39;PK\u0026#39;] # Read the product multiple times (simulating user traffic) for _ in range(3): monitored_repo.get_product_monitored(product_id) # Generate performance report report = monitored_repo.get_performance_report() print(\u0026#34;\\n=== PERFORMANCE REPORT ===\u0026#34;) print(f\u0026#34;Total Operations: {report[\u0026#39;repository_metrics\u0026#39;][\u0026#39;total_operations\u0026#39;]}\u0026#34;) print(f\u0026#34;Error Rate: {report[\u0026#39;repository_metrics\u0026#39;][\u0026#39;error_rate\u0026#39;]}%\u0026#34;) print(f\u0026#34;Ops/Second: {report[\u0026#39;repository_metrics\u0026#39;][\u0026#39;operations_per_second\u0026#39;]}\u0026#34;) print(\u0026#34;\\n=== GET PRODUCT STATS ===\u0026#34;) get_stats = report[\u0026#39;operation_details\u0026#39;][\u0026#39;get_product\u0026#39;] if \u0026#39;error\u0026#39; not in get_stats: print(f\u0026#34;Success Rate: {get_stats[\u0026#39;success_rate\u0026#39;]}%\u0026#34;) print(f\u0026#34;Avg Latency: {get_stats[\u0026#39;avg_duration_ms\u0026#39;]}ms\u0026#34;) print(f\u0026#34;P95 Latency: {get_stats[\u0026#39;p95_duration_ms\u0026#39;]}ms\u0026#34;) print(\u0026#34;\\n=== CREATE PRODUCT STATS ===\u0026#34;) create_stats = report[\u0026#39;operation_details\u0026#39;][\u0026#39;create_product\u0026#39;] if \u0026#39;error\u0026#39; not in create_stats: print(f\u0026#34;Success Rate: {create_stats[\u0026#39;success_rate\u0026#39;]}%\u0026#34;) print(f\u0026#34;Avg Latency: {create_stats[\u0026#39;avg_duration_ms\u0026#39;]}ms\u0026#34;) return report # Run simulation performance_report = simulate_production_traffic() \rScreenshot Location: Add screenshot showing performance monitoring output with detailed metrics\n\rProduction Patterns Best Practices Architecture Guidelines Enterprise-grade DynamoDB architecture:\n Separation of Concerns: Clear repository/service layers Error Handling: Comprehensive error management Resource Management: Connection pooling and optimization Monitoring: Detailed metrics and observability Caching: Intelligent caching strategies  Performance Optimization Production performance patterns:\n Connection Reuse: Pool database connections Batch Operations: Group related operations Async Processing: Non-blocking operation patterns Circuit Breakers: Protect against cascade failures Graceful Degradation: Fallback strategies  Operational Excellence Production readiness checklist:\n âœ… Comprehensive logging: All operations logged âœ… Metrics collection: Performance and business metrics âœ… Error handling: Graceful failure management âœ… Resource optimization: Efficient resource usage âœ… Monitoring dashboards: Real-time operational visibility  Production Mastery: You\u0026rsquo;ve mastered enterprise-grade DynamoDB patterns! Your applications are now ready for production with robust error handling, performance optimization, and comprehensive monitoring.\n\rExercise Summary You\u0026rsquo;ve implemented production-ready patterns:\n âœ… Repository pattern with clean data access abstraction âœ… Caching strategies for performance optimization âœ… Error handling with retry logic and circuit breakers âœ… Connection pooling for resource efficiency âœ… Comprehensive monitoring for operational excellence âœ… Production patterns used by enterprise applications  Module 6 Complete Congratulations! You\u0026rsquo;ve mastered advanced DynamoDB patterns and can now build enterprise-grade applications with:\nğŸš€ Batch operations for maximum efficiency ğŸ›¡ï¸ Conditional updates for data integrity âš¡ Query optimization for performance and cost efficiency ğŸ¢ Production patterns for enterprise reliability\nYour DynamoDB expertise now matches that of companies serving millions of users - all while staying within AWS Free Tier limits!\n"
},
{
	"uri": "http://localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]