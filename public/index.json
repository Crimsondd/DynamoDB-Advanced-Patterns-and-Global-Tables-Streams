[
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/1-setup-infrastructure/",
	"title": "1. Setup &amp; Infrastructure Deployment",
	"tags": [],
	"description": "",
	"content": "Setup \u0026amp; Infrastructure Deployment ğŸš€ Complete guide for setting up AWS infrastructure for DynamoDB Advanced Patterns workshop\nThis module provides the foundational setup required for the DynamoDB Advanced Patterns workshop, ensuring all participants have a working environment using AWS Free Tier.\nğŸ“‹ Learning Objectives By the end of this module, you will:  âœ… Verify AWS account and Free Tier eligibility âœ… Navigate AWS Console efficiently âœ… Deploy infrastructure via CloudFormation âœ… Verify all resources created successfully âœ… Setup monitoring and billing alerts  ğŸ—ï¸ Architecture Overview We\u0026rsquo;ll deploy infrastructure across two AWS regions: ğŸ¯ What We\u0026rsquo;ll Build We\u0026rsquo;re building a simplified e-commerce platform with:  Users: Customer profiles and authentication Products: Catalog with categories and pricing Orders: Shopping cart and order management Real-time processing: Stream-based updates Global availability: Multi-region deployment  ğŸ“¦ Resources Created This CloudFormation template will create:  DynamoDB Table: Global table with streams enabled Lambda Function: Stream processor for real-time updates IAM Roles: Secure access policies CloudWatch Dashboard: Monitoring and metrics Billing Alerts: Cost protection mechanisms  ğŸš€ Prerequisites Before starting, ensure you have:  AWS Account with Free Tier eligibility Administrative access to AWS Console Basic understanding of AWS services Modern web browser (Chrome, Firefox, Safari)  Let\u0026rsquo;s begin with the infrastructure setup that will support our multi-region DynamoDB implementation.  You can choose Personal or Business account  Add payment method  Enter your credit card information and select Verify and Add.  Note: You can choose a different address for your account by selecting Use a new address before Verify and Add.    Verify your phone number  Enter the phone number. Enter the security check code then select Call me now. AWS will contact and verify account opening.  Select Support Plan  In the Select a support plan page, select an effective plan, to compare plans, see Compare AWS Support Plans.  Wait for your account to be activated  After selecting Support plan, the account is usually activated after a few minutes, but the process can take up to 24 hours. You will still be able to log in to your AWS account at this time, the AWS Home page may show a â€œComplete Sign Upâ€ button during this time, even if you have completed all the steps in the registration section. After receiving an email confirming your account has been activated, you can access all AWS services.  Important  The following AWS Identity and Access Management (IAM) actions will reach the end of standard support on July 2023: aws-portal:ModifyAccount and aws-portal:ViewAccount. See the Using fine-grained AWS Billing actions to replace these actions with fine-grained actions so you have access to AWS Billing, AWS Cost Management, and AWS accounts consoles. If you created your AWS account or AWS Organizations Management account before March 6, 2023, the fine-grained actions will be effective starting July 2023. We recommend you to add the fine-grained actions, but not remove your existing permissions with aws-portal or purchase-orders prefixes. If you created your AWS account or AWS Organizations Management account on or after March 6, 2023, the fine-grained actions are effective immediately. AWS assigns the following unique identifiers to each AWS account: AWS account ID: A 12-digit number, such as 012345678901, that uniquely identifies an AWS account. Many AWS resources include the account ID in their Amazon Resource Names (ARNs). The account ID portion distinguishes resources in one account from the resources in another account. If you\u0026rsquo;re an AWS Identity and Access Management (IAM) user, you can sign in to the AWS Management Console using either the account ID or account alias. While account IDs, like any identifying information, should be used and shared carefully, they are not considered secret, sensitive, or confidential information. Canonical user ID: An alpha-numeric identifier, such as 79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be, that is an obfuscated form of the AWS account ID. You can use this ID to identify an AWS account when granting cross-account access to buckets and objects using Amazon Simple Storage Service (Amazon S3). You can retrieve the canonical user ID for your AWS account as either the root user or an IAM user. You must be authenticated with AWS to view these identifiers.  Warning Do not provide your AWS credentials (including passwords and access keys) to a third party that needs your AWS account identifiers to share AWS resources with you. Doing so would give them the same access to the AWS account that you have.\n"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/",
	"title": "DynamoDB Advanced Patterns Workshop",
	"tags": [],
	"description": "",
	"content": "DynamoDB Advanced Patterns Building Multi-Region Architectures with Global Tables and Streams Overview In this comprehensive workshop, you will be building a Multi-Region E-commerce Platform using DynamoDB Advanced Patterns and AWS Free Tier. You will learn Single Table Design, implement Global Tables for multi-region replication, and build real-time stream processing with Lambda. Finally, we will implement comprehensive monitoring and optimization strategies while maintaining strict cost control within Free Tier limits.\nSingle Table Design Single Table Design is a DynamoDB modeling approach where you store multiple entity types in one table using composite keys. This pattern optimizes for performance and cost by reducing the number of requests and leveraging DynamoDB\u0026rsquo;s partition-based architecture. When implemented correctly, it provides sub-millisecond query performance while minimizing capacity consumption.\nAs a best practice, design your access patterns first before creating your table structure. Single Table Design requires careful planning of partition keys (PK) and sort keys (SK) to support all your query patterns efficiently. This workshop uses a proven e-commerce data model that supports 6 optimized access patterns while staying within Free Tier limits.\n\rGlobal Tables Multi-Region Global Tables provide fully managed multi-region, multi-active database replication. Data written to any region is automatically replicated to all other regions within seconds. This enables you to build globally distributed applications with local read and write access, improving performance and providing disaster recovery capabilities.\nDynamoDB Streams \u0026amp; Lambda DynamoDB Streams capture data modification events in your table in near real-time. When combined with AWS Lambda, you can build event-driven architectures that automatically process changes, update derived data, send notifications, or trigger business workflows. This pattern is essential for building reactive, scalable applications.\nGlobal Secondary Indexes (GSI) Global Secondary Indexes allow you to query your data using different access patterns than your main table. GSIs have their own partition and sort keys, enabling efficient queries across different dimensions of your data. Proper GSI design is crucial for performance optimization and cost control.\nMonitoring \u0026amp; Cost Optimization CloudWatch monitoring provides real-time visibility into your DynamoDB performance, capacity utilization, and costs. Combined with billing alerts and Free Tier tracking, you can ensure optimal performance while maintaining strict cost control. This workshop implements comprehensive monitoring dashboards and automated alerting.\nFree Tier Compliance AWS Free Tier provides generous limits for learning and experimentation. This workshop is designed to use only 60% of available Free Tier resources, ensuring zero cost while providing enterprise-grade learning experience. All participants will implement production-ready patterns without incurring any charges.\nMain Content  Setup \u0026amp; Infrastructure Deployment Single Table Design Implementation Global Tables Multi-Region Setup DynamoDB Streams \u0026amp; Lambda Processing Monitoring \u0026amp; Performance Optimization Advanced Patterns \u0026amp; Best Practices Cleanup \u0026amp; Resource Management  "
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/2-single-table-design/",
	"title": "2. Single Table Design Implementation",
	"tags": [],
	"description": "",
	"content": "Single Table Design Implementation ğŸ“Š Learn how to implement DynamoDB Single Table Design patterns for optimal performance and cost efficiency\nOverview  Single Table Design is a revolutionary approach to data modeling in DynamoDB. Instead of using multiple tables like in relational databases, we store all entity types (Users, Products, Orders) in one table using composite keys for relationships.  Why Single Table Design? Traditional Relational Approach Problems:  Multiple tables = Multiple queries = Higher latency JOINs are expensive and not available in DynamoDB Inconsistent performance across different query patterns Higher costs from managing multiple tables  DynamoDB Single Table Benefits:  Single query retrieves related data Consistent performance across all access patterns Lower costs with fewer tables and operations Atomic transactions across entity types  Learning Objectives By the end of this module, you will:  âœ… Understand Single Table Design principles and benefits âœ… Design composite keys (PK + SK) for multiple entity types âœ… Create and query data using DynamoDB Console âœ… Implement Global Secondary Indexes (GSI) for flexible access patterns âœ… Analyze performance metrics and costs  Module Duration: 90 minutes  Theory: 20 minutes - Core concepts and principles Demo: 25 minutes - Console navigation and data creation Hands-on: 35 minutes - Create your own e-commerce data Review: 10 minutes - Performance analysis and Q\u0026amp;A  E-commerce Data Model Overview We\u0026rsquo;ll build a simplified e-commerce platform with these entities:    PK SK Entity Data     USER#user1 PROFILE User name, email, phone   USER#user1 ORDER#ord1 Order status, total, date   PRODUCT#p1 DETAILS Product name, price, category   ORDER#ord1 ITEM#p1 OrderItem quantity, price, product    Access Patterns We\u0026rsquo;ll Implement    Pattern Description Query Method     1 Get user profile PK = USER#id, SK = PROFILE   2 Get user\u0026rsquo;s orders PK = USER#id, SK begins_with ORDER#   3 Get order details with items PK = ORDER#id   4 Get products by category GSI1: CATEGORY# queries   5 Get products by price range GSI2: PRICE# queries   6 Get orders by status GSI2: STATUS# queries    Key Concepts Composite Keys Strategy  Partition Key (PK): Groups related items together Sort Key (SK): Enables range queries and relationships GSI Keys: Enable additional query patterns  Entity Namespacing  USER#: All user-related data PRODUCT#: All product-related data ORDER#: All order-related data CATEGORY#: Product groupings STATUS#: Order status groupings  Design Philosophy: In Single Table Design, we model our table structure based on HOW we\u0026rsquo;ll query the data, not how we\u0026rsquo;ll store it. This is the opposite of relational database design!\n\rWhat You\u0026rsquo;ll Build By the end of this module, you\u0026rsquo;ll have created:  User profiles with proper key structure Product catalog with category and price indexing Order management with item relationships Efficient queries using table and GSI patterns Performance insights from CloudWatch metrics  Cost Safety: All exercises use minimal data and stay well within AWS Free Tier limits. Monitor the CloudWatch dashboard to track usage.\n\rPrerequisites Before starting this module, ensure you have:  Completed Module 1: Infrastructure Setup DynamoDB table demo-ecommerce-freetier is Active Access to AWS Console with DynamoDB permissions Basic understanding of NoSQL concepts  Ready to revolutionize your data modeling approach? Let\u0026rsquo;s dive into Single Table Design!\n"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/3-global-tables-setup/",
	"title": "3. Global Tables Multi-Region Setup",
	"tags": [],
	"description": "",
	"content": "Global Tables Multi-Region Setup ğŸŒ Set up multi-region DynamoDB for worldwide access\nOverview Global Tables transforms your single-region DynamoDB table into a globally distributed database that serves users worldwide with low latency.\nWhy Global Tables? The Problem  High Latency: Users far from database experience slow response No Disaster Recovery: Single point of failure Limited Scale: All traffic through one region  The Solution Before: US-EAST-1 only â†’ High latency for EU users After: US-EAST-1 + EU-WEST-1 â†’ Low latency globally What You\u0026rsquo;ll Learn  Verify Global Setup: Check multi-region configuration Test Replication: Write in one region, read in another Multi-Region Operations: Handle global data scenarios  Key Benefits  Sub-10ms latency for users worldwide Automatic replication between regions (0.5-2 seconds) Free Tier friendly: Applies per region Built-in disaster recovery  Global Tables Basics Replication Flow 1. Write to US-EAST-1 â†’ ORDER#12345 created 2. DynamoDB Streams captures change 3. Auto-replication to EU-WEST-1 4. ORDER#12345 available in Europe (1-2 seconds) Key Features  Bi-directional: Read/write from any region Eventually Consistent: Changes sync within seconds Conflict Resolution: Last Writer Wins Zero downtime: Regional failover automatic  Module Contents  Global Tables Overview - Understand the architecture Verify Global Setup - Check your configuration Multi-Region Operations - Test cross-region functionality  Setup: Your CloudFormation deployment already configured Global Tables between US-East-1 and EU-West-1.\n\r\r3.1 Global Tables Overview\r\r\r3.2 Verify Global Setup\r\r\r3.3 Multi-Region Operations\r\r\rLet\u0026rsquo;s make your DynamoDB table globally accessible!\n"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/4-streams-lambda-processing/",
	"title": "4. Streams &amp; Lambda Processing",
	"tags": [],
	"description": "",
	"content": "DynamoDB Streams \u0026amp; Lambda Processing âš¡ Set up real-time event processing for your DynamoDB table\nModule Overview Learn how to capture and process data changes in real-time using DynamoDB Streams and AWS Lambda.\nWhat You\u0026rsquo;ll Learn  Enable Streams: Turn on change tracking for your table Create Lambda: Build a function to process events Test Processing: See events trigger in real-time  Simple Architecture DynamoDB Table â†’ DynamoDB Streams â†’ Lambda Function â†“ â†“ â†“ Data Change Capture Event Process Event Key Benefits  Real-time: Process changes instantly Automatic: No polling required Scalable: Lambda handles concurrency Cost-effective: Pay only for usage  Module Contents  Stream Configuration - Enable streams on your table Lambda Function Setup - Create and connect Lambda  Free Tier: Lambda provides 1 million free requests per month. This demo uses minimal resources.\n\r\r4.1 Stream Configuration\r\r\r4.2 Lambda Function Setup\r\r\rLet\u0026rsquo;s add real-time processing to your DynamoDB table.\nReal-time Processing\n Process data changes within 100-500 milliseconds No polling required - events pushed automatically Scale to millions of events per second  Event-Driven Architecture\n Decouple data storage from business logic Trigger multiple downstream systems Build reactive, responsive applications  Cost Effective\n Pay only for actual processing time AWS Free Tier includes 1M Lambda invocations No infrastructure to manage  Common Use Cases    Pattern Trigger Action     Audit Trail Any change Log to S3/CloudWatch   Cache Invalidation Item update Clear Redis/ElastiCache   Notifications Order created Send email/SMS   Analytics User activity Update metrics dashboard   Search Index Product change Update Elasticsearch   Workflow Status change Trigger Step Functions    Stream Processing Patterns Fan-out Pattern: One change triggers multiple Lambda functions\nDynamoDB Change â†’ Stream â†’ Lambda 1 (Email)\râ†’ Lambda 2 (Analytics) â†’ Lambda 3 (Cache Update)\rPipeline Pattern: Sequential processing through multiple stages\nOrder Created â†’ Validate â†’ Process Payment â†’ Update Inventory â†’ Ship\rAggregation Pattern: Combine multiple changes into summaries\nSales Records â†’ Real-time Revenue Dashboard\rUser Actions â†’ Activity Analytics\rPerformance Characteristics  Latency: Typically 100-500ms from change to processing Throughput: Scales automatically with your data volume Reliability: Automatic retries and error handling Ordering: Changes processed in order per item Retention: Stream records available for 24 hours  Module Structure This module is organized into hands-on sections that build upon each other:  Stream Configuration - Enable and configure DynamoDB Streams Lambda Function Setup - Create and deploy stream processing functions Event Processing Practice - Test with real data changes Monitoring \u0026amp; Debugging - Track performance and troubleshoot issues  Each section includes:  âœ… Step-by-step AWS Console instructions âœ… Code examples and templates âœ… Screenshot placeholders for documentation âœ… Troubleshooting guides âœ… Real-world scenarios  Prerequisites Before starting this module, ensure you have:  âœ… Completed Module 1 (DynamoDB table setup) âœ… Basic understanding of AWS Lambda âœ… Familiarity with JSON and event-driven concepts âœ… AWS Console access with appropriate permissions  Learning Objectives By the end of this module, you will:  Understand DynamoDB Streams architecture and event flow Configure Lambda functions to process stream events Implement common event-driven patterns Monitor stream processing performance Debug stream processing issues Design scalable event-driven applications  "
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/5-monitoring-optimization/",
	"title": "5. Monitoring &amp; Optimization",
	"tags": [],
	"description": "",
	"content": "Monitoring \u0026amp; Optimization ğŸ“ˆ Essential monitoring and cost optimization for your DynamoDB workshop\nModule Overview Learn how to monitor your DynamoDB table and stay within Free Tier limits with practical CloudWatch dashboards and cost tracking.\nWhat You\u0026rsquo;ll Learn  CloudWatch Dashboard: Create a monitoring view for your table Cost Tracking: Monitor Free Tier usage to avoid charges Basic Alerts: Set up simple alarms for capacity limits  Key Metrics to Monitor    Metric Free Tier Limit What to Watch     Read Capacity 25 RCU Keep usage under 20 RCU   Write Capacity 25 WCU Keep usage under 20 WCU   Storage 25 GB Monitor data growth    Free Tier Monitoring Stay within limits with these simple checks:\nâœ… Dashboard shows green metrics\râœ… Storage under 20 GB\râœ… RCU/WCU usage under 80%\râœ… Billing dashboard shows $0.00\rModule Contents  CloudWatch Dashboards - Create monitoring views Cost Analysis - Track Free Tier usage  Focus: This module covers only essential monitoring needed for the workshop. Advanced patterns are covered separately.\n\r\r5.1 CloudWatch Dashboards\r\r\r5.2 Cost Analysis \u0026amp; Optimization\r\r\rLet\u0026rsquo;s set up basic monitoring to keep your workshop running smoothly and cost-free.\n"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/6-advanced-patterns/",
	"title": "6. Advanced Patterns",
	"tags": [],
	"description": "",
	"content": "Advanced DynamoDB Patterns ğŸš€ Learn key DynamoDB techniques for better performance\nModule Overview Master essential DynamoDB patterns that improve performance and prevent common issues in your applications.\nWhat You\u0026rsquo;ll Learn  Batch Operations: Process multiple items efficiently Conditional Updates: Prevent data conflicts and race conditions Query Optimization: Get better performance from your queries  Key Benefits  Better Performance: Reduce API calls and latency Data Safety: Prevent overselling and data corruption Cost Efficiency: Use fewer request units Free Tier Friendly: Maximize value within limits  Core Patterns 1. Batch Operations Process multiple items in single API calls:\nâŒ Single: 100 separate API calls âœ… Batch: 4 API calls (25 items each) 2. Conditional Updates Prevent race conditions:\nâŒ Without conditions: Two users buy last item âœ… With conditions: Only first user succeeds Module Contents  Batch Operations - Efficient multi-item processing Conditional Updates - Safe data modifications  Focus: These patterns are essential for any production DynamoDB application.\n\r\r6.1 Batch Operations\r\r\r6.2 Conditional Updates\r\r\rLet\u0026rsquo;s implement advanced patterns for better DynamoDB applications.\nLearning Path This module follows a progressive learning approach:\n 6.1 Batch Operations: Learn efficient multi-item processing 6.2 Conditional Updates: Implement data integrity patterns 6.3 Advanced Query Techniques: Optimize performance and costs 6.4 Production Patterns: Apply enterprise-grade best practices  Real-World Applications E-commerce Platform:\n Batch product imports and updates Inventory management with conditional updates Order processing with optimistic locking Efficient catalog queries with projections  Content Management:\n Bulk content operations Version control for collaborative editing User permission checks with conditions Optimized content delivery queries  Analytics Dashboard:\n Batch data ingestion Concurrent metric updates Efficient data aggregation queries Performance-optimized reporting  Prerequisites Before starting this module, ensure you have:\n âœ… Completed Modules 1-5 (setup through monitoring) âœ… Understanding of single-table design principles âœ… Familiarity with GSI patterns âœ… Basic query and scan operations knowledge  Expected Outcomes By the end of this module, you will:\n âœ… Master batch operations for efficient data processing âœ… Implement conditional updates to ensure data integrity âœ… Use optimistic locking for concurrent access control âœ… Optimize queries for performance and cost efficiency âœ… Apply production patterns used by enterprise applications   Let\u0026rsquo;s dive into advanced DynamoDB patterns that will transform you from a beginner into an expert!\n Advanced query patterns and optimizations Security configurations and access controls Backup and disaster recovery strategies Production-ready deployment patterns  Let\u0026rsquo;s explore advanced DynamoDB patterns for enterprise applications.\n"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/7-cleanup-resources/",
	"title": "7. Cleanup Resources",
	"tags": [],
	"description": "",
	"content": "Resource Cleanup In this section, you will learn how to clean up resources on AWS Cloud to prevent incurring unnecessary costs.\nWhy Cleanup is Important  Cost Control: Prevent unexpected AWS charges Resource Management: Remove unused infrastructure Best Practices: Learn proper resource lifecycle management  1. Delete the DynamoDB Table Created in the Lab  Access the DynamoDB Management Console On the left navigation bar, select Tables Select the DynamoDB table demo-ecommerce-freetier related to the lab Click on Actions Choose Delete table Type the table name to confirm Confirm by clicking Delete  2. Delete CloudWatch Resources Created in the Lab  Access the CloudWatch Management Console On the left navigation bar, go to Dashboards Select all dashboards related to the lab Click on Actions Choose Delete dashboards Confirm deletion by clicking Delete On the left navigation bar, go to Alarms Select all alarms related to the lab Click on Actions Choose Delete Confirm deletion by clicking Delete  3. Delete Lambda Functions (if created)  Access the Lambda Management Console On the left navigation bar, navigate to Functions Select the Lambda functions related to the lab Click on Actions Choose Delete function Confirm by clicking Delete  4. Delete SNS Topics (if created)  Access the SNS Management Console On the left navigation bar, select Topics Select all SNS topics related to the lab Click on Actions Choose Delete Type \u0026ldquo;delete me\u0026rdquo; to confirm Confirm deletion by clicking Delete  5. Verify Billing and Costs  Access the Billing and Cost Management Console Check the current month charges Verify that all charges show $0.00 Confirm no unexpected services are running  Final Verification âœ… Cleanup Checklist:\n DynamoDB table deleted CloudWatch dashboards and alarms removed Lambda functions deleted (if any) SNS topics deleted (if any) Billing shows $0.00 charges   Congratulations! You have successfully completed the DynamoDB Advanced Patterns workshop and cleaned up all resources.\n"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/1-setup-infrastructure/1.1-aws-freetier-overview/",
	"title": "1.1 AWS Free Tier Overview",
	"tags": [],
	"description": "",
	"content": "AWS Free Tier Overview ğŸ†“ Understanding AWS Free Tier limits and ensuring zero-cost workshop experience\nWhat is AWS Free Tier? AWS Free Tier provides new customers access to AWS services at no charge for a limited time. It includes three types of offerings:\n1. 12 Months Free Available for 12 months following your AWS sign-up date:\n Amazon DynamoDB: 25 GB storage, 25 WCU, 25 RCU AWS Lambda: 1 million requests per month Amazon CloudWatch: 10 metrics, 10 alarms, 1 million API requests  2. Always Free Available to all AWS customers indefinitely:\n DynamoDB: 25 GB storage (always free) Lambda: 1 million requests, 400,000 GB-seconds compute time CloudWatch: 10 custom metrics and 10 alarms  3. Trials Short-term free access to certain services\nFree Tier Limits for This Workshop Service Usage Breakdown: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Service â”‚ Free Tier â”‚ Our Usage â”‚ Safety % â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ DynamoDB RCU â”‚ 25 units â”‚ 15 units â”‚ 60% â”‚ â”‚ DynamoDB WCU â”‚ 25 units â”‚ 15 units â”‚ 60% â”‚ â”‚ DynamoDB Storageâ”‚ 25 GB â”‚ \u0026lt;1 GB â”‚ 4% â”‚ â”‚ Lambda Requests â”‚ 1M/month â”‚ ~100/day â”‚ 0.3% â”‚ â”‚ Lambda Duration â”‚ 400K GB-sec â”‚ \u0026lt;1K GB-sec â”‚ 0.25% â”‚ â”‚ CloudWatch â”‚ 10 metrics â”‚ 6 metrics â”‚ 60% â”‚ â”‚ Data Transfer â”‚ 1 GB â”‚ \u0026lt;100 MB â”‚ 10% â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Overall Free Tier Usage: ~50% = 100% SAFE! ğŸ›¡ï¸ \rCost Protection: We\u0026rsquo;re using only 50% of available Free Tier limits, ensuring zero charges throughout the workshop.\n\rRegional Considerations Primary Region: US East (N. Virginia)  Why chosen: Highest Free Tier allowances DynamoDB: Full 25 RCU/WCU available Lambda: Full 1M requests available Best for: Primary workloads and testing  Secondary Region: EU West (Ireland)  Purpose: Global Tables replication Free Tier: Same limits as primary region Usage: Minimal (replica table only) Cost impact: Near zero  Monitoring Free Tier Usage AWS Billing Dashboard  Navigate to: AWS Console â†’ Billing \u0026amp; Cost Management Free Tier usage: Track current consumption Alerts: Set up when approaching 80% of limits Forecasting: Predict monthly usage  CloudWatch Billing Alarms Automatic alerts when:\n DynamoDB consumed units \u0026gt; 20 (80% of limit) Lambda invocations \u0026gt; 800K/month (80% of limit) Overall estimated charges \u0026gt; $1.00  Free Tier Best Practices 1. Monitor Regularly  Check Free Tier dashboard daily during workshop Set up billing alerts before deployment Monitor resource utilization in CloudWatch  2. Use Appropriate Capacity DynamoDB Provisioned Capacity: - Read Capacity Units (RCU): 5 (well under 25 limit) - Write Capacity Units (WCU): 5 (well under 25 limit) - On-Demand: NOT recommended (can exceed Free Tier) 3. Clean Up Resources  Delete test data regularly Remove unused Lambda functions Clean up CloudWatch logs older than 7 days  Common Free Tier Pitfalls to Avoid Avoid These Mistakes: 1. On-Demand DynamoDB: Can quickly exceed Free Tier 2. Multiple Regions: Deploying same resources in \u0026gt;2 regions 3. Large Data Sets: Uploading \u0026gt;25GB to DynamoDB 4. Forgot Cleanup: Leaving resources running beyond workshop\n\rMistake #1: Provisioned vs On-Demand âŒ WRONG: DynamoDB On-Demand - Pay per request (can be expensive) - Unpredictable costs - No Free Tier protection âœ… CORRECT: DynamoDB Provisioned - Fixed capacity units - Predictable costs - Protected by Free Tier limits Mistake #2: Resource Multiplication âŒ WRONG: Deploy to 5 regions - 5x resource consumption - 5x Free Tier usage - Likely to exceed limits âœ… CORRECT: Deploy to 2 regions max - Minimal resource usage - Well within Free Tier - Global availability maintained Emergency Procedures If You See Charges Appearing  STOP IMMEDIATELY: Pause all workshop activities Check Billing Dashboard: Identify source of charges Review Resources: List all active resources Contact Support: Use AWS Free Tier support if needed Delete Resources: Remove anything outside workshop scope  Quick Resource Audit Commands # List all DynamoDB tables aws dynamodb list-tables --region us-east-1 # List all Lambda functions  aws lambda list-functions --region us-east-1 # Check CloudFormation stacks aws cloudformation list-stacks --region us-east-1 Pre-Workshop Checklist Before starting the infrastructure deployment:\n AWS account created and verified Free Tier eligibility confirmed (account \u0026lt;12 months old) Billing alerts configured Free Tier dashboard bookmarked Emergency contact information ready Understanding of resource limits confirmed  Ready to Continue: Once you\u0026rsquo;ve reviewed the Free Tier limits and understand the safety measures, proceed to the Architecture Overview to understand what we\u0026rsquo;ll be building.\n\r"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/1-setup-infrastructure/1.2-architecture-overview/",
	"title": "1.2 Architecture Overview",
	"tags": [],
	"description": "",
	"content": "Architecture Overview ğŸ—ï¸ Understanding the complete infrastructure we\u0026rsquo;ll deploy for the DynamoDB workshop\nHigh-Level Architecture Our workshop infrastructure spans two AWS regions to demonstrate Global Tables functionality while maintaining Free Tier compliance.\nCore Components 1. DynamoDB Global Table Primary Table (US-East-1):\n Table Name: demo-ecommerce-freetier Partition Key: PK (String) Sort Key: SK (String) Capacity: 5 RCU / 5 WCU (Provisioned) Streams: Enabled (NEW_AND_OLD_IMAGES) Point-in-time Recovery: Enabled  Replica Table (EU-West-1):\n Synchronized: Automatic replication Read Capacity: 5 RCU Eventual Consistency: Cross-region Local Reads: Low latency for EU users  2. Lambda Stream Processor Function Configuration:\nRuntime: Python 3.9 Memory: 128 MB (Free Tier optimized) Timeout: 30 seconds Environment: Demo Trigger: DynamoDB Streams Processing Logic:\n Stream Records: Process INSERT, MODIFY, REMOVE events Data Validation: Ensure data integrity Audit Logging: Track all changes Error Handling: Dead letter queue for failed records  3. CloudWatch Monitoring Dashboard Components:\n DynamoDB Metrics: Read/Write capacity utilization Lambda Metrics: Invocation count, duration, errors Cost Tracking: Real-time Free Tier usage Performance: Latency and throughput metrics  Billing Alarms:\n Alert at $1.00: Early warning system Alert at 80% Free Tier: Usage monitoring Email Notifications: Immediate awareness  Data Model Architecture Single Table Design Pattern Our e-commerce platform uses a single DynamoDB table with multiple entity types:\nEntity Types and Access Patterns: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Entity Type â”‚ Partition Key â”‚ Sort Key â”‚ Purpose â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ User Profile â”‚ USER#id â”‚ PROFILE â”‚ User metadata â”‚ â”‚ User Addresses â”‚ USER#id â”‚ ADDRESS#id â”‚ Shipping info â”‚ â”‚ Product â”‚ PRODUCT#id â”‚ DETAILS â”‚ Product catalog â”‚ â”‚ Product Reviews â”‚ PRODUCT#id â”‚ REVIEW#user_id â”‚ Customer reviewsâ”‚ â”‚ Order Header â”‚ ORDER#id â”‚ DETAILS â”‚ Order metadata â”‚ â”‚ Order Items â”‚ ORDER#id â”‚ ITEM#product_id â”‚ Order contents â”‚ â”‚ Category â”‚ CATEGORY#id â”‚ DETAILS â”‚ Product groups â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Access Patterns  Get User Profile: PK=USER#123, SK=PROFILE Get User\u0026rsquo;s Orders: PK=USER#123, SK begins_with ORDER# Get Product Details: PK=PRODUCT#456, SK=DETAILS Get Order with Items: PK=ORDER#789, SK begins_with ITEM# Get Product Reviews: PK=PRODUCT#456, SK begins_with REVIEW#  Security Architecture IAM Roles and Policies Lambda Execution Role:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:DescribeStream\u0026#34;, \u0026#34;dynamodb:GetRecords\u0026#34;, \u0026#34;dynamodb:GetShardIterator\u0026#34;, \u0026#34;dynamodb:ListStreams\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:*:*:table/demo-ecommerce-freetier/stream/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; } ] } Principle of Least Privilege:\n Lambda: Only stream read permissions CloudWatch: Only metrics and logs write DynamoDB: Table-specific access only  Network Architecture Regional Deployment Strategy Primary Region (US-East-1):\n Availability: 99.99% SLA Latency: \u0026lt;10ms for US users Capacity: Full read/write operations Backup: Point-in-time recovery  Secondary Region (EU-West-1):\n Availability: 99.99% SLA (independent) Latency: \u0026lt;10ms for EU users Capacity: Read optimized Sync: Eventually consistent replication  Data Flow  Write Operations: Always to primary region Read Operations: Can be from either region Replication: Automatic cross-region sync Conflict Resolution: Last-writer-wins Failover: Manual promotion if needed  Cost Architecture Free Tier Optimization DynamoDB Costs:\nPrimary Table (US-East-1): - Provisioned RCU: 5 units (Free: 25) = $0.00 - Provisioned WCU: 5 units (Free: 25) = $0.00 - Storage: \u0026lt;1 GB (Free: 25 GB) = $0.00 Replica Table (EU-West-1): - Provisioned RCU: 5 units (Free: 25) = $0.00 - Cross-region replication: \u0026lt;1 GB/month = $0.00 Lambda Costs:\nStream Processor: - Invocations: ~100/day (Free: 1M/month) = $0.00 - Duration: 128MB Ã— 1s Ã— 100 = \u0026lt;1 GB-second = $0.00 Total Workshop Cost: $0.00 âœ…\nScalability Considerations Horizontal Scaling DynamoDB:\n Auto Scaling: Can enable if needed On-Demand: Switch from provisioned Global Secondary Indexes: Add for new access patterns  Lambda:\n Concurrent Executions: Up to 1000 default Dead Letter Queue: Handle failures Reserved Concurrency: Control scaling  Performance Optimization Read Performance:\n Consistent Reads: When data consistency required Eventually Consistent: For better performance DAX: DynamoDB Accelerator for caching  Write Performance:\n Batch Operations: Reduce API calls Parallel Writes: Multiple partition keys Write Sharding: Distribute hot partitions  Monitoring and Observability Key Metrics to Monitor DynamoDB:\n Consumed Read Capacity: Target \u0026lt;80% of provisioned Consumed Write Capacity: Target \u0026lt;80% of provisioned Throttled Requests: Should be 0 Error Rate: Target \u0026lt;1%  Lambda:\n Invocation Count: Track processing volume Duration: Monitor performance trends Error Rate: Target \u0026lt;1% Dead Letter Queue: Monitor failed records  Alerting Strategy Critical Alerts:\n DynamoDB throttling events Lambda function errors Billing threshold exceeded Free Tier limit approaching  Warning Alerts:\n High capacity utilization (\u0026gt;70%) Increased error rates Performance degradation  Architecture Ready: This architecture provides a production-ready foundation for learning DynamoDB advanced patterns while staying within AWS Free Tier limits.\n\rNext Steps With the architecture understanding complete, we\u0026rsquo;ll now proceed to deploy this infrastructure using CloudFormation templates in the next section.\n"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/1-setup-infrastructure/1.3-cloudformation-deployment/",
	"title": "1.3 CloudFormation Deployment",
	"tags": [],
	"description": "",
	"content": "CloudFormation Deployment ğŸš€ Step-by-step guide to deploy AWS infrastructure using Infrastructure as Code\nOverview In this section, you\u0026rsquo;ll deploy the complete DynamoDB workshop infrastructure using AWS CloudFormation. This approach ensures consistent, reproducible deployments and follows AWS best practices.\nPrerequisites Before starting deployment, ensure:\n AWS Console access with administrative permissions Verified region: US East (N. Virginia) us-east-1 Current billing status: $0.00 CloudFormation template downloaded  CloudFormation Template Overview Our template creates the following resources:Download CloudFormation Template\nCore Infrastructure  DynamoDB Table: Single table with Global Tables enabled Lambda Function: Stream processor for real-time events IAM Roles: Least privilege access policies DynamoDB Streams: Change data capture configuration  Monitoring \u0026amp; Alerting  CloudWatch Dashboard: Real-time metrics visualization CloudWatch Alarms: Threshold-based alerting Billing Alerts: Cost protection mechanisms  Security Configuration  IAM Policies: Fine-grained permissions Resource Encryption: Data protection at rest VPC Integration: Network isolation (optional)  Step-by-Step Deployment Step 1: Access CloudFormation Service   Navigate to CloudFormation\n Open AWS Management Console Search for \u0026ldquo;CloudFormation\u0026rdquo; or find it in services menu Ensure you\u0026rsquo;re in US East (N. Virginia) region    Create New Stack\n Click \u0026ldquo;Create stack\u0026rdquo; button Select \u0026ldquo;With new resources (standard)\u0026rdquo;    Step 2: Upload Template   Choose Template Source\n Select \u0026ldquo;Upload a template file\u0026rdquo; Click \u0026ldquo;Choose file\u0026rdquo; button Select the infrastructure.yaml file    Validate Template\n CloudFormation will automatically validate syntax Review template details if needed Click \u0026ldquo;Next\u0026rdquo; to proceed    Step 3: Configure Stack Parameters Stack Details:\nStack name: demo-dynamodb-freetier\rDescription: DynamoDB Advanced Patterns Workshop Infrastructure\rParameters:\nResource Configuration:\nReadCapacityUnits: 5\rWriteCapacityUnits: 5\rStreamViewType: NEW_AND_OLD_IMAGES\rStep 4: Configure Stack Options Tags (Optional):\nWorkshop: DynamoDB-Advanced-Patterns\rEnvironment: Demo\rCostCenter: Learning\rPermissions:\n Use existing service role (if available) Or allow CloudFormation to create new role  Advanced Options:\n Keep all defaults Rollback on failure: Enabled Stack creation timeout: 10 minutes  Step 5: Review and Deploy   Review Configuration\n Verify all parameters are correct Check resource list matches expectations Confirm estimated costs ($0.00 for Free Tier)    Acknowledge Capabilities\n âœ… Check: \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources\u0026rdquo; âœ… Check: \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources with custom names\u0026rdquo;    Create Stack\n Click \u0026ldquo;Create stack\u0026rdquo; button Deployment begins immediately    Monitoring Deployment Stack Events Tab Monitor real-time deployment progress:\nExpected Timeline  Total Duration: 5-7 minutes IAM Resources: 1-2 minutes DynamoDB Table: 2-3 minutes Lambda Function: 1-2 minutes CloudWatch Components: 1-2 minutes  Troubleshooting Common Issues Issue: Insufficient Permissions Symptoms: CREATE_FAILED for IAM resources Solution:\n Verify account has administrator access Check IAM permissions for CloudFormation Use root account if necessary (for workshop only)  Issue: Resource Limits Exceeded Symptoms: CREATE_FAILED for DynamoDB or Lambda Solution:\n Check Free Tier usage in billing console Verify no existing resources consuming limits Contact AWS support if needed  Issue: Region Mismatch Symptoms: Template validation errors Solution:\n Verify region is us-east-1 Check all parameters are region-appropriate Restart deployment in correct region  Verification Steps After successful deployment:\n Stack Status: CREATE_COMPLETE âœ… Navigate to Outputs tab Record important values:  Table Name Lambda Function ARN Dashboard URL Stream ARN    Deployment Complete! Your DynamoDB Advanced Patterns infrastructure is now running. In the next section, we\u0026rsquo;ll verify all components are working correctly.\n\rNext Steps  Verify DynamoDB table is active Test Lambda function deployment Check CloudWatch dashboard Confirm zero billing charges Begin data modeling exercises  "
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/1-setup-infrastructure/1.4-infrastructure-verification/",
	"title": "1.4 Infrastructure Verification",
	"tags": [],
	"description": "",
	"content": "Infrastructure Verification âœ… Comprehensive testing to ensure all AWS resources are properly deployed and functioning\nOverview After CloudFormation deployment, it\u0026rsquo;s critical to verify that all resources are working correctly. This section provides step-by-step verification procedures to ensure your infrastructure is ready for the workshop.\nVerification Checklist Use this checklist to systematically verify each component:\n CloudFormation stack status: CREATE_COMPLETE DynamoDB table: Active and accessible DynamoDB Global Tables: Replication configured Lambda function: Deployed and connected to stream CloudWatch dashboard: Metrics visible IAM roles: Properly configured permissions Billing status: $0.00 charges Free Tier usage: Within limits  Step 1: Verify CloudFormation Stack 1.1 Check Stack Status Navigate to CloudFormation:\n AWS Console â†’ CloudFormation â†’ Stacks Find stack: demo-dynamodb-freetier Status should be: CREATE_COMPLETE âœ…  If status shows anything else:\n CREATE_IN_PROGRESS: Wait for completion CREATE_FAILED: Check Events tab for errors ROLLBACK_COMPLETE: Delete and redeploy  1.2 Review Stack Outputs Click on your stack â†’ Outputs tab:\nRecord these values - you\u0026rsquo;ll use them for further verification.\nStep 2: Verify DynamoDB Table 2.1 Access DynamoDB Console Navigate to DynamoDB:\n AWS Console â†’ Services â†’ DynamoDB Click Tables in left sidebar Find table: demo-ecommerce-freetier  2.2 Check Table Status Table Overview:\nTable Details: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Property â”‚ Expected Value â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ Table Status â”‚ Active âœ… â”‚ â”‚ Partition Key â”‚ PK (String) â”‚ â”‚ Sort Key â”‚ SK (String) â”‚ â”‚ Read Capacity â”‚ 5 (Provisioned) â”‚ â”‚ Write Capacity â”‚ 5 (Provisioned) â”‚ â”‚ Point-in-time Rec. â”‚ Enabled â”‚ â”‚ Streams â”‚ Enabled (NEW_AND_OLD_IMAGES) â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 2.3 Verify Table Configuration Click on table name to view details:\nGeneral Tab:\n Table name: demo-ecommerce-freetier Primary key: PK (String), SK (String) Table status: Active Creation date: Today\u0026rsquo;s date  Capacity Tab:\n Read capacity: 5 units (Provisioned) Write capacity: 5 units (Provisioned) Auto scaling: Disabled (for Free Tier safety)  2.4 Check DynamoDB Streams Exports and streams Tab:\n DynamoDB stream: Enabled âœ… Stream view type: New and old images Stream ARN: Should match CloudFormation output  Step 3: Verify Global Tables Setup 3.1 Check Global Tables Configuration Global Tables Tab:\n Primary region: us-east-1 (US East N. Virginia) Replica regions: eu-west-1 (Europe Ireland) Replication status: Active  3.2 Verify Secondary Region Switch to EU-West-1:\n Change region in AWS Console to \u0026ldquo;Europe (Ireland)\u0026rdquo; Navigate to DynamoDB â†’ Tables Find replica table: demo-ecommerce-freetier Status should be: Active  Replica Table Properties:\n Table status: Active Read capacity: 5 units Global table: Yes (replica) Primary region: us-east-1  Step 4: Verify Lambda Function 4.1 Access Lambda Console Navigate to Lambda:\n Switch back to US-East-1 region AWS Console â†’ Services â†’ Lambda Click Functions in left sidebar Find function: demo-dynamodb-stream-processor  4.2 Check Function Configuration Function Overview:\nLambda Function Details: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Property â”‚ Expected Value â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ Function Name â”‚ demo-dynamodb-stream-processor â”‚ â”‚ Runtime â”‚ Python 3.9 â”‚ â”‚ Memory â”‚ 128 MB â”‚ â”‚ Timeout â”‚ 30 seconds â”‚ â”‚ Handler â”‚ lambda_function.lambda_handler â”‚ â”‚ Last Modified â”‚ Today\u0026#39;s date â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 4.3 Verify Stream Trigger Configuration Tab:\n Triggers: DynamoDB stream should be listed Source: demo-ecommerce-freetier table State: Enabled Batch size: 100 (default)  4.4 Test Function Permissions Permissions Tab:\n Execution role: Should have DynamoDB stream read permissions Resource-based policy: Should be configured automatically  Step 5: Verify CloudWatch Dashboard 5.1 Access CloudWatch Console Navigate to CloudWatch:\n AWS Console â†’ Services â†’ CloudWatch Click Dashboards in left sidebar Find dashboard: demo-dynamodb-freetier-monitoring  5.2 Check Dashboard Widgets Expected Widgets:\nDashboard Layout: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Widget â”‚ Description â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ DynamoDB RCU â”‚ Read Capacity Utilization â”‚ â”‚ DynamoDB WCU â”‚ Write Capacity Utilization â”‚ â”‚ DynamoDB Throttles â”‚ Throttled Read/Write Requests â”‚ â”‚ Lambda Invocations â”‚ Function invocation count â”‚ â”‚ Lambda Errors â”‚ Function error rate â”‚ â”‚ Lambda Duration â”‚ Function execution duration â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 5.3 Verify Metrics Data Initial State (no activity yet):\n DynamoDB metrics: Should show 0 consumed capacity Lambda metrics: Should show 0 invocations All metrics: Should be visible but with no data points yet  Step 6: Verify IAM Roles and Policies 6.1 Check Lambda Execution Role Navigate to IAM:\n AWS Console â†’ Services â†’ IAM Click Roles in left sidebar Find role with name containing: demo-dynamodb-freetier  6.2 Verify Role Permissions Attached Policies:\n AWS managed policy: AWSLambdaBasicExecutionRole Inline policy: Custom DynamoDB stream permissions  Custom Policy Permissions:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:DescribeStream\u0026#34;, \u0026#34;dynamodb:GetRecords\u0026#34;, \u0026#34;dynamodb:GetShardIterator\u0026#34;, \u0026#34;dynamodb:ListStreams\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:us-east-1:*:table/demo-ecommerce-freetier/stream/*\u0026#34; } ] } Step 7: Test Data Operations 7.1 Create Test Item Add sample data to verify table functionality:\n Go to DynamoDB Console â†’ Tables â†’ demo-ecommerce-freetier Click \u0026ldquo;Explore table items\u0026rdquo; Click \u0026ldquo;Create item\u0026rdquo;  Test Item:\n{ \u0026#34;PK\u0026#34;: \u0026#34;USER#test123\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;PROFILE\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Test User\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;test@example.com\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-08-11T10:00:00Z\u0026#34; } 7.2 Verify Item Creation Confirm item appears:\n Item should be visible in table Item count should increase to 1 No errors should appear  7.3 Check Lambda Trigger Verify stream processing:\n Go to Lambda Console Click on stream processor function Check \u0026ldquo;Monitor\u0026rdquo; tab Should see 1 invocation (from the item creation)  Step 8: Cost and Free Tier Verification 8.1 Check Current Billing Navigate to Billing:\n AWS Console â†’ Services â†’ Billing \u0026amp; Cost Management Current charges: Should show $0.00 âœ… Month-to-date: Should show $0.00 âœ…  8.2 Verify Free Tier Usage Free Tier Dashboard:\n DynamoDB: Should show minimal usage Lambda: Should show \u0026lt;10 invocations CloudWatch: Should show active metrics  Usage Breakdown:\nCurrent Free Tier Usage: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Service â”‚ Used â”‚ Available â”‚ % Used â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ DynamoDB RCU â”‚ \u0026lt;1 unit â”‚ 25 units â”‚ \u0026lt;4% â”‚ â”‚ DynamoDB WCU â”‚ \u0026lt;1 unit â”‚ 25 units â”‚ \u0026lt;4% â”‚ â”‚ DynamoDB Storageâ”‚ \u0026lt;0.01 GB â”‚ 25 GB â”‚ \u0026lt;0.1% â”‚ â”‚ Lambda Requests â”‚ 1 request â”‚ 1M requests â”‚ \u0026lt;0.001% â”‚ â”‚ Lambda Duration â”‚ \u0026lt;1 GB-sec â”‚ 400K GB-sec â”‚ \u0026lt;0.001% â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Total Usage: \u0026lt;1% of Free Tier limits âœ… Step 9: Troubleshooting Common Issues 9.1 DynamoDB Table Issues Table not found:\n Check region (must be us-east-1) Verify CloudFormation stack completed Check table name spelling  Table not Active:\n Wait 2-3 minutes for creation Check CloudFormation Events for errors Verify account limits not exceeded  9.2 Lambda Function Issues Function not found:\n Check region Verify CloudFormation outputs Check IAM permissions  No invocations after adding data:\n Verify stream is enabled on table Check trigger configuration Review function logs for errors  9.3 Permission Issues Access denied errors:\n Verify IAM role attached to Lambda Check execution role permissions Ensure resource ARNs match  9.4 Billing Issues Unexpected charges appearing:\n STOP all activities immediately Check Billing dashboard for charge sources Review resource configuration Contact AWS support if needed Consider deleting and redeploying resources  Step 10: Final Verification Summary 10.1 Complete Verification Checklist Infrastructure Status:\n CloudFormation: CREATE_COMPLETE DynamoDB: Active table with streams Global Tables: Replication working Lambda: Function deployed and triggered CloudWatch: Dashboard accessible IAM: Proper permissions configured Billing: $0.00 charges Test data: Successfully created and processed  10.2 Ready for Next Module Infrastructure Health Check:\nğŸŸ¢ All Systems Operational â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Component â”‚ Status â”‚ Health â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ DynamoDB Table â”‚ Active â”‚ ğŸŸ¢ Healthy â”‚ â”‚ Global Tables â”‚ Replicating â”‚ ğŸŸ¢ Healthy â”‚ â”‚ Lambda Function â”‚ Active â”‚ ğŸŸ¢ Healthy â”‚ â”‚ CloudWatch â”‚ Monitoring â”‚ ğŸŸ¢ Healthy â”‚ â”‚ Cost Management â”‚ $0.00 â”‚ ğŸŸ¢ On Track â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ \rInfrastructure Verification Complete: All resources are properly deployed and functioning. You\u0026rsquo;re ready to proceed to Module 2: Single Table Design.\n\rNext Steps With infrastructure successfully verified, you now have:\n Production-ready DynamoDB table with Global Tables Fully functional Lambda stream processor Complete monitoring and alerting setup Zero-cost Free Tier deployment  Ready for: Module 2: Single Table Design\nKeep This Environment: Don\u0026rsquo;t delete these resources yet - we\u0026rsquo;ll use them throughout the workshop for hands-on exercises.\n\r"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/2-single-table-design/2.1-design-principles/",
	"title": "2.1 Design Principles",
	"tags": [],
	"description": "",
	"content": "Single Table Design Principles ğŸ¯ Understanding the core concepts that make Single Table Design powerful\nThe Paradigm Shift From Relational to NoSQL Thinking Traditional relational databases organize data by entities (separate tables for Users, Products, Orders). DynamoDB organizes data by access patterns (how you\u0026rsquo;ll query the data).\nRelational Approach:\n   Table Fields     Users user_id, name, email   Products product_id, name, category   Orders order_id, user_id, status    Single Table Approach:\n   PK SK Entity Additional Data     USER#user1 PROFILE User name, email, phone   USER#user1 ORDER#ord1 Order status, total, date   PRODUCT#p1 DETAILS Product name, price, stock   ORDER#ord1 ITEM#p1 OrderItem quantity, price    Core Principles 1. Composite Primary Key Strategy Partition Key (PK) + Sort Key (SK) creates unique item identification and enables relationships:\n PK: Groups related items together (like a namespace) SK: Sorts items within a partition and creates hierarchies Together: Enable 1-to-1, 1-to-many, and many-to-many relationships  2. Entity Namespacing Use prefixes to create logical separation:\n   Entity Type PK Pattern SK Pattern Purpose     User Profile USER#user123 PROFILE Store user information   User Orders USER#user123 ORDER#order456 Link orders to users   Product Details PRODUCT#prod789 DETAILS Store product information   Order Items ORDER#order456 ITEM#prod789 Link products to orders    3. Access Pattern First Design Start with questions, then design the key structure:\n \u0026ldquo;How will I get user profile?\u0026rdquo; â†’ PK=USER#id, SK=PROFILE \u0026ldquo;How will I get user\u0026rsquo;s orders?\u0026rdquo; â†’ PK=USER#id, SK begins_with ORDER# \u0026ldquo;How will I get order details?\u0026rdquo; â†’ PK=ORDER#id, SK begins_with ITEM# \u0026ldquo;How will I get products by category?\u0026rdquo; â†’ Use GSI with CATEGORY# keys  Global Secondary Index (GSI) Strategy When to Use GSIs Use GSIs when you need to query data by attributes other than the primary key:\n Different grouping: Products by category instead of by product ID Different sorting: Orders by status instead of by user Cross-entity queries: All pending orders across all users  GSI Key Design GSI1 - Category-based queries:\n   GSI1PK GSI1SK     CATEGORY#electronics PRODUCT#prod1   CATEGORY#electronics PRODUCT#prod2   CATEGORY#books PRODUCT#prod3    GSI2 - Status/Price-based queries:\n   GSI2PK GSI2SK     STATUS#pending ORDER#order1   STATUS#shipped ORDER#order2   PRICE#100-500 PRODUCT#prod1    Benefits in Practice Performance Benefits  Single Query: Get user profile + all orders in one query Predictable Latency: Single-digit millisecond response times No JOINs: All related data retrieved together Efficient Scaling: Consistent performance at any scale  Cost Benefits  Fewer Tables: Lower DynamoDB table costs Fewer Operations: Batch queries instead of multiple calls Optimized Capacity: Better utilization of provisioned capacity Reduced Data Transfer: Less network overhead  Operational Benefits  Atomic Transactions: Update related items together Simplified Backup: One table to backup/restore Easier Monitoring: Single table metrics to track Consistent Security: One set of IAM policies  Key Design Patterns 1. Adjacency List Pattern Store related items next to each other:\nPK=USER#user1, SK=PROFILE (User details) PK=USER#user1, SK=ORDER#order1 (Order 1) PK=USER#user1, SK=ORDER#order2 (Order 2) 2. Hierarchical Data Pattern Use sort key to represent hierarchy:\nPK=ORDER#order1, SK=DETAILS (Order header) PK=ORDER#order1, SK=ITEM#prod1 (Order item 1) PK=ORDER#order1, SK=ITEM#prod2 (Order item 2) 3. GSI Overloading Pattern Use same GSI for multiple query patterns:\nGSI1PK=CATEGORY#electronics, GSI1SK=PRODUCT#prod1 GSI1PK=USER#user1@email.com, GSI1SK=PROFILE \rDesign Rule: Always start with your access patterns, then design your key structure. Don\u0026rsquo;t start with entities!\n\rCommon Anti-Patterns to Avoid âŒ Don\u0026rsquo;t Use Scan Operations  Wrong: Scan entire table to find items Right: Use Query with proper key structure  âŒ Don\u0026rsquo;t Create Too Many GSIs  Wrong: One GSI per query pattern Right: Overload GSIs for multiple patterns  âŒ Don\u0026rsquo;t Ignore Hot Partitions  Wrong: All items have same partition key Right: Distribute items across multiple partitions  âŒ Don\u0026rsquo;t Use Relational Patterns  Wrong: Normalize data across multiple items Right: Denormalize related data together  Remember: Single Table Design requires a mindset shift. Think in terms of access patterns, not entity relationships!\n\rReady for Implementation Now that you understand the principles, let\u0026rsquo;s move to the AWS Console to see these concepts in action. In the next section, we\u0026rsquo;ll navigate the DynamoDB Console and start creating our e-commerce data model.\n"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/2-single-table-design/2.2-console-navigation/",
	"title": "2.2 Console Navigation",
	"tags": [],
	"description": "",
	"content": "DynamoDB Console Navigation ğŸ–¥ï¸ Quick guide to navigate DynamoDB Console for Single Table Design\nAccessing Your Table Step 1: Navigate to DynamoDB Service  Open AWS Console: https://console.aws.amazon.com Verify Region: Ensure you\u0026rsquo;re in US East (N. Virginia) Access DynamoDB: Services â†’ Database â†’ DynamoDB  Step 2: Find Your Workshop Table  Click \u0026ldquo;Tables\u0026rdquo; in the left sidebar Find table: demo-ecommerce-freetier Verify Status: Should show \u0026ldquo;Active\u0026rdquo; Click table name to enter table details  Table Overview Dashboard Understanding the Table Layout When you click on your table, you\u0026rsquo;ll see several tabs:\n   Tab Purpose What You\u0026rsquo;ll Use It For     Overview Table configuration Check status, keys, capacity   Items Data management Create, view, edit items   Metrics Performance data Monitor usage and costs   Indexes GSI management View Global Secondary Indexes   Global tables Multi-region setup Check replication status    Key Information to Note Table Configuration:\n Table name: demo-ecommerce-freetier Partition key: PK (String) Sort key: SK (String) Table status: Active Item count: Currently 0 (empty table)  Items Tab - Your Data Workspace Accessing the Items View  Click \u0026ldquo;Items\u0026rdquo; tab View table structure: Currently empty Note the columns: PK, SK, and any additional attributes  This is where you\u0026rsquo;ll:\n âœ… Create new items (users, products, orders) âœ… View existing data âœ… Edit item attributes âœ… Delete items if needed  Creating Items Interface To create a new item:\n Click \u0026ldquo;Create item\u0026rdquo; button Choose input method:  Form view: Point-and-click interface JSON view: Direct JSON editing (recommended)   Switch to JSON view for easier data entry  JSON View for Data Entry Understanding JSON Format When creating items, you\u0026rsquo;ll use this JSON structure:\n{ \u0026#34;PK\u0026#34;: \u0026#34;USER#user123\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;PROFILE\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john@example.com\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-08-11T10:00:00Z\u0026#34; } Key points:\n PK and SK: Always required (your composite primary key) Additional attributes: Add as needed for each entity type Data types: Strings, numbers, booleans, lists, maps supported  Query Interface Accessing Query Functionality To query your table:\n Go to Items tab Click \u0026ldquo;Query\u0026rdquo; button (next to Create item) Choose query type:  Table query: Query main table Index query: Query GSI    Query Parameters For table queries, you\u0026rsquo;ll specify:\n Partition key (PK): Exact value (e.g., USER#user123) Sort key (SK): Optional conditions:  Exact match: PROFILE Begins with: ORDER# Between: Range queries    Global Secondary Index (GSI) Navigation Viewing GSI Configuration  Click \u0026ldquo;Indexes\u0026rdquo; tab View GSI1: Used for category-based queries View GSI2: Used for status/price-based queries  GSI Structure:\n GSI1: GSI1PK (Partition) + GSI1SK (Sort) GSI2: GSI2PK (Partition) + GSI2SK (Sort)  Metrics and Monitoring Checking Usage and Performance  Click \u0026ldquo;Metrics\u0026rdquo; tab Monitor key metrics:  Consumed read/write capacity Throttled requests (should be 0) Item count (increases as you add data)    Why this matters:\n âœ… Stay within Free Tier limits âœ… Monitor performance âœ… Detect any issues early  Quick Actions Reference Common Console Actions    Action Location Purpose     Create Item Items tab â†’ Create item Add new data   Query Table Items tab â†’ Query Search by PK/SK   View Metrics Metrics tab Monitor performance   Check Capacity Overview tab Verify provisioned capacity    Console Tips Efficiency Tips  Use JSON view for faster item creation Copy/paste item structures for consistency Use Query, not Scan for better performance Check metrics regularly to monitor usage  Navigation Shortcuts  Tables list: DynamoDB home â†’ Tables Quick table access: Bookmark your table URL Region switching: Use region selector in top-right Service search: Use Ctrl+K for quick service access  Pro Tip: Keep the DynamoDB console open in a separate browser tab during the workshop for quick access between exercises.\n\rReady for Data Creation Now that you\u0026rsquo;re familiar with the console interface, you\u0026rsquo;re ready to start creating your e-commerce data model. In the next section, we\u0026rsquo;ll create users, products, and orders using the patterns you\u0026rsquo;ve learned.\nBefore Starting: Make sure you\u0026rsquo;re in the correct table (demo-ecommerce-freetier) and understand the difference between Query and Scan operations.\n\r"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/2-single-table-design/2.3-create-data-items/",
	"title": "2.3 Create Data Items",
	"tags": [],
	"description": "",
	"content": "Create Data Items ğŸ“ Step-by-step guide to create e-commerce data using Single Table Design patterns\nOverview In this section, you\u0026rsquo;ll create the core entities for our e-commerce platform: Users, Products, Orders, and Order Items. Each entity type follows specific key patterns to enable efficient querying.\nEntity Types We\u0026rsquo;ll Create    Entity PK Pattern SK Pattern Purpose     User Profile USER#userID PROFILE Store customer information   Product PRODUCT#productID DETAILS Store product catalog   Order USER#userID ORDER#orderID Link orders to customers   Order Item ORDER#orderID ITEM#productID Link products to orders    Step 1: Create User Profile Access Item Creation  Navigate to: DynamoDB â†’ Tables â†’ demo-ecommerce-freetier Click: \u0026ldquo;Items\u0026rdquo; tab Click: \u0026ldquo;Create item\u0026rdquo; button Switch to: JSON view  User Profile JSON Template Copy and paste this template, then modify with your details:\n{ \u0026#34;PK\u0026#34;: \u0026#34;USER#user001\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;PROFILE\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;USER#john.doe@email.com\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;PROFILE\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;user001\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john.doe@email.com\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;+1-555-0123\u0026#34;, \u0026#34;address\u0026#34;: { \u0026#34;street\u0026#34;: \u0026#34;123 Main St\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Seattle\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;WA\u0026#34;, \u0026#34;zip\u0026#34;: \u0026#34;98101\u0026#34; }, \u0026#34;created_at\u0026#34;: \u0026#34;2025-08-11T10:00:00Z\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34; } Key Patterns Explained:\n PK: USER#user001 - Groups all user data together SK: PROFILE - Identifies this as the user\u0026rsquo;s profile record GSI1PK: USER#john.doe@email.com - Enables email-based lookups GSI1SK: PROFILE - Maintains consistency in GSI  Save Your User  Review the JSON for syntax errors Click \u0026ldquo;Create item\u0026rdquo; Verify creation: Item should appear in the table view  Step 2: Create Products Electronics Product Example Create your first product - customize the values:\n{ \u0026#34;PK\u0026#34;: \u0026#34;PRODUCT#laptop001\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;DETAILS\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;CATEGORY#electronics\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;PRODUCT#laptop001\u0026#34;, \u0026#34;GSI2PK\u0026#34;: \u0026#34;PRICE#500-1000\u0026#34;, \u0026#34;GSI2SK\u0026#34;: \u0026#34;PRODUCT#laptop001\u0026#34;, \u0026#34;product_id\u0026#34;: \u0026#34;laptop001\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Professional Laptop\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;High-performance laptop for professionals\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;electronics\u0026#34;, \u0026#34;price\u0026#34;: 799, \u0026#34;stock\u0026#34;: 25, \u0026#34;brand\u0026#34;: \u0026#34;TechCorp\u0026#34;, \u0026#34;specifications\u0026#34;: { \u0026#34;processor\u0026#34;: \u0026#34;Intel i7\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;16GB RAM\u0026#34;, \u0026#34;storage\u0026#34;: \u0026#34;512GB SSD\u0026#34; }, \u0026#34;created_at\u0026#34;: \u0026#34;2025-08-11T10:00:00Z\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34; } Key Patterns Explained:\n GSI1PK: CATEGORY#electronics - Enables category-based queries GSI2PK: PRICE#500-1000 - Enables price range queries Nested attributes: Store complex product details  Books Product Example Create a second product in a different category:\n{ \u0026#34;PK\u0026#34;: \u0026#34;PRODUCT#book001\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;DETAILS\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;CATEGORY#books\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;PRODUCT#book001\u0026#34;, \u0026#34;GSI2PK\u0026#34;: \u0026#34;PRICE#10-50\u0026#34;, \u0026#34;GSI2SK\u0026#34;: \u0026#34;PRODUCT#book001\u0026#34;, \u0026#34;product_id\u0026#34;: \u0026#34;book001\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;DynamoDB Patterns Guide\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Complete guide to DynamoDB design patterns\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;books\u0026#34;, \u0026#34;price\u0026#34;: 29, \u0026#34;stock\u0026#34;: 100, \u0026#34;author\u0026#34;: \u0026#34;Database Expert\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;978-1234567890\u0026#34;, \u0026#34;pages\u0026#34;: 350, \u0026#34;created_at\u0026#34;: \u0026#34;2025-08-11T10:00:00Z\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34; } Notice the differences:\n Different category: CATEGORY#books Different price range: PRICE#10-50 Book-specific attributes: author, isbn, pages  Step 3: Create Order Order Header Create an order linked to your user:\n{ \u0026#34;PK\u0026#34;: \u0026#34;USER#user001\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;ORDER#order001\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;ORDER#order001\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;DETAILS\u0026#34;, \u0026#34;GSI2PK\u0026#34;: \u0026#34;STATUS#pending\u0026#34;, \u0026#34;GSI2SK\u0026#34;: \u0026#34;ORDER#order001\u0026#34;, \u0026#34;order_id\u0026#34;: \u0026#34;order001\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;user001\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;total_amount\u0026#34;: 828, \u0026#34;tax_amount\u0026#34;: 66.24, \u0026#34;shipping_cost\u0026#34;: 0, \u0026#34;shipping_address\u0026#34;: { \u0026#34;street\u0026#34;: \u0026#34;123 Main St\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Seattle\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;WA\u0026#34;, \u0026#34;zip\u0026#34;: \u0026#34;98101\u0026#34; }, \u0026#34;payment_method\u0026#34;: \u0026#34;credit_card\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-08-11T11:00:00Z\u0026#34;, \u0026#34;updated_at\u0026#34;: \u0026#34;2025-08-11T11:00:00Z\u0026#34; } Key Patterns Explained:\n PK: USER#user001 - Links order to user (enables \u0026ldquo;get user\u0026rsquo;s orders\u0026rdquo;) SK: ORDER#order001 - Identifies this as an order record GSI1PK: ORDER#order001 - Enables direct order lookup GSI2PK: STATUS#pending - Enables status-based queries  Step 4: Create Order Items First Order Item Add the laptop to the order:\n{ \u0026#34;PK\u0026#34;: \u0026#34;ORDER#order001\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;ITEM#laptop001\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;PRODUCT#laptop001\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;ORDER#order001\u0026#34;, \u0026#34;order_id\u0026#34;: \u0026#34;order001\u0026#34;, \u0026#34;product_id\u0026#34;: \u0026#34;laptop001\u0026#34;, \u0026#34;product_name\u0026#34;: \u0026#34;Professional Laptop\u0026#34;, \u0026#34;quantity\u0026#34;: 1, \u0026#34;unit_price\u0026#34;: 799, \u0026#34;total_price\u0026#34;: 799, \u0026#34;added_at\u0026#34;: \u0026#34;2025-08-11T11:00:00Z\u0026#34; } Second Order Item Add the book to the same order:\n{ \u0026#34;PK\u0026#34;: \u0026#34;ORDER#order001\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;ITEM#book001\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;PRODUCT#book001\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;ORDER#order001\u0026#34;, \u0026#34;order_id\u0026#34;: \u0026#34;order001\u0026#34;, \u0026#34;product_id\u0026#34;: \u0026#34;book001\u0026#34;, \u0026#34;product_name\u0026#34;: \u0026#34;DynamoDB Patterns Guide\u0026#34;, \u0026#34;quantity\u0026#34;: 1, \u0026#34;unit_price\u0026#34;: 29, \u0026#34;total_price\u0026#34;: 29, \u0026#34;added_at\u0026#34;: \u0026#34;2025-08-11T11:00:00Z\u0026#34; } Key Patterns Explained:\n PK: ORDER#order001 - Groups items with their order SK: ITEM#productID - Identifies specific items GSI1: Creates product-to-order relationship  Verify Your Data Structure Check Table Contents After creating all items, your table should contain:\n   PK SK Entity Type     USER#user001 PROFILE User Profile   USER#user001 ORDER#order001 Order (linked to user)   PRODUCT#laptop001 DETAILS Product (Electronics)   PRODUCT#book001 DETAILS Product (Books)   ORDER#order001 ITEM#laptop001 Order Item (Laptop)   ORDER#order001 ITEM#book001 Order Item (Book)    Validate Relationships Verify these relationships work:\n User â†’ Orders: PK=USER#user001 returns profile + orders Order â†’ Items: PK=ORDER#order001 returns order details + items Category grouping: GSI1 with CATEGORY#electronics returns products Status grouping: GSI2 with STATUS#pending returns orders  Data Creation Tips JSON Best Practices  Use consistent naming: Follow the established patterns Validate JSON syntax: Check for missing commas, brackets Include required attributes: PK, SK, and GSI keys Use meaningful IDs: Make them readable and unique  Common Mistakes to Avoid  âŒ Missing GSI keys: Always populate GSI1PK/GSI1SK and GSI2PK/GSI2SK âŒ Inconsistent patterns: Stick to ENTITY#ID format âŒ Wrong data types: Use strings for keys, appropriate types for values âŒ JSON syntax errors: Missing quotes, commas, or brackets  Important: If you get JSON syntax errors, check your quotation marks, commas, and brackets. The console will highlight syntax issues.\n\rCustomize Your Data Make It Personal Customize these values to make the workshop yours:\n User names and emails: Use your own information Product names: Create products you find interesting Addresses: Use your city/state Prices: Realistic values for your products  Add More Items Consider creating additional:\n More users (friends, family names) More products (different categories) Additional orders Multiple items per order  Ready for Querying Now that you have a complete e-commerce dataset with proper Single Table Design patterns, you\u0026rsquo;re ready to explore different query patterns. In the next section, we\u0026rsquo;ll learn how to efficiently retrieve this data using various query techniques.\nData Creation Complete: You\u0026rsquo;ve successfully implemented Single Table Design with proper entity relationships, composite keys, and GSI patterns!\n\r"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/2-single-table-design/2.4-query-patterns/",
	"title": "2.4 Query Patterns",
	"tags": [],
	"description": "",
	"content": "Query Patterns ğŸ” Learn efficient query techniques for Single Table Design using DynamoDB Console\nOverview Now that you have created your e-commerce data, let\u0026rsquo;s explore the powerful query patterns that make Single Table Design so effective. You\u0026rsquo;ll learn how to retrieve data efficiently using both table queries and Global Secondary Index (GSI) queries.\nQuery vs Scan - Critical Difference Always Use Query (Not Scan)  Query: Fast, efficient, cost-effective (uses primary keys) Scan: Slow, expensive, reads entire table (avoid in production)  Important: Always use Query operations in this workshop. Scan operations are inefficient and can quickly exceed Free Tier limits.\n\rPattern 1: Get User Profile Single Item Lookup Objective: Retrieve a specific user\u0026rsquo;s profile information\nAccess the Query Interface:\n Go to: Items tab in your DynamoDB table Click: \u0026ldquo;Query\u0026rdquo; button (not Scan) Ensure: Table query is selected (not index)  Configure User Profile Query Query Parameters:\n Partition key (PK): USER#user001 Sort key (SK): PROFILE Query condition: Use \u0026ldquo;equals\u0026rdquo; (default)  Expected Result: Single item containing user profile data\nExecute and Verify  Click \u0026ldquo;Run\u0026rdquo; Check results: Should return 1 item Verify data: Profile information should be displayed  Performance: ~1-2ms latency, 1 RCU consumed\nPattern 2: Get User\u0026rsquo;s Orders One-to-Many Relationship Query Objective: Retrieve all orders for a specific user\nQuery Configuration:\n Partition key (PK): USER#user001 Sort key condition: \u0026ldquo;begins_with\u0026rdquo; Sort key value: ORDER#  This pattern retrieves both the user profile AND all their orders in a single query.\nAdvanced Sort Key Options Available sort key conditions:\n = (equals): Exact match begins_with: Prefix matching between: Range queries \u0026gt;, \u0026gt;=, \u0026lt;, \u0026lt;=: Comparison operators  For this pattern: Use \u0026ldquo;begins_with\u0026rdquo; to get all items where SK starts with \u0026ldquo;ORDER#\u0026rdquo;\nExecute User Orders Query Expected Results:\n User profile (SK = PROFILE) All user orders (SK = ORDER#order001, etc.)  Why this works: All items with PK = USER#user001 are stored together and can be retrieved in one efficient query.\nPattern 3: Get Order Details with Items Hierarchical Data Query Objective: Get complete order information including all items\nQuery Configuration:\n Partition key (PK): ORDER#order001 Sort key: Leave empty (gets all items in partition)  Expected Results:\n Order details (SK = DETAILS) All order items (SK = ITEM#laptop001, SK = ITEM#book001)  Pattern 4: Products by Category (GSI Query) Using Global Secondary Index Objective: Find all products in a specific category\nSwitch to GSI Query:\n Click Query dropdown: Select \u0026ldquo;Query (index)\u0026rdquo; Choose Index: GSI1 Query the GSI: Use GSI key structure  Configure Category Query GSI1 Query Parameters:\n GSI1 Partition key: CATEGORY#electronics GSI1 Sort key: Leave empty (gets all products in category)  Why this works: All electronics products have GSI1PK = CATEGORY#electronics\nExecute Category Query Expected Results: All products where category = \u0026ldquo;electronics\u0026rdquo;\nTry additional categories:\n CATEGORY#books CATEGORY#clothing (if you created any)  Pattern 5: Orders by Status (GSI Query) Status-based Filtering Objective: Find all orders with a specific status\nGSI2 Query Configuration:\n Choose Index: GSI2 GSI2 Partition key: STATUS#pending GSI2 Sort key: Leave empty  Expected Results: All orders with status = \u0026ldquo;pending\u0026rdquo;\nTry Different Status Values Query other statuses:\n STATUS#shipped STATUS#delivered STATUS#cancelled  Pattern 6: Price Range Queries (GSI Query) Range-based Product Search Objective: Find products within a price range\nGSI2 Query Configuration:\n Choose Index: GSI2 GSI2 Partition key: PRICE#500-1000 GSI2 Sort key: Leave empty  Price Ranges Used in Our Data:\n PRICE#10-50 (books, accessories) PRICE#50-200 (mid-range items) PRICE#200-500 (premium items) PRICE#500-1000 (high-end items)  Query Performance Analysis Monitor Query Efficiency Check Performance Metrics:\n Go to: Metrics tab Monitor: Consumed read capacity Verify: No throttled requests  Expected Performance:\n Single item queries: ~1-2ms, 1 RCU Multi-item queries: ~3-5ms, 2-5 RCU GSI queries: ~2-4ms, 1-3 RCU  Advanced Query Techniques Combining Conditions Sort Key with Multiple Conditions:\n Between dates: SK between 2025-01-01 and 2025-12-31 Greater than: SK \u0026gt; ORDER#order001 Prefix + range: Complex filtering patterns  Query Result Options Additional Query Settings:\n Limit: Maximum number of items to return Scan index forward: Sort order (ascending/descending) Projection expression: Specific attributes to return Filter expression: Additional filtering after query  Query Pattern Summary Patterns You\u0026rsquo;ve Mastered    Pattern Query Type Key Structure Use Case     User Profile Table PK + SK exact Get specific user   User Orders Table PK + SK prefix Get user\u0026rsquo;s orders   Order Items Table PK all items Get order details   Category Products GSI1 Category grouping Product catalog   Status Orders GSI2 Status grouping Order management   Price Range GSI2 Price grouping Product search    Query Best Practices  Always use Query: Never use Scan for production workloads Design keys for queries: Think about access patterns first Use GSIs strategically: Enable multiple query patterns Monitor performance: Track consumed capacity and latency Test query patterns: Verify they return expected results  Key Insight: Single Table Design enables all these query patterns with consistent performance and minimal cost. Traditional relational approaches would require multiple queries and JOINs.\n\rQuery Troubleshooting Common Issues No Results Returned:\n âœ… Check partition key spelling âœ… Verify sort key conditions âœ… Ensure data exists with those keys  Unexpected Results:\n âœ… Review sort key conditions (exact vs begins_with) âœ… Check index selection (table vs GSI) âœ… Verify data was created correctly  Performance Issues:\n âœ… Avoid Scan operations âœ… Use specific partition keys âœ… Monitor consumed capacity  Ready for Global Scale You\u0026rsquo;ve now mastered Single Table Design query patterns! In the next module, we\u0026rsquo;ll take this data global by configuring Global Tables for multi-region deployment, enabling users worldwide to access your e-commerce platform with low latency.\nQuery Mastery Achieved: You can now efficiently retrieve data using all major Single Table Design patterns with consistent performance and cost optimization!\n\r"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/3-global-tables-setup/3.1-global-tables-overview/",
	"title": "3.1 Global Tables Overview",
	"tags": [],
	"description": "",
	"content": "Global Tables Overview ğŸŒ Understanding DynamoDB Global Tables architecture and replication mechanisms\nWhat Are Global Tables? Global Tables enable you to create a multi-region, multi-master database that provides local read and write performance for globally distributed applications. Your CloudFormation deployment has already configured this for you.\nArchitecture Components Current Workshop Setup Your infrastructure already includes:\n   Component US-East-1 EU-West-1 Status     DynamoDB Table Primary Replica âœ… Active   Table Name demo-ecommerce-freetier demo-ecommerce-freetier âœ… Synced   DynamoDB Streams Enabled Enabled âœ… Replicating   Replication Bi-directional Bi-directional âœ… Healthy    Replication Process How Data Flows Between Regions    Step Action Details     Step 1 User writes to US-EAST-1     Item USER#john, SK: PROFILE    Local write Immediate success    Stream record Created   Step 2 DynamoDB Streams captures change     Stream record NEW_AND_OLD_IMAGES    Timestamp 2025-08-11T15:30:00.123Z    Event INSERT   Step 3 Cross-region replication     Source us-east-1 stream    Target eu-west-1 table    Latency 500ms - 2 seconds   Step 4 EU-WEST-1 receives update     Item appears USER#john, SK: PROFILE    Available for reads Immediately    Status Replicated âœ…    Consistency Model Eventually Consistent Global Tables provides eventual consistency across regions:\n Immediate: Write succeeds immediately in the source region Propagation: Changes replicate to other regions within 0.5-2 seconds Convergence: All regions eventually have identical data Durability: Data is never lost during replication  Conflict Resolution When the same item is modified in multiple regions simultaneously:\nLast Writer Wins strategy:\n Compare timestamps of conflicting updates Keep the later timestamp (more recent change) Overwrite earlier changes in all regions Notify through CloudWatch metrics  Example conflict scenario:\nTime: 15:30:00 - US user updates: name = \u0026#34;John Smith\u0026#34; Time: 15:30:01 - EU user updates: name = \u0026#34;John Doe\u0026#34; Result: All regions will have name = \u0026#34;John Doe\u0026#34; (EU update wins due to later timestamp) Global Tables Benefits Performance Benefits  Local Latency: Sub-10ms response times in each region Global Scale: Serve users worldwide without performance penalty Load Distribution: Traffic distributed across regions  Availability Benefits  Regional Failover: Automatic failover if one region becomes unavailable Disaster Recovery: Built-in DR across geographic regions 99.999% Availability: Higher availability than single-region deployments  Operational Benefits  No Code Changes: Applications work with any region Automatic Scaling: Each region scales independently Unified Management: Single table view across all regions  Key Concepts to Remember Multi-Master Replication  Any region can accept writes All regions can serve reads No single point of failure  Stream-Based Replication  DynamoDB Streams power the replication Ordered delivery ensures consistency Retry logic handles temporary failures  Region Independence  Each region operates independently Network partitions don\u0026rsquo;t affect local operations Cross-region connectivity only needed for replication  Workshop Advantage: Your CloudFormation template has already configured all Global Tables components. You can focus on understanding and testing the functionality!\n\rLimitations to Understand Eventual Consistency Challenges  Temporary inconsistencies possible for 0.5-2 seconds Application design must handle eventual consistency Strong consistency only available within single region  Conflict Resolution Limitations  Last Writer Wins can overwrite changes No custom conflict resolution logic Application-level conflict handling may be needed  Cross-Region Dependencies  Network connectivity required for replication Regional outages can delay replication Cross-region latency affects replication speed  Real-World Use Cases Ideal for Global Tables    Use Case Why It Works Considerations     User Profiles Infrequent updates, read-heavy Handle profile conflicts   Product Catalogs Content distribution, global access Inventory sync challenges   Gaming Leaderboards Global competition, eventual consistency OK Score conflicts possible   IoT Sensor Data Time-series data, append-only High write volume    Challenging Scenarios  Financial transactions (require strong consistency) Inventory management (stock levels need accuracy) Real-time collaboration (immediate consistency needed)  Next Steps Now that you understand Global Tables architecture, let\u0026rsquo;s verify your multi-region setup and see replication in action through the AWS Console.\nReady to Explore: Your Global Tables are already configured and running. Time to see them in action!\n\r"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/3-global-tables-setup/3.2-verify-global-setup/",
	"title": "3.2 Verify Global Setup",
	"tags": [],
	"description": "",
	"content": "Verify Global Setup âœ… Step-by-step verification of your Global Tables configuration through AWS Console\nOverview Your CloudFormation template has automatically configured Global Tables between US-East-1 and EU-West-1. Let\u0026rsquo;s verify everything is working correctly before we start testing replication.\nStep 1: Access Primary Region Navigate to US-East-1  Open AWS Console: Ensure you\u0026rsquo;re logged in Check Region: Top-right corner should show \u0026ldquo;N. Virginia\u0026rdquo; Switch if needed: Click region dropdown â†’ \u0026ldquo;US East (N. Virginia)\u0026rdquo;  Screenshot Location: Add screenshot of AWS Console with region selector showing US East (N. Virginia)\n\rFind Your DynamoDB Table  Services: Navigate to DynamoDB service Tables: Click \u0026ldquo;Tables\u0026rdquo; in left sidebar Locate: Find demo-ecommerce-freetier Status: Verify table shows \u0026ldquo;Active\u0026rdquo;  Screenshot Location: Add screenshot of DynamoDB Tables list showing demo-ecommerce-freetier with Active status\n\rStep 2: Check Global Tables Configuration Access Global Tables Tab  Click: Table name demo-ecommerce-freetier Navigate: Click \u0026ldquo;Global tables\u0026rdquo; tab Review: Global Tables configuration  Screenshot Location: Add screenshot of table overview with Global tables tab highlighted\n\rVerify Global Tables Status Expected Configuration:\n   Region Status Role Health     us-east-1 Active Primary âœ… Healthy   eu-west-1 Active Replica âœ… Healthy    Key indicators to verify:\n Replication Status: \u0026ldquo;Healthy\u0026rdquo; or \u0026ldquo;Active\u0026rdquo; Last Replication: Recent timestamp Pending Updates: Should be 0  Screenshot Location: Add screenshot of Global tables configuration showing both regions with healthy status\n\rStep 3: Verify Secondary Region Switch to EU-West-1  Region Selector: Click region dropdown (top-right) Select: \u0026ldquo;Europe (Ireland)\u0026rdquo; - eu-west-1 Wait: Allow console to switch regions  Screenshot Location: Add screenshot of region selector dropdown with Europe (Ireland) highlighted\n\rCheck Replica Table  Navigate: DynamoDB â†’ Tables Find: Same table name demo-ecommerce-freetier Verify: Table exists and shows \u0026ldquo;Active\u0026rdquo; Check: Global tables tab shows replica status  Expected in EU-West-1:\n Table Name: demo-ecommerce-freetier (identical) Status: Active Role: Replica table Primary Region: us-east-1  Screenshot Location: Add screenshot of EU region showing the replica table with same name\n\rStep 4: Compare Table Schemas Verify Schema Consistency Both regions should have identical table schema:\nPrimary Keys:\n Partition Key: PK (String) Sort Key: SK (String)  Global Secondary Indexes:\n GSI1: GSI1PK (String), GSI1SK (String) GSI2: GSI2PK (String), GSI2SK (String)  Settings:\n Read Capacity: 5 units (provisioned) Write Capacity: 5 units (provisioned) Point-in-time Recovery: Enabled DynamoDB Streams: Enabled  Screenshot Location: Add screenshot comparing table schema between US and EU regions\n\rStep 5: Check Current Data Verify Existing Data Replication If you\u0026rsquo;ve completed Module 2, check that your existing data appears in both regions:\nSwitch to US-East-1:\n Go to: Items tab Count items: Note the number of items  Switch to EU-West-1:\n Go to: Items tab Compare count: Should match US region exactly  If item counts don\u0026rsquo;t match:\n Wait 1-2 minutes for replication Refresh the browser page Check Global Tables health status  Screenshot Location: Add screenshot showing identical item counts in both regions\n\rStep 6: Verify Stream Configuration Check DynamoDB Streams In US-East-1:\n Table Overview: Go to table details Exports and streams: Click tab Stream details: Verify settings  Expected Stream Configuration:\n Stream Status: Enabled Stream view type: New and old images Stream ARN: Should be present Shard count: 1 or more active shards  Screenshot Location: Add screenshot of DynamoDB Streams configuration showing enabled status\n\rStep 7: Health Check Dashboard Monitor Replication Health Access Monitoring:\n Metrics Tab: Click in table view Global Tables metrics: Look for replication metrics Key metrics:  Replication latency Pending replication count Failed replication events    Healthy Indicators:\n Replication Latency: \u0026lt; 2 seconds average Pending Count: 0 or very low Error Rate: 0%  Screenshot Location: Add screenshot of metrics dashboard showing healthy replication metrics\n\rStep 8: Network Connectivity Test Test Cross-Region Communication Simple connectivity verification:\n Create test item in US-East-1 Wait 30 seconds Check EU-West-1 for the item Delete test item from either region  Test Item Example:\n{ \u0026#34;PK\u0026#34;: \u0026#34;TEST#connectivity\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;VERIFICATION\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-08-11T15:00:00Z\u0026#34;, \u0026#34;test_purpose\u0026#34;: \u0026#34;global_tables_verification\u0026#34; } \rScreenshot Location: Add screenshot of test item appearing in both regions\n\rTroubleshooting Common Issues Issue: Table Not Found in EU-West-1 Possible Causes:\n Wrong region selected CloudFormation deployment incomplete Global Tables setup failed  Solutions:\n Double-check region in top-right corner Verify CloudFormation stack completed successfully Check IAM permissions for cross-region access  Issue: Replication Status Unhealthy Check These Items:\n Network connectivity between regions DynamoDB Streams enabled on source table IAM permissions for Global Tables service Table capacity not throttling  Issue: Item Counts Don\u0026rsquo;t Match Troubleshooting Steps:\n Wait longer (up to 2 minutes) Refresh browser page Check for errors in CloudWatch logs Verify no throttling in metrics  Verification Checklist Before proceeding to multi-region operations:\n Both regions accessible through AWS Console Table exists in both us-east-1 and eu-west-1 Global Tables status shows \u0026ldquo;Healthy\u0026rdquo; or \u0026ldquo;Active\u0026rdquo; Schema identical between regions DynamoDB Streams enabled Existing data replicated (if any) Test connectivity working Monitoring metrics available  Verification Complete: Your Global Tables setup is healthy and ready for multi-region operations!\n\rNext Steps With Global Tables verified and healthy, you\u0026rsquo;re ready to experience multi-region operations. In the next section, we\u0026rsquo;ll create data in one region and watch it automatically appear in another!\nPro Tip: Keep both region tabs open in your browser (US-East-1 and EU-West-1) to easily switch between them during exercises.\n\r"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/3-global-tables-setup/3.3-multi-region-operations/",
	"title": "3.3 Multi-Region Operations",
	"tags": [],
	"description": "",
	"content": "Multi-Region Operations ğŸŒ Hands-on practice with cross-region read/write operations and replication testing\nOverview Now that your Global Tables are verified, let\u0026rsquo;s experience multi-region operations firsthand. You\u0026rsquo;ll create data in one region, verify it replicates to another, and test conflict resolution scenarios.\nExercise 1: Write to Primary, Read from Replica Step 1: Create Global User in US-East-1 Ensure you\u0026rsquo;re in US-East-1:\n Check region: Top-right should show \u0026ldquo;N. Virginia\u0026rdquo; Navigate: DynamoDB â†’ Tables â†’ demo-ecommerce-freetier Go to: Items tab Click: \u0026ldquo;Create item\u0026rdquo;  Screenshot Location: Add screenshot of US-East-1 region with Create item dialog open\n\rUser Creation Template Switch to JSON view and create:\n{ \u0026#34;PK\u0026#34;: \u0026#34;USER#global-user-[your-name]\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;PROFILE\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;USER#[your-name]@global.com\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;PROFILE\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;global-user-[your-name]\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[Your Name] Global\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;[your-name]@global.com\u0026#34;, \u0026#34;region_created\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;created_timestamp\u0026#34;: \u0026#34;2025-08-11T15:30:00Z\u0026#34;, \u0026#34;test_type\u0026#34;: \u0026#34;global_replication\u0026#34;, \u0026#34;workshop_session\u0026#34;: \u0026#34;module3\u0026#34; } Important: Replace [your-name] with your actual name to make items unique.\nScreenshot Location: Add screenshot of JSON editor with global user data being entered\n\rStep 2: Note Creation Time Record the details:\n Click \u0026ldquo;Create item\u0026rdquo; Note the time: Record when you clicked create Take screenshot: Of the created item  Screenshot Location: Add screenshot showing successfully created item in US region\n\rStep 3: Switch to EU-West-1 Change regions:\n Region selector: Click dropdown (top-right) Select: \u0026ldquo;Europe (Ireland)\u0026rdquo; Wait: For region switch to complete Navigate: DynamoDB â†’ Tables â†’ demo-ecommerce-freetier  Step 4: Query for Replicated Data Search for your user:\n Items tab: Navigate to items view Click: \u0026ldquo;Query\u0026rdquo; button Configure query:  Partition key (PK): USER#global-user-[your-name] Sort key (SK): PROFILE   Click: \u0026ldquo;Run\u0026rdquo;  Screenshot Location: Add screenshot of query setup in EU region looking for the US-created user\n\rStep 5: Verify Replication Expected results:\n If immediate: Item appears right away If delayed: Wait 30-60 seconds and try again Replication time: Note how long it took  Verify the data:\n All attributes: Should match exactly region_created: Should still show \u0026ldquo;us-east-1\u0026rdquo; Timestamps: Should be identical  Screenshot Location: Add screenshot showing the replicated item in EU region with identical data\n\rExercise 2: Write from Replica, Read from Primary Step 1: Create Product in EU-West-1 Stay in EU-West-1 and create a product:\n{ \u0026#34;PK\u0026#34;: \u0026#34;PRODUCT#eu-product-[unique-id]\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;DETAILS\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;CATEGORY#eu-electronics\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;PRODUCT#eu-product-[unique-id]\u0026#34;, \u0026#34;GSI2PK\u0026#34;: \u0026#34;PRICE#200-500\u0026#34;, \u0026#34;GSI2SK\u0026#34;: \u0026#34;PRODUCT#eu-product-[unique-id]\u0026#34;, \u0026#34;product_id\u0026#34;: \u0026#34;eu-product-[unique-id]\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;European Smartphone\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Created in EU region\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;eu-electronics\u0026#34;, \u0026#34;price\u0026#34;: 299, \u0026#34;stock\u0026#34;: 50, \u0026#34;region_created\u0026#34;: \u0026#34;eu-west-1\u0026#34;, \u0026#34;created_timestamp\u0026#34;: \u0026#34;2025-08-11T15:35:00Z\u0026#34;, \u0026#34;test_type\u0026#34;: \u0026#34;reverse_replication\u0026#34; } \rScreenshot Location: Add screenshot of product creation in EU region\n\rStep 2: Switch Back to US-East-1 Return to primary region:\n Region selector: \u0026ldquo;US East (N. Virginia)\u0026rdquo; Navigate: DynamoDB â†’ Tables â†’ Items Query for product:  PK: PRODUCT#eu-product-[unique-id] SK: DETAILS    Screenshot Location: Add screenshot of US region query looking for EU-created product\n\rStep 3: Verify Reverse Replication Check the results:\n Product appears: In US region region_created: Still shows \u0026ldquo;eu-west-1\u0026rdquo; All data intact: Exact copy from EU  This demonstrates bi-directional replication - you can write to any region!\nScreenshot Location: Add screenshot showing EU-created product now visible in US region\n\rExercise 3: Conflict Resolution Testing Step 1: Create Base Order In US-East-1, create an order:\n{ \u0026#34;PK\u0026#34;: \u0026#34;USER#global-user-[your-name]\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;ORDER#conflict-test\u0026#34;, \u0026#34;GSI1PK\u0026#34;: \u0026#34;ORDER#conflict-test\u0026#34;, \u0026#34;GSI1SK\u0026#34;: \u0026#34;DETAILS\u0026#34;, \u0026#34;GSI2PK\u0026#34;: \u0026#34;STATUS#pending\u0026#34;, \u0026#34;GSI2SK\u0026#34;: \u0026#34;ORDER#conflict-test\u0026#34;, \u0026#34;order_id\u0026#34;: \u0026#34;conflict-test\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;global-user-[your-name]\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;total_amount\u0026#34;: 100, \u0026#34;last_updated_region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;conflict_test\u0026#34;: true } Wait 2 minutes for replication to complete.\nScreenshot Location: Add screenshot of conflict test order creation in US region\n\rStep 2: Simultaneous Updates (Advanced) If working with a partner:\n Partner A: Update order in US-East-1 Partner B: Update same order in EU-West-1 Both execute: Within 10 seconds of each other  US Update (Partner A):\n{ \u0026#34;total_amount\u0026#34;: 150, \u0026#34;last_updated_region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;update_timestamp\u0026#34;: \u0026#34;2025-08-11T15:40:00Z\u0026#34; } EU Update (Partner B):\n{ \u0026#34;total_amount\u0026#34;: 200, \u0026#34;last_updated_region\u0026#34;: \u0026#34;eu-west-1\u0026#34;, \u0026#34;update_timestamp\u0026#34;: \u0026#34;2025-08-11T15:40:05Z\u0026#34; } \rScreenshot Location: Add screenshot showing edit dialog for conflict testing\n\rStep 3: Observe Conflict Resolution After 2-3 minutes:\n Check both regions: Query the same order Compare results: Which update won? Understand why: Later timestamp wins  Expected outcome: EU update wins because timestamp 15:40:05 \u0026gt; 15:40:00\nScreenshot Location: Add screenshot showing final conflict resolution result in both regions\n\rExercise 4: Query Patterns Across Regions Step 1: Category Query in EU Test GSI queries work across regions:\n Stay in EU-West-1 Query Index: GSI1 GSI1PK: CATEGORY#eu-electronics Run query  Expected: Shows products created in EU region\nScreenshot Location: Add screenshot of GSI category query in EU region\n\rStep 2: Status Query in US Test cross-region status queries:\n Switch to US-East-1 Query Index: GSI2 GSI2PK: STATUS#pending Run query  Expected: Shows orders from both regions with pending status\nScreenshot Location: Add screenshot of GSI status query showing orders from multiple regions\n\rExercise 5: Replication Timing Analysis Step 1: Measure Replication Speed Create timestamped items:\n Record start time: Note exact time before creation Create item: In one region Switch regions: Immediately Query repeatedly: Until item appears Calculate delay: End time - start time  Test Item Template:\n{ \u0026#34;PK\u0026#34;: \u0026#34;TEST#timing-[timestamp]\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;REPLICATION\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;[exact-timestamp]\u0026#34;, \u0026#34;source_region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;test_purpose\u0026#34;: \u0026#34;timing_analysis\u0026#34; } Step 2: Document Results Record your findings:\n Fastest replication: ___ seconds Slowest replication: ___ seconds Average time: ___ seconds Consistency: Usually \u0026lt; 2 seconds  Real-World Scenarios Scenario 1: Global User Login Simulate global application:\n User logs in: US region Profile updated: Last login timestamp User travels: EU region App checks: Profile from EU Verify: Recent login time visible  Scenario 2: Inventory Management Product stock updates:\n Product sold: US region (-1 stock) Same product: EU region query Stock level: Eventually consistent Business logic: Handle temporary inconsistency  Scenario 3: Order Processing Multi-region order flow:\n Order created: EU region Payment processed: US region Status updated: EU region Fulfillment: Reads from nearest region  Performance Monitoring Check Replication Metrics During exercises:\n Monitor: CloudWatch metrics Watch: Replication latency Observe: Pending replication count Track: Error rates (should be 0)  Screenshot Location: Add screenshot of CloudWatch metrics showing replication performance during exercises\n\rTroubleshooting Guide Replication Not Working Common issues:\n Wrong region: Double-check region selection Typos in keys: Exact match required for queries Browser cache: Refresh page Wait longer: Up to 2 minutes possible  Queries Returning Empty Checklist:\n Correct PK/SK: Exact string match Region correct: Item exists in queried region GSI populated: GSI keys included in item Query type: Using Query, not Scan  Conflict Resolution Unexpected Understanding:\n Timestamp precision: Millisecond level Clock synchronization: AWS handles timing Application design: Plan for overwrites  Important: If you experience issues, check the Global Tables health status in the console and verify network connectivity.\n\rExercise Summary By completing these exercises, you\u0026rsquo;ve experienced:\n âœ… Cross-region replication in both directions âœ… Eventual consistency timing âœ… Conflict resolution with Last Writer Wins âœ… Query patterns working across regions âœ… Real-world scenarios and timing analysis  Multi-Region Mastery: You now understand how Global Tables enables truly global applications with local performance!\n\rNext Steps With hands-on Global Tables experience complete, let\u0026rsquo;s monitor the replication performance and understand the metrics that help you operate global applications in production.\n"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/4-streams-lambda-processing/4.1-stream-configuration/",
	"title": "4.1 Stream Configuration",
	"tags": [],
	"description": "",
	"content": "Enable DynamoDB Streams ğŸ”§ Turn on change tracking for your DynamoDB table\nOverview DynamoDB Streams capture every change to your table items. This enables real-time processing with Lambda functions.\nStream View Types Choose what data to capture:\n NEW_AND_OLD_IMAGES: Complete before/after data (recommended for demo) NEW_IMAGE: Only updated data OLD_IMAGE: Only previous data KEYS_ONLY: Just the keys  Step-by-Step: Enable Streams Step 1: Open DynamoDB Console  AWS Console â†’ Search \u0026ldquo;DynamoDB\u0026rdquo; Tables â†’ Click your table demo-ecommerce-freetier Click \u0026ldquo;Exports and streams\u0026rdquo; tab  Step 2: Turn On Stream  Find \u0026ldquo;DynamoDB stream\u0026rdquo; section Click \u0026ldquo;Turn on\u0026rdquo; button Select \u0026ldquo;New and old images\u0026rdquo; Click \u0026ldquo;Turn on stream\u0026rdquo;  Step 3: Note Stream ARN Copy the Stream ARN - you\u0026rsquo;ll need it for Lambda setup:\narn:aws:dynamodb:us-east-1:123456789012:table/demo-ecommerce-freetier/stream/2025-08-13T10:00:00.000\r\rStream Enabled! Your table now captures all changes. Next, we\u0026rsquo;ll create a Lambda function to process these events.\n\rVerification âœ… Stream status shows \u0026ldquo;Enabled\u0026rdquo;\nâœ… Stream ARN is displayed\nâœ… View type is \u0026ldquo;New and old images\u0026rdquo; 3. View type selection: Choose \u0026ldquo;New and old images\u0026rdquo; 4. Confirmation: Click \u0026ldquo;Turn on stream\u0026rdquo;\nScreenshot Location: Add screenshot of stream configuration dialog with \u0026ldquo;New and old images\u0026rdquo; selected\n\rStep 3: Verify Stream Configuration Check stream status:\n Stream details: Note the stream ARN appears Status: Should show \u0026ldquo;Active\u0026rdquo; View type: Confirms \u0026ldquo;New and old images\u0026rdquo; Creation time: Shows when stream was enabled  Screenshot Location: Add screenshot showing active stream with ARN and configuration details\n\rExercise 2: Understanding Stream Settings Stream View Type Comparison Choose the right view type for your use case:\n   View Type Use Case Data Captured     KEYS_ONLY Audit logging PK, SK only   NEW_IMAGE Cache updates Item after change   OLD_IMAGE Change tracking Item before change   NEW_AND_OLD_IMAGES Full audit Both versions    Performance Considerations Stream Configuration Impact:\n Storage: NEW_AND_OLD_IMAGES uses most space Lambda payload: Larger payloads with full images Processing time: More data = longer processing Cost: Minimal additional cost for streams  Free Tier Note: DynamoDB Streams are included at no additional charge. Lambda processing stays within Free Tier limits.\n\rExercise 3: Test Stream Functionality Step 1: Create Test Item Generate a stream event:\n Items tab: Go back to \u0026ldquo;Items\u0026rdquo; tab Create item: Click \u0026ldquo;Create item\u0026rdquo; Add test data:  { \u0026#34;PK\u0026#34;: \u0026#34;STREAM#test-item\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;EVENT#001\u0026#34;, \u0026#34;event_type\u0026#34;: \u0026#34;stream_test\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Testing DynamoDB Stream functionality\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-08-11T16:00:00Z\u0026#34;, \u0026#34;test_purpose\u0026#34;: \u0026#34;verify_stream_capture\u0026#34; } Create: Click \u0026ldquo;Create item\u0026rdquo;  Screenshot Location: Add screenshot of item creation dialog with stream test data\n\rStep 2: Monitor Stream Activity Check stream metrics:\n CloudWatch: Open CloudWatch console in new tab Metrics: Navigate to Metrics DynamoDB: Click \u0026ldquo;DynamoDB\u0026rdquo; namespace Stream metrics: Look for stream-related metrics IncomingRecords: Should show 1 new record  Screenshot Location: Add screenshot of CloudWatch showing DynamoDB stream metrics\n\rStep 3: Understand Stream Records Stream record structure (for reference):\n{ \u0026#34;Records\u0026#34;: [ { \u0026#34;eventID\u0026#34;: \u0026#34;12345...\u0026#34;, \u0026#34;eventName\u0026#34;: \u0026#34;INSERT\u0026#34;, \u0026#34;eventVersion\u0026#34;: \u0026#34;1.1\u0026#34;, \u0026#34;eventSource\u0026#34;: \u0026#34;aws:dynamodb\u0026#34;, \u0026#34;awsRegion\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;dynamodb\u0026#34;: { \u0026#34;ApproximateCreationDateTime\u0026#34;: 1642857600, \u0026#34;Keys\u0026#34;: { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;STREAM#test-item\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;EVENT#001\u0026#34;} }, \u0026#34;NewImage\u0026#34;: { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;STREAM#test-item\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;EVENT#001\u0026#34;}, \u0026#34;event_type\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;stream_test\u0026#34;}, \u0026#34;description\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Testing DynamoDB Stream functionality\u0026#34;}, \u0026#34;created_at\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;2025-08-11T16:00:00Z\u0026#34;}, \u0026#34;test_purpose\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;verify_stream_capture\u0026#34;} }, \u0026#34;SequenceNumber\u0026#34;: \u0026#34;123456789\u0026#34;, \u0026#34;SizeBytes\u0026#34;: 245, \u0026#34;StreamViewType\u0026#34;: \u0026#34;NEW_AND_OLD_IMAGES\u0026#34; } } ] } Key components:\n eventName: INSERT (since this is a new item) Keys: Primary key of the changed item NewImage: Complete item data after creation OldImage: Would be empty for INSERT events  Exercise 4: Stream Configuration Best Practices Optimal Configuration for Workshop Recommended settings:\n âœ… View Type: NEW_AND_OLD_IMAGES (comprehensive audit trail) âœ… Retention: 24 hours (default, sufficient for processing) âœ… Shards: Auto-managed by AWS âœ… Processing: Lambda with appropriate batch size  Security Considerations Access control:\n IAM permissions: Lambda needs stream read permissions Encryption: Streams inherit table encryption settings VPC: Streams work within your VPC configuration Monitoring: CloudTrail logs stream access  Cost Optimization Stream cost factors:\n Read requests: No additional charge for stream writes Lambda invocations: Count toward Free Tier Data transfer: Minimal for in-region processing Storage: Stream records retained for 24 hours only  Exercise 5: Advanced Stream Configuration Multiple Consumer Pattern When you need multiple processors:\n Single stream: One DynamoDB stream per table Multiple Lambdas: Each can process the same stream Kinesis Data Streams: For more complex routing Event filtering: Lambda-level filtering  Cross-Region Considerations Global Tables + Streams:\n Each region: Has its own stream Replication events: Generate stream records Filtering: Distinguish app writes from replication Processing: Handle regional differences  Screenshot Location: Add screenshot of Global Tables with streams enabled in multiple regions\n\rTroubleshooting Common Issues Stream Not Appearing Check these items:\n Permissions: Ensure you have DynamoDB full access Region: Verify you\u0026rsquo;re in the correct AWS region Table status: Table must be ACTIVE to enable streams Refresh: Browser refresh may be needed  Stream Configuration Failed Possible causes:\n Table updating: Wait for table to be ACTIVE Permissions: Need dynamodb:EnableStream permission Rate limits: Wait and retry if rate limited Billing: Ensure account is in good standing  Stream Records Missing Debugging steps:\n Stream status: Confirm stream is ACTIVE Write operations: Ensure items are actually changing Time delay: Allow 1-2 minutes for propagation Metrics: Check CloudWatch for IncomingRecords  Configuration Summary By completing this exercise, you have:\n âœ… Enabled DynamoDB Streams on your table âœ… Configured view type for comprehensive change capture âœ… Tested stream functionality with sample data âœ… Understood stream record structure and components âœ… Applied best practices for optimal configuration  Stream Ready: Your DynamoDB table now captures every change and is ready for Lambda processing!\n\rNext Steps With streams configured, you\u0026rsquo;re ready to create Lambda functions that will process these events in real-time. In the next section, we\u0026rsquo;ll build and deploy a Lambda function optimized for DynamoDB stream processing.\n"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/4-streams-lambda-processing/4.2-lambda-function-setup/",
	"title": "4.2 Lambda Function Setup",
	"tags": [],
	"description": "",
	"content": "Create Stream Processing Lambda âš™ï¸ Build a Lambda function to process DynamoDB stream events in real-time\nOverview AWS Lambda provides serverless compute to process DynamoDB stream events. Your function will automatically trigger when items change in your table, enabling real-time processing patterns.\nFunction Requirements Free Tier Optimized Configuration:\n Runtime: Python 3.9 (reliable and well-supported) Memory: 128 MB (minimum for Free Tier) Timeout: 30 seconds (sufficient for stream processing) Concurrent executions: 10 (Free Tier safe)  Exercise 1: Create Lambda Function Step 1: Access Lambda Console Navigate to Lambda service:\n AWS Console: Search \u0026ldquo;Lambda\u0026rdquo; Functions: Click \u0026ldquo;Functions\u0026rdquo; in left sidebar Create function: Click \u0026ldquo;Create function\u0026rdquo; button Author from scratch: Select this option  Screenshot Location: Add screenshot of Lambda console with Create function button highlighted\n\rStep 2: Configure Basic Settings Function configuration:\n Function name: demo-dynamodb-stream-processor Runtime: Select \u0026ldquo;Python 3.9\u0026rdquo; Architecture: Leave as \u0026ldquo;x86_64\u0026rdquo; Permissions: \u0026ldquo;Create a new role with basic Lambda permissions\u0026rdquo; Create function: Click to proceed  Screenshot Location: Add screenshot of Lambda function creation form with specified settings\n\rStep 3: Configure Function Settings Optimize for Free Tier:\n Configuration tab: Click after function creation General configuration: Click \u0026ldquo;Edit\u0026rdquo; Memory: Set to 128 MB Timeout: Set to 30 seconds Save: Click to apply changes  Screenshot Location: Add screenshot of Lambda function configuration settings with memory and timeout values\n\rExercise 2: Add Stream Processing Code Step 1: Replace Function Code Navigate to code editor:\n Code tab: Click to open code editor lambda_function.py: Replace existing code with:  import json import boto3 import logging from datetime import datetime # Configure logging logger = logging.getLogger() logger.setLevel(logging.INFO) def lambda_handler(event, context): \u0026#34;\u0026#34;\u0026#34; Process DynamoDB Stream events Optimized for AWS Free Tier \u0026#34;\u0026#34;\u0026#34; try: processed_records = 0 # Process each record in the batch for record in event[\u0026#39;Records\u0026#39;]: event_name = record[\u0026#39;eventName\u0026#39;] # Process INSERT, MODIFY, REMOVE events if event_name in [\u0026#39;INSERT\u0026#39;, \u0026#39;MODIFY\u0026#39;, \u0026#39;REMOVE\u0026#39;]: process_stream_record(record) processed_records += 1 # Return success response return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;message\u0026#39;: f\u0026#39;Successfully processed {processed_records}records\u0026#39;, \u0026#39;timestamp\u0026#39;: datetime.utcnow().isoformat(), \u0026#39;processed_count\u0026#39;: processed_records }) } except Exception as e: logger.error(f\u0026#34;Error processing stream records: {str(e)}\u0026#34;) # Re-raise for Lambda retry logic raise e def process_stream_record(record): \u0026#34;\u0026#34;\u0026#34; Process individual stream record Add your business logic here \u0026#34;\u0026#34;\u0026#34; event_name = record[\u0026#39;eventName\u0026#39;] # Extract key information if \u0026#39;dynamodb\u0026#39; in record: keys = record[\u0026#39;dynamodb\u0026#39;].get(\u0026#39;Keys\u0026#39;, {}) pk = keys.get(\u0026#39;PK\u0026#39;, {}).get(\u0026#39;S\u0026#39;, \u0026#39;\u0026#39;) sk = keys.get(\u0026#39;SK\u0026#39;, {}).get(\u0026#39;S\u0026#39;, \u0026#39;\u0026#39;) logger.info(f\u0026#34;Processing {event_name}for item: {pk}#{sk}\u0026#34;) # Handle different event types if event_name == \u0026#39;INSERT\u0026#39;: handle_insert_event(record) elif event_name == \u0026#39;MODIFY\u0026#39;: handle_modify_event(record) elif event_name == \u0026#39;REMOVE\u0026#39;: handle_remove_event(record) def handle_insert_event(record): \u0026#34;\u0026#34;\u0026#34; Handle new item creation \u0026#34;\u0026#34;\u0026#34; logger.info(\u0026#34;Processing INSERT event\u0026#34;) # Get new item data new_image = record[\u0026#39;dynamodb\u0026#39;].get(\u0026#39;NewImage\u0026#39;, {}) # Example: Send notification for new user if \u0026#39;USER#\u0026#39; in str(new_image.get(\u0026#39;PK\u0026#39;, {})): logger.info(\u0026#34;New user created - could send welcome email\u0026#34;) # Example: Update inventory for new product elif \u0026#39;PRODUCT#\u0026#39; in str(new_image.get(\u0026#39;PK\u0026#39;, {})): logger.info(\u0026#34;New product created - could update search index\u0026#34;) # Example: Process new order elif \u0026#39;ORDER#\u0026#39; in str(new_image.get(\u0026#39;SK\u0026#39;, {})): logger.info(\u0026#34;New order created - could trigger fulfillment\u0026#34;) def handle_modify_event(record): \u0026#34;\u0026#34;\u0026#34; Handle item updates \u0026#34;\u0026#34;\u0026#34; logger.info(\u0026#34;Processing MODIFY event\u0026#34;) # Get before and after images old_image = record[\u0026#39;dynamodb\u0026#39;].get(\u0026#39;OldImage\u0026#39;, {}) new_image = record[\u0026#39;dynamodb\u0026#39;].get(\u0026#39;NewImage\u0026#39;, {}) # Example: Check for status changes old_status = old_image.get(\u0026#39;status\u0026#39;, {}).get(\u0026#39;S\u0026#39;, \u0026#39;\u0026#39;) new_status = new_image.get(\u0026#39;status\u0026#39;, {}).get(\u0026#39;S\u0026#39;, \u0026#39;\u0026#39;) if old_status != new_status: logger.info(f\u0026#34;Status changed from {old_status}to {new_status}\u0026#34;) # Could trigger notifications, cache updates, etc. # Example: Price change detection old_price = old_image.get(\u0026#39;price\u0026#39;, {}).get(\u0026#39;N\u0026#39;, \u0026#39;0\u0026#39;) new_price = new_image.get(\u0026#39;price\u0026#39;, {}).get(\u0026#39;N\u0026#39;, \u0026#39;0\u0026#39;) if old_price != new_price: logger.info(f\u0026#34;Price changed from {old_price}to {new_price}\u0026#34;) # Could invalidate cache, update recommendations, etc. def handle_remove_event(record): \u0026#34;\u0026#34;\u0026#34; Handle item deletion \u0026#34;\u0026#34;\u0026#34; logger.info(\u0026#34;Processing REMOVE event\u0026#34;) # Get deleted item data old_image = record[\u0026#39;dynamodb\u0026#39;].get(\u0026#39;OldImage\u0026#39;, {}) # Example: Cleanup related data if \u0026#39;USER#\u0026#39; in str(old_image.get(\u0026#39;PK\u0026#39;, {})): logger.info(\u0026#34;User deleted - could cleanup user data\u0026#34;) # Example: Remove from search index elif \u0026#39;PRODUCT#\u0026#39; in str(old_image.get(\u0026#39;PK\u0026#39;, {})): logger.info(\u0026#34;Product deleted - could remove from search\u0026#34;) Deploy: Click \u0026ldquo;Deploy\u0026rdquo; to save the code  Screenshot Location: Add screenshot of Lambda code editor with the stream processing code\n\rStep 2: Test Function Syntax Validate the code:\n Test tab: Click \u0026ldquo;Test\u0026rdquo; tab Create test event: Click \u0026ldquo;Create new event\u0026rdquo; Event template: Select \u0026ldquo;DynamoDB Stream\u0026rdquo; template Event name: test-stream-event Test: Click \u0026ldquo;Test\u0026rdquo; to validate syntax  Screenshot Location: Add screenshot of Lambda test configuration with DynamoDB Stream template\n\rExercise 3: Configure Event Source Mapping Step 1: Add DynamoDB Trigger Connect Lambda to DynamoDB Stream:\n Function overview: In Lambda console Add trigger: Click \u0026ldquo;Add trigger\u0026rdquo; button Trigger configuration:  Source: Select \u0026ldquo;DynamoDB\u0026rdquo; DynamoDB table: Choose demo-ecommerce-freetier Batch size: Set to 10 (Free Tier optimized) Starting position: Select \u0026ldquo;Trim horizon\u0026rdquo;   Add: Click to create trigger  Screenshot Location: Add screenshot of trigger configuration dialog with DynamoDB settings\n\rStep 2: Verify Event Source Mapping Check trigger configuration:\n Function overview: Should show DynamoDB trigger Configuration: Verify settings:  Batch size: 10 records Starting position: Trim horizon Status: Enabled State: Creating â†’ Enabled    Screenshot Location: Add screenshot showing successful DynamoDB trigger configuration\n\rStep 3: Configure IAM Permissions Update Lambda execution role:\n Configuration tab: Click \u0026ldquo;Permissions\u0026rdquo; Execution role: Click role name link IAM console: Opens in new tab Attach policies: Add AWSLambdaDynamoDBExecutionRole Save: Return to Lambda console  Screenshot Location: Add screenshot of IAM role with DynamoDB stream permissions\n\rExercise 4: Test Stream Processing Step 1: Create Test Item Generate stream event:\n DynamoDB console: Open in new tab Items tab: Navigate to your table Create item: Add test data:  { \u0026#34;PK\u0026#34;: \u0026#34;LAMBDA#test-processing\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;EVENT#001\u0026#34;, \u0026#34;event_type\u0026#34;: \u0026#34;lambda_test\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Testing Lambda stream processing\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-08-11T16:30:00Z\u0026#34;, \u0026#34;test_purpose\u0026#34;: \u0026#34;verify_lambda_trigger\u0026#34; } Create: Save the item  Screenshot Location: Add screenshot of DynamoDB item creation for Lambda testing\n\rStep 2: Monitor Lambda Execution Check function invocation:\n Lambda console: Return to your function Monitor tab: Click to view metrics Invocations: Should show 1 new invocation Duration: Typically \u0026lt; 1 second Errors: Should be 0  Screenshot Location: Add screenshot of Lambda monitoring tab showing successful invocation\n\rStep 3: Check Processing Logs View detailed logs:\n CloudWatch logs: Click \u0026ldquo;View CloudWatch logs\u0026rdquo; Log stream: Click latest log stream Log entries: Look for:  START RequestId: [uuid] Processing INSERT for item: LAMBDA#test-processing#EVENT#001 Processing INSERT event Successfully processed 1 records END RequestId: [uuid]    Screenshot Location: Add screenshot of CloudWatch logs showing successful stream processing\n\rExercise 5: Advanced Configuration Error Handling Configuration Configure retry and error handling:\n Event source mapping: Edit your DynamoDB trigger Additional settings:  Retry attempts: 3 (default) Maximum record age: 3600 seconds Split batch on error: Enable Dead letter queue: Configure SNS/SQS (optional)    Performance Optimization Free Tier optimization settings:\n Parallelization factor: 1 (avoid excess concurrency) Batch size: 10 records (balance latency vs cost) Reserved concurrency: 10 (control costs) Provisioned concurrency: 0 (not needed for streams)  Monitoring and Alerting Set up basic monitoring:\n CloudWatch Alarms: Create for:  Function errors \u0026gt; 0 Function duration \u0026gt; 20 seconds Iterator age \u0026gt; 30 seconds   Notifications: SNS topic for alerts Dashboard: Add metrics to CloudWatch dashboard  Screenshot Location: Add screenshot of CloudWatch alarm configuration for Lambda function\n\rFunction Testing Patterns Test Different Event Types Comprehensive testing:\n INSERT: Create new items MODIFY: Update existing items REMOVE: Delete items Batch: Multiple rapid changes  Validation Checklist Verify your setup:\n âœ… Lambda function created with correct runtime âœ… Stream trigger configured with proper permissions âœ… Code deployed and syntax validated âœ… Test successful with sample data âœ… Logs showing processing details âœ… Metrics indicating healthy execution  Troubleshooting Common Issues Lambda Not Triggering Check these items:\n Stream enabled: DynamoDB stream is active Permissions: Lambda has stream read permissions Event source mapping: Trigger is enabled Function state: Lambda is active (not failed)  Processing Errors Debug steps:\n CloudWatch logs: Check for error messages Timeout issues: Increase timeout if needed Memory errors: Monitor memory usage Permissions: Verify all required permissions  Performance Issues Optimization tips:\n Batch size: Adjust based on processing time Memory allocation: Right-size for your workload Cold starts: Consider provisioned concurrency if needed Error handling: Implement proper retry logic  Lambda Ready: Your function is now processing DynamoDB stream events in real-time!\n\rNext Steps With your Lambda function processing stream events, you\u0026rsquo;re ready to practice with real data changes and explore different event-driven patterns. The next section covers hands-on stream processing exercises.\n"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/5-monitoring-optimization/5.1-cloudwatch-dashboards/",
	"title": "5.1 CloudWatch Dashboards",
	"tags": [],
	"description": "",
	"content": "Visual Monitoring Setup ğŸ“Š Create comprehensive CloudWatch dashboards for real-time DynamoDB monitoring\nOverview CloudWatch dashboards provide visual insights into your DynamoDB performance, capacity utilization, and system health. Build production-ready monitoring that helps you detect issues before they impact users.\nDashboard Strategy Multi-layered monitoring approach:\n Executive Dashboard: High-level health indicators Operations Dashboard: Detailed metrics for daily monitoring Troubleshooting Dashboard: Deep-dive analysis views  Exercise 1: Create Primary Monitoring Dashboard Step 1: Access CloudWatch Console Navigate to dashboard creation:\n AWS Console: Search \u0026ldquo;CloudWatch\u0026rdquo; Dashboards: Click \u0026ldquo;Dashboards\u0026rdquo; in left sidebar Create dashboard: Click \u0026ldquo;Create dashboard\u0026rdquo; button Dashboard name: DynamoDB-Production-Monitoring Create dashboard: Click to proceed  Screenshot Location: Add screenshot of CloudWatch console with Dashboards section highlighted\n\rStep 2: Add DynamoDB Capacity Widget Create capacity utilization monitoring:\n  Add widget: Click \u0026ldquo;Add widget\u0026rdquo; button\n  Widget type: Select \u0026ldquo;Line\u0026rdquo; chart\n  Data source: Choose \u0026ldquo;Metrics\u0026rdquo;\n  Browse metrics:\n AWS/DynamoDB â†’ Table Metrics Table: Select demo-ecommerce-freetier Metrics: Check all these:  ConsumedReadCapacityUnits ConsumedWriteCapacityUnits ProvisionedReadCapacityUnits ProvisionedWriteCapacityUnits      Configure widget:\n Title: \u0026ldquo;DynamoDB Capacity Utilization\u0026rdquo; Period: 5 minutes Statistic: Average Y-axis range: Auto    Screenshot Location: Add screenshot of widget configuration with DynamoDB capacity metrics selected\n\rStep 3: Add Performance Metrics Widget Monitor response times and errors:\n  Add widget: Click \u0026ldquo;Add widget\u0026rdquo; (second widget)\n  Widget type: Line chart\n  Metrics selection:\n AWS/DynamoDB â†’ Table Metrics Table: demo-ecommerce-freetier Select metrics:  SuccessfulRequestLatency (Query operations) SuccessfulRequestLatency (GetItem operations) UserErrors SystemErrors ThrottledRequests      Widget configuration:\n Title: \u0026ldquo;Performance \u0026amp; Error Metrics\u0026rdquo; Period: 5 minutes Y-axis: Split axis (latency vs errors)    Screenshot Location: Add screenshot of performance metrics widget configuration\n\rStep 4: Add Storage and Item Count Widget Track data growth and storage usage:\n  Add widget: Third widget\n  Widget type: Line chart\n  Metrics:\n AWS/DynamoDB â†’ Table Metrics Table: demo-ecommerce-freetier Select:  ItemCount TableSizeBytes      Configuration:\n Title: \u0026ldquo;Storage Utilization\u0026rdquo; Period: 1 hour (slower changing metrics) Y-axis: Split axis (count vs bytes)    Screenshot Location: Add screenshot of storage metrics configuration showing ItemCount and TableSizeBytes\n\rStep 5: Add Lambda Stream Processing Widget Monitor stream processing performance:\n  Add widget: Fourth widget\n  Metrics:\n AWS/Lambda â†’ Function Metrics Function: demo-dynamodb-stream-processor Select:  Invocations Duration Errors Throttles      Configuration:\n Title: \u0026ldquo;Stream Processing Health\u0026rdquo; Period: 5 minutes    Screenshot Location: Add screenshot of Lambda metrics widget showing stream processing performance\n\rExercise 2: Global Tables Monitoring (if applicable) Step 1: Add Replication Metrics Monitor cross-region replication:\n  Add widget: Fifth widget\n  Metrics:\n AWS/DynamoDB â†’ Global Table Metrics Source/Destination: Your regions Select:  ReplicationLatency PendingReplicationCount      Configuration:\n Title: \u0026ldquo;Global Tables Replication\u0026rdquo; Period: 5 minutes    Screenshot Location: Add screenshot of Global Tables replication metrics configuration\n\rExercise 3: Dashboard Customization Step 1: Organize Widget Layout Optimize dashboard layout:\n Resize widgets: Drag corners to adjust size Arrange layout:  Top row: Capacity + Performance (most critical) Second row: Storage + Lambda processing Third row: Global Tables (if applicable)   Widget spacing: Leave room for annotations  Screenshot Location: Add screenshot of final dashboard layout with all widgets organized\n\rStep 2: Configure Time Range and Auto-Refresh Set operational parameters:\n Time range: Set to \u0026ldquo;Last 3 hours\u0026rdquo; Auto-refresh: Enable 1-minute refresh Time zone: Set to your local time zone Save dashboard: Click \u0026ldquo;Save dashboard\u0026rdquo;  Step 3: Add Dashboard Annotations Add context and documentation:\n Add widget: Text widget Widget content:  # DynamoDB Production Dashboard ## Key Metrics: - **Capacity**: Target \u0026lt;80% utilization - **Latency**: Target \u0026lt;50ms average - **Errors**: Target 0% error rate - **Storage**: Monitor 25GB Free Tier limit ## Alert Thresholds: - High capacity: \u0026gt;4 RCU/WCU (80% of 5) - High latency: \u0026gt;100ms sustained - Any errors: \u0026gt;0 errors per period Last Updated: [Date] \rScreenshot Location: Add screenshot of text widget with dashboard documentation\n\rExercise 4: Specialized Dashboards Step 1: Create Cost Monitoring Dashboard Track Free Tier usage:\n New dashboard: \u0026ldquo;DynamoDB-Cost-Analysis\u0026rdquo; Add widgets:  Billing metrics: EstimatedCharges Usage metrics: Free Tier consumption Capacity trends: Weekly capacity usage Storage growth: Data size over time    Step 2: Create Troubleshooting Dashboard Deep-dive analysis dashboard:\n Dashboard name: \u0026ldquo;DynamoDB-Troubleshooting\u0026rdquo; Detailed metrics:  Per-operation latency: Broken down by operation type Hot partition detection: ConsumedCapacity by partition Error analysis: Error types and frequencies Throttling patterns: When and why throttling occurs    Screenshot Location: Add screenshot of troubleshooting dashboard with detailed operational metrics\n\rExercise 5: Dashboard Sharing and Access Step 1: Configure Dashboard Permissions Set up team access:\n Dashboard actions: Click \u0026ldquo;Actions\u0026rdquo; menu Share dashboard: Select sharing options Access controls:  Public: No (keep private) Account access: Specific IAM users/roles Read-only: Recommended for most users    Step 2: Create Dashboard URLs Generate shareable links:\n Get shareable URL: Copy dashboard URL Embed options: For inclusion in other tools Export options: PNG/PDF for reports  Screenshot Location: Add screenshot of dashboard sharing configuration options\n\rExercise 6: Dashboard Maintenance Step 1: Set Up Dashboard Alerts Monitor dashboard health:\n Missing data alerts: When metrics stop reporting Dashboard access logs: Who is viewing dashboards Widget performance: Load times and responsiveness  Step 2: Regular Review Process Dashboard optimization schedule:\n Daily: Quick health check of all dashboards Weekly: Review metric relevance and thresholds Monthly: Optimize layout and add new metrics Quarterly: Archive unused dashboards  Dashboard Best Practices Widget Organization Effective dashboard design:\n   Row Left Widget Right Widget     Top Critical Metrics Performance Trends    (Capacity, Errors) (Latency, Success)   Middle Resource Usage Stream Processing    (Storage, Items) (Lambda, Events)   Bottom Global Tables \u0026amp; Replication     (if applicable)     Color Coding Strategy Visual consistency:\n Green: Normal/healthy metrics Yellow: Warning thresholds approached Red: Critical issues requiring attention Blue: Informational/baseline metrics  Metric Selection Guidelines Choose the right metrics:\n   Purpose Primary Metrics Secondary Metrics     Health Errors, Throttles Success rate, Availability   Performance Latency, Duration Throughput, Concurrency   Capacity Consumed vs Provisioned Utilization percentage   Cost Storage size, Operations Free Tier consumption    Troubleshooting Dashboard Issues Common Problems Dashboard not loading:\n Check region: Verify correct AWS region Permissions: Ensure CloudWatch read access Browser cache: Clear cache and reload Service status: Check AWS status page  Missing metrics data:\n Time range: Adjust time window Metric delay: Wait for data propagation Resource activity: Generate activity to create metrics Metric retention: Check data retention periods  Performance issues:\n Too many widgets: Reduce widget count Long time ranges: Shorten time periods High resolution: Use lower resolution for overview Auto-refresh: Reduce refresh frequency  Free Tier Note: CloudWatch provides 10 custom metrics and 3 dashboards free. Plan your dashboard strategy accordingly.\n\rExercise Summary You\u0026rsquo;ve created comprehensive monitoring dashboards:\n âœ… Primary dashboard with essential DynamoDB metrics âœ… Performance monitoring for latency and errors âœ… Capacity tracking to prevent throttling âœ… Storage monitoring for cost management âœ… Stream processing health visibility âœ… Specialized dashboards for cost and troubleshooting  Dashboard Mastery: You now have production-grade visual monitoring that provides complete visibility into your DynamoDB system!\n\rNext Steps With dashboards providing visual monitoring, the next step is setting up proactive alerting to detect issues before they impact users. We\u0026rsquo;ll configure intelligent alarms and notification systems.\n"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/5-monitoring-optimization/5.2-cost-analysis-optimization/",
	"title": "5.2 Cost Analysis &amp; Optimization",
	"tags": [],
	"description": "",
	"content": "Free Tier Management \u0026amp; Cost Control ğŸ’° Analyze AWS costs, optimize Free Tier usage, and implement cost control strategies\nOverview Effective cost management ensures you maximize AWS Free Tier benefits while building production-ready skills. Learn to monitor spending, optimize resource usage, and scale efficiently without unexpected charges.\nCost Analysis Strategy Multi-dimensional cost tracking:\n Service-level: DynamoDB, Lambda, CloudWatch costs Resource-level: Per-table, per-function expenses Time-based: Daily, weekly, monthly trends Feature-level: Global Tables, Streams, Backups  Free Tier Optimization Goals Maximize value within limits:\n Stay within Free Tier: Avoid any charges Learn production patterns: Real-world techniques Scale-ready design: Prepared for growth Cost visibility: Understand pricing model  Exercise 1: AWS Cost Explorer Analysis Step 1: Access Billing Dashboard Navigate to cost analysis tools:\n AWS Console: Click account name (top-right) Billing and Cost Management: Select from dropdown Cost Explorer: Click \u0026ldquo;Cost Explorer\u0026rdquo; in left sidebar Launch Cost Explorer: Click to open (may take a few minutes first time)  Screenshot Location: Add screenshot of AWS Console billing dropdown and Cost Explorer access\n\rStep 2: Analyze DynamoDB Costs Review current DynamoDB expenses:\n Time range: Set to \u0026ldquo;Last 3 months\u0026rdquo; Group by: Service Filter services: Add filter for \u0026ldquo;DynamoDB\u0026rdquo; View type: Daily costs Expected result: $0.00 for all periods (Free Tier)  Screenshot Location: Add screenshot of Cost Explorer showing DynamoDB costs at $0.00\n\rStep 3: Service Breakdown Analysis Detailed cost breakdown:\n Group by: Service and Operation Services: Include DynamoDB, Lambda, CloudWatch Analysis:  DynamoDB: Read/write operations, storage Lambda: Invocations, duration CloudWatch: Metrics, logs, alarms   Verify: All services within Free Tier limits  Screenshot Location: Add screenshot of detailed service cost breakdown with operations\n\rExercise 2: Free Tier Usage Monitoring Step 1: Access Free Tier Dashboard Check Free Tier consumption:\n Billing console: Navigate to main billing page Free Tier: Click \u0026ldquo;Free Tier\u0026rdquo; tab Service filters: Select relevant services Review usage:  DynamoDB: Read/write capacity usage Lambda: Invocations and compute time CloudWatch: Metrics and alarms    Screenshot Location: Add screenshot of Free Tier dashboard showing DynamoDB usage percentages\n\rStep 2: Analyze Usage Patterns DynamoDB Free Tier consumption:\nCurrent usage tracking:\nFree Tier Utilization Analysis:\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ Resource â”‚ Free Tier Limit â”‚ Current Usage â”‚\râ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\râ”‚ Read Capacity â”‚ 25 RCU â”‚ ~15 RCU (60%) â”‚\râ”‚ Write Capacity â”‚ 25 WCU â”‚ ~15 WCU (60%) â”‚\râ”‚ Storage â”‚ 25 GB â”‚ \u0026lt;0.1 GB (0.4%) â”‚\râ”‚ Lambda Invocations â”‚ 1M requests â”‚ \u0026lt;1K (0.1%) â”‚\râ”‚ CloudWatch Metrics â”‚ 10 metrics â”‚ 8 metrics (80%) â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\rStep 3: Set Usage Alerts Prevent Free Tier overages:\n Billing preferences: Navigate to preferences Billing alerts: Enable \u0026ldquo;Receive Free Tier Usage Alerts\u0026rdquo; Email address: Confirm notification email Usage thresholds: Set alerts at 80% of limits  Screenshot Location: Add screenshot of Free Tier alert configuration\n\rExercise 3: DynamoDB Cost Optimization Step 1: Capacity Utilization Analysis Optimize read/write capacity:\n  CloudWatch: Navigate to DynamoDB metrics\n  Capacity analysis:\n Peak usage: Identify highest consumption periods Average usage: Calculate typical utilization Efficiency: Consumed vs provisioned ratio    Optimization opportunities:\n Current setup: 5 RCU/WCU per table + GSIs Utilization: ~40-60% average usage Optimization: Well-sized for Free Tier    Screenshot Location: Add screenshot of CloudWatch showing DynamoDB capacity utilization patterns\n\rStep 2: Storage Optimization Minimize storage costs:\n  Data lifecycle analysis:\n Current storage: Check table size metrics Growth rate: Monitor size increase over time Data patterns: Identify large or unused items    Optimization strategies:\n Item design: Remove unnecessary attributes Data archiving: Move old data to S3 Compression: Efficient data formats    Step 3: Query Optimization Improve read efficiency:\n  Query analysis:\n Scan vs Query: Prefer Query operations Projection: Use GSI projections effectively Batch operations: Group requests when possible    Cost impact:\n Query: 1 RCU per 4KB read Scan: Consumes RCU for entire scan GSI queries: Use projected attributes    Screenshot Location: Add screenshot showing query performance metrics and RCU consumption\n\rExercise 4: Lambda Cost Optimization Step 1: Function Performance Analysis Optimize Lambda costs:\n  CloudWatch: Review Lambda metrics\n  Performance analysis:\n Duration: Target \u0026lt;1 second execution Memory: 128MB optimal for Free Tier Invocations: Monitor request frequency    Cost breakdown:\n Requests: 1M free per month Compute time: 400,000 GB-seconds free Current usage: \u0026lt;1% of limits    Screenshot Location: Add screenshot of Lambda cost analysis showing Free Tier usage\n\rStep 2: Code Optimization Improve function efficiency:\n# Optimized Lambda function for cost efficiency import json import boto3 import logging # Reuse connections (outside handler) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(\u0026#39;demo-ecommerce-freetier\u0026#39;) logger = logging.getLogger() logger.setLevel(logging.INFO) def lambda_handler(event, context): \u0026#34;\u0026#34;\u0026#34; Cost-optimized stream processing \u0026#34;\u0026#34;\u0026#34; try: # Process in batches for efficiency processed = 0 for record in event[\u0026#39;Records\u0026#39;]: # Minimal processing for Free Tier optimization process_efficiently(record) processed += 1 return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;processed\u0026#39;: processed } except Exception as e: logger.error(f\u0026#34;Error: {e}\u0026#34;) raise def process_efficiently(record): \u0026#34;\u0026#34;\u0026#34; Streamlined processing to minimize duration \u0026#34;\u0026#34;\u0026#34; # Essential processing only event_name = record[\u0026#39;eventName\u0026#39;] logger.info(f\u0026#34;Processed {event_name}event\u0026#34;) # Additional business logic as needed Step 3: Monitoring and Alerting Costs CloudWatch cost management:\n  Metrics optimization:\n Custom metrics: Use sparingly (10 free) Log retention: Set appropriate retention periods Dashboard widgets: Limit to essential views    Cost tracking:\n Logs: 5GB free per month Metrics: 10 custom metrics free Alarms: 10 alarms free    Exercise 5: Scaling Cost Projections Step 1: Growth Scenario Planning Project costs for scaling:\n  Current baseline: Document Free Tier usage\n  Growth scenarios:\n 10x traffic: Capacity and cost impact 100x traffic: Migration to on-demand billing Global expansion: Multi-region costs    Cost modeling:\n  Scaling Cost Projections:\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ Traffic Level â”‚ RCU/WCU â”‚ Monthly Costâ”‚ Billing Modeâ”‚\râ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\râ”‚ Current (Free) â”‚ 15/15 â”‚ $0.00 â”‚ Provisioned â”‚\râ”‚ 2x Growth â”‚ 30/30 â”‚ ~$15 â”‚ Provisioned â”‚\râ”‚ 10x Growth â”‚ 150/150 â”‚ ~$75 â”‚ On-Demand â”‚\râ”‚ 100x Growth â”‚ 1500/1500 â”‚ ~$750 â”‚ On-Demand â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\rScreenshot Location: Add screenshot of cost projection analysis in Cost Explorer\n\rStep 2: Optimization Strategies for Scale Cost-effective scaling approaches:\n  Provisioned vs On-Demand:\n Predictable traffic: Provisioned capacity Variable traffic: On-demand billing Hybrid approach: Mix both modes    Auto-scaling configuration:\n Target utilization: 70% capacity Scale-out: Quick scaling up Scale-in: Conservative scaling down    Reserved capacity (for large scale):\n Commitment discounts: Up to 76% savings Predictable workloads: Best for steady traffic    Exercise 6: Cost Governance Setup Step 1: Budget Creation Set spending limits:\n  AWS Budgets: Navigate to Budgets service\n  Create budget: Click \u0026ldquo;Create budget\u0026rdquo;\n  Budget configuration:\n Budget type: Cost budget Time range: Monthly Budget amount: $5.00 (safety buffer) Filters: DynamoDB service    Alert thresholds:\n 80% threshold: Warning alert 100% threshold: Critical alert    Screenshot Location: Add screenshot of budget creation with DynamoDB-specific filters\n\rStep 2: Cost Anomaly Detection Automated cost monitoring:\n  Cost Anomaly Detection: Enable service\n  Detection configuration:\n Services: DynamoDB, Lambda, CloudWatch Sensitivity: High (detect small anomalies) Notifications: Email alerts    Machine learning: AWS analyzes patterns automatically\n  Step 3: Resource Tagging Strategy Cost allocation and tracking:\n  Tagging strategy:\nTag Structure:\r- Environment: workshop/dev/prod\r- Project: dynamodb-learning\r- Owner: [your-name]\r- Cost-Center: education\r  Tag-based budgets: Create budgets by tag values\n  Cost allocation: Track costs by project/owner\n  Exercise 7: Optimization Recommendations Step 1: Right-Sizing Analysis Capacity optimization:\n  Current efficiency:\n Table utilization: 40-60% average GSI utilization: 20-40% average Lambda efficiency: \u0026lt;1 second execution    Optimization actions:\n Maintain current sizing: Well-optimized for Free Tier Monitor growth: Scale when approaching limits Query optimization: Continue efficient patterns    Step 2: Feature Optimization Service feature cost analysis:\n  Global Tables:\n Cost: Same as single-region within Free Tier Value: Multi-region availability Recommendation: Keep enabled for learning    DynamoDB Streams:\n Cost: Free feature Value: Real-time processing capability Recommendation: Continue using    Backups:\n Continuous backups: Free On-demand backups: First 10GB free monthly    Screenshot Location: Add screenshot of feature-by-feature cost analysis\n\rStep 3: Long-term Strategy Scaling preparation:\n Monitoring foundation: Already established Cost visibility: Comprehensive tracking in place Optimization patterns: Learned efficient practices Scaling readiness: Understand cost implications  Cost Optimization Best Practices Resource Efficiency Maximize Free Tier value:\n Batch operations: Group requests for efficiency Efficient queries: Use Query instead of Scan Right-size capacity: Match provisioning to usage Data lifecycle: Archive old data appropriately  Monitoring and Alerting Cost control automation:\n Usage alerts: Set at 80% of Free Tier limits Budget notifications: Multiple threshold levels Anomaly detection: Automated unusual usage detection Regular reviews: Monthly cost analysis  Scaling Economics Cost-effective growth:\n Gradual scaling: Increase capacity incrementally Performance testing: Validate before scaling Alternative billing: Consider on-demand for variable loads Reserved capacity: For predictable, large-scale workloads  Cost Safety: Always monitor Free Tier usage closely and set up billing alerts to prevent unexpected charges.\n\rExercise Summary You\u0026rsquo;ve mastered cost analysis and optimization:\n âœ… Cost Explorer analysis for detailed spending insights âœ… Free Tier monitoring to maximize benefits âœ… Resource optimization for efficient utilization âœ… Scaling projections for growth planning âœ… Cost governance with budgets and alerts âœ… Best practices for long-term cost management  Cost Mastery: You can now operate DynamoDB efficiently within Free Tier limits while preparing for cost-effective scaling!\n\rNext Steps With cost optimization mastered, the final section covers performance tuning techniques to maximize efficiency and prepare your system for production-scale operations.\n"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/6-advanced-patterns/6.1-batch-operations/",
	"title": "6.1 Batch Operations",
	"tags": [],
	"description": "",
	"content": "Efficient Multi-Item Processing âš¡ Master batch operations to process multiple items with single API calls for maximum efficiency\nOverview Batch operations are the key to efficient DynamoDB usage. Instead of making individual API calls for each item, batch operations allow you to process up to 25 items in a single request, dramatically reducing latency and improving performance.\nBatch Operation Types Available batch operations:\n BatchWriteItem: Insert or delete up to 25 items BatchGetItem: Retrieve up to 100 items Reduced API calls: Fewer round trips to DynamoDB Lower latency: Bulk processing efficiency  Performance Comparison Single Operations (Inefficient):\r- 100 items = 100 API calls\r- Higher latency per operation\r- More request units consumed\rBatch Operations (Efficient):\r- 100 items = 4 API calls (25 items each)\r- Lower overall latency\r- Optimal request unit usage\rExercise 1: Batch Write Operations Step 1: Create Batch Product Data Prepare batch data for multiple product insertion:\n AWS CloudShell: Open AWS CloudShell from the AWS Console Create batch file: Generate JSON for multiple products  Create batch-products.json file:\ncat \u0026gt; batch-products.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; { \u0026#34;demo-ecommerce-freetier\u0026#34;: [ { \u0026#34;PutRequest\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-laptop-001\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;}, \u0026#34;GSI1PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;CATEGORY#electronics\u0026#34;}, \u0026#34;GSI1SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-laptop-001\u0026#34;}, \u0026#34;name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Gaming Laptop Pro\u0026#34;}, \u0026#34;price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1299\u0026#34;}, \u0026#34;stock\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;15\u0026#34;}, \u0026#34;category\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;electronics\u0026#34;}, \u0026#34;brand\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;TechPro\u0026#34;} } } }, { \u0026#34;PutRequest\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-mouse-002\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;}, \u0026#34;GSI1PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;CATEGORY#electronics\u0026#34;}, \u0026#34;GSI1SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-mouse-002\u0026#34;}, \u0026#34;name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Wireless Gaming Mouse\u0026#34;}, \u0026#34;price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;79\u0026#34;}, \u0026#34;stock\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;50\u0026#34;}, \u0026#34;category\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;electronics\u0026#34;}, \u0026#34;brand\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;GamerGear\u0026#34;} } } }, { \u0026#34;PutRequest\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-keyboard-003\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;}, \u0026#34;GSI1PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;CATEGORY#electronics\u0026#34;}, \u0026#34;GSI1SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-keyboard-003\u0026#34;}, \u0026#34;name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Mechanical Keyboard RGB\u0026#34;}, \u0026#34;price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;159\u0026#34;}, \u0026#34;stock\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;25\u0026#34;}, \u0026#34;category\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;electronics\u0026#34;}, \u0026#34;brand\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;KeyMaster\u0026#34;} } } } ] } EOF \rScreenshot Location: Add screenshot of CloudShell with the batch-products.json file created\n\rStep 2: Execute Batch Write Perform batch write operation:\n Execute batch write command:  aws dynamodb batch-write-item --request-items file://batch-products.json Verify successful execution:  Check for no error messages Note the response showing processed items All 3 items created with single API call    Screenshot Location: Add screenshot showing successful batch write response in CloudShell\n\rStep 3: Verify Batch Creation Confirm items were created in DynamoDB Console:\n DynamoDB Console: Navigate to DynamoDB service Select table: Click on demo-ecommerce-freetier table View items: Click \u0026ldquo;Explore table items\u0026rdquo; Verify batch items: Look for the 3 newly created products  Items to verify:\n PRODUCT#batch-laptop-001: Gaming Laptop Pro PRODUCT#batch-mouse-002: Wireless Gaming Mouse PRODUCT#batch-keyboard-003: Mechanical Keyboard RGB  Screenshot Location: Add screenshot of DynamoDB console showing the 3 batch-created items\n\rExercise 2: Batch Read Operations Step 1: Create Batch Get Request Prepare batch read operation:\n Create batch-get.json file:  cat \u0026gt; batch-get.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; { \u0026#34;demo-ecommerce-freetier\u0026#34;: { \u0026#34;Keys\u0026#34;: [ { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-laptop-001\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;} }, { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-mouse-002\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;} }, { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-keyboard-003\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;} } ] } } EOF Step 2: Execute Batch Get Retrieve multiple items with single API call:\n Execute batch get command:  aws dynamodb batch-get-item --request-items file://batch-get.json Analyze response:  All 3 items returned in single response Complete item data for each product Much faster than 3 separate GetItem calls    Screenshot Location: Add screenshot of batch get response showing all 3 items retrieved at once\n\rStep 3: Performance Comparison Compare batch vs individual operations:\nSingle GetItem (for comparison):\n# Single item retrieval (less efficient) aws dynamodb get-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#batch-laptop-001\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; Performance analysis:\n Batch operation: 1 API call for 3 items Individual operations: 3 API calls for 3 items Efficiency gain: 3x fewer API calls Latency reduction: Significant improvement  Performance Benefit: Batch operations reduce API calls by up to 25x for writes and 100x for reads, dramatically improving application performance!\n\rExercise 3: Advanced Batch Patterns Step 1: Mixed Batch Operations Combine PutRequest and DeleteRequest in single batch:\n Create mixed-batch.json:  cat \u0026gt; mixed-batch.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; { \u0026#34;demo-ecommerce-freetier\u0026#34;: [ { \u0026#34;PutRequest\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-headset-004\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;}, \u0026#34;GSI1PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;CATEGORY#electronics\u0026#34;}, \u0026#34;GSI1SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-headset-004\u0026#34;}, \u0026#34;name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Wireless Headset Pro\u0026#34;}, \u0026#34;price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;199\u0026#34;}, \u0026#34;stock\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;20\u0026#34;}, \u0026#34;category\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;electronics\u0026#34;} } } }, { \u0026#34;DeleteRequest\u0026#34;: { \u0026#34;Key\u0026#34;: { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-mouse-002\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;} } } } ] } EOF Execute mixed batch operation:  aws dynamodb batch-write-item --request-items file://mixed-batch.json \rScreenshot Location: Add screenshot showing mixed batch operation execution (add + delete)\n\rStep 2: Error Handling in Batch Operations Understanding batch operation limitations:\n Create oversized batch (to demonstrate limits):  # Create a batch with more than 25 items (will be split automatically by AWS CLI) cat \u0026gt; large-batch.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; { \u0026#34;demo-ecommerce-freetier\u0026#34;: [ EOF # Add multiple items to demonstrate batch limits for i in {1..30}; do cat \u0026gt;\u0026gt; large-batch.json \u0026lt;\u0026lt; EOF { \u0026#34;PutRequest\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-test-$i\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;}, \u0026#34;name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Test Product $i\u0026#34;}, \u0026#34;price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;10\u0026#34;} } } }$([ $i -lt 30 ] \u0026amp;\u0026amp; echo \u0026#34;,\u0026#34;) EOF done cat \u0026gt;\u0026gt; large-batch.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; ] } EOF Execute and observe AWS CLI handling:  aws dynamodb batch-write-item --request-items file://large-batch.json Batch operation best practices:\n Maximum 25 items per BatchWriteItem request Maximum 100 items per BatchGetItem request Handle UnprocessedItems in response for retries Implement exponential backoff for failed items  Step 3: Batch Operations with Projections Optimize batch reads with projection expressions:\n Create projection batch get:  cat \u0026gt; batch-get-projection.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; { \u0026#34;demo-ecommerce-freetier\u0026#34;: { \u0026#34;Keys\u0026#34;: [ { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-laptop-001\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;} }, { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-keyboard-003\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;} }, { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#batch-headset-004\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;} } ], \u0026#34;ProjectionExpression\u0026#34;: \u0026#34;PK, SK, #n, price, stock\u0026#34;, \u0026#34;ExpressionAttributeNames\u0026#34;: { \u0026#34;#n\u0026#34;: \u0026#34;name\u0026#34; } } } EOF Execute optimized batch get:  aws dynamodb batch-get-item --request-items file://batch-get-projection.json Benefits of projection in batch operations:\n Reduced data transfer: Only fetch needed attributes Lower RCU consumption: Pay only for retrieved data Faster response times: Less data to process Network efficiency: Smaller payload sizes  Screenshot Location: Add screenshot comparing full item vs projected item responses\n\rExercise 4: Production Batch Patterns Step 1: Batch Processing with Error Handling Implement robust batch processing:\nimport boto3 import json import time from botocore.exceptions import ClientError class BatchProcessor: def __init__(self, table_name): self.dynamodb = boto3.client(\u0026#39;dynamodb\u0026#39;) self.table_name = table_name def batch_write_with_retry(self, items, max_retries=3): \u0026#34;\u0026#34;\u0026#34; Batch write with automatic retry for unprocessed items \u0026#34;\u0026#34;\u0026#34; request_items = {self.table_name: items} for attempt in range(max_retries): try: response = self.dynamodb.batch_write_item( RequestItems=request_items ) # Check for unprocessed items unprocessed = response.get(\u0026#39;UnprocessedItems\u0026#39;, {}) if not unprocessed: return True # All items processed successfully # Retry unprocessed items with exponential backoff wait_time = (2 ** attempt) * 0.1 time.sleep(wait_time) request_items = unprocessed except ClientError as e: print(f\u0026#34;Batch write error (attempt {attempt + 1}): {e}\u0026#34;) if attempt == max_retries - 1: raise time.sleep(2 ** attempt) return False # Some items remained unprocessed def batch_get_with_projection(self, keys, projection_expression=None): \u0026#34;\u0026#34;\u0026#34; Batch get with optional projection \u0026#34;\u0026#34;\u0026#34; request_items = { self.table_name: {\u0026#39;Keys\u0026#39;: keys} } if projection_expression: request_items[self.table_name][\u0026#39;ProjectionExpression\u0026#39;] = projection_expression response = self.dynamodb.batch_get_item(RequestItems=request_items) return response.get(\u0026#39;Responses\u0026#39;, {}).get(self.table_name, []) Step 2: Batch Operation Monitoring Monitor batch operation performance:\n  CloudWatch metrics to monitor:\n UserErrors: Failed batch operations SystemErrors: DynamoDB-side issues ThrottledRequests: Capacity exceeded ConsumedReadCapacityUnits: RCU usage ConsumedWriteCapacityUnits: WCU usage    Performance optimization:\n Batch size tuning: Use maximum batch sizes Parallel processing: Multiple concurrent batches Error handling: Implement retry logic Monitoring: Track success rates and latency    Screenshot Location: Add screenshot of CloudWatch metrics showing batch operation performance\n\rStep 3: Cost Optimization with Batching Maximize Free Tier efficiency:\nCost comparison:\nIndividual Operations:\r- 100 items Ã— 1 WCU each = 100 WCU\r- 100 separate API calls\r- Higher latency\rBatch Operations: - 100 items Ã· 25 per batch = 4 API calls\r- Same 100 WCU total\r- 25x fewer API calls\r- Lower latency\rFree Tier benefits:\n Efficient RCU/WCU usage: Same capacity consumption Reduced API overhead: Fewer requests Better throughput: More items processed per second Improved user experience: Faster application response  Batch Operations Best Practices Efficiency Guidelines Maximize batch effectiveness:\n Use maximum batch sizes: 25 items for writes, 100 for reads Group related operations: Batch similar operations together Implement retry logic: Handle UnprocessedItems properly Monitor performance: Track batch success rates  Error Handling Robust batch processing:\n Exponential backoff: Gradual retry delays Partial success handling: Process successful items Logging and monitoring: Track failed operations Graceful degradation: Fall back to individual operations  Performance Optimization Batch operation tuning:\n Parallel batches: Process multiple batches concurrently Projection expressions: Reduce data transfer Capacity planning: Ensure adequate throughput Connection pooling: Reuse DynamoDB connections  Batch Mastery: You\u0026rsquo;ve mastered batch operations! You can now process multiple items efficiently, reduce API calls by up to 25x, and optimize for both performance and cost.\n\rExercise Summary You\u0026rsquo;ve learned batch operation techniques:\n âœ… Batch write operations for efficient data insertion âœ… Batch read operations for fast data retrieval âœ… Mixed batch operations combining puts and deletes âœ… Error handling patterns for robust processing âœ… Performance optimization with projections âœ… Production patterns for enterprise applications  Next Steps Continue to 6.2 Conditional Updates to learn data integrity patterns and prevent race conditions in concurrent environments.\n"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/6-advanced-patterns/6.2-conditional-updates/",
	"title": "6.2 Conditional Updates",
	"tags": [],
	"description": "",
	"content": "Data Integrity and Race Condition Prevention ğŸ›¡ï¸ Master conditional updates to ensure data integrity and prevent race conditions in concurrent environments\nOverview Conditional updates are essential for maintaining data integrity in multi-user applications. They prevent race conditions, ensure business rules are enforced, and protect against data corruption when multiple clients access the same data simultaneously.\nThe Race Condition Problem Scenario: Two users buying the last item:\nWithout Conditions (Dangerous):\rUser A reads: stock = 1\rUser B reads: stock = 1 User A updates: stock = 0 âœ…\rUser B updates: stock = 0 âŒ (Should fail!)\rResult: Both succeed (oversold!)\rWith Conditions (Safe):\rUser A reads: stock = 1\rUser B reads: stock = 1\rUser A updates: stock = 0 (condition: stock \u0026gt;= 1) âœ…\rUser B updates: stock = 0 (condition: stock \u0026gt;= 1) âŒ FAILS\rResult: Only first succeeds âœ…\rConditional Expression Types Available condition types:\n attribute_exists(path): Item/attribute must exist attribute_not_exists(path): Item/attribute must not exist Comparison operators: =, \u0026lt;\u0026gt;, \u0026lt;, \u0026lt;=, \u0026gt;, \u0026gt;= BETWEEN and IN: Range and membership tests Functions: begins_with(), contains(), size()  Exercise 1: Basic Conditional Updates Step 1: Setup Test Product Create a product for conditional update testing:\n AWS CloudShell: Open CloudShell from AWS Console Create test product:  aws dynamodb put-item \\  --table-name demo-ecommerce-freetier \\  --item \u0026#39;{ \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#conditional-test\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;}, \u0026#34;GSI1PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;CATEGORY#test\u0026#34;}, \u0026#34;GSI1SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#conditional-test\u0026#34;}, \u0026#34;name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Conditional Test Product\u0026#34;}, \u0026#34;price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;100\u0026#34;}, \u0026#34;stock\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;5\u0026#34;}, \u0026#34;version\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;} }\u0026#39; Verify creation: Check item exists in DynamoDB Console  Screenshot Location: Add screenshot showing the test product created in DynamoDB console\n\rStep 2: Price Update with Condition Update price only if current price matches expected value:\n Successful conditional update:  aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET price = :new_price, last_updated = :timestamp\u0026#34; \\  --condition-expression \u0026#34;price = :current_price\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:new_price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;90\u0026#34;}, \u0026#34;:current_price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;100\u0026#34;}, \u0026#34;:timestamp\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;2025-08-12T10:00:00Z\u0026#34;} }\u0026#39; Verify price change: Check updated price in console  Screenshot Location: Add screenshot showing successful price update with condition\n\rStep 3: Failed Conditional Update Attempt update with wrong condition (should fail):\n Try update with incorrect current price:  aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET price = :new_price\u0026#34; \\  --condition-expression \u0026#34;price = :wrong_price\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:new_price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;80\u0026#34;}, \u0026#34;:wrong_price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;100\u0026#34;} }\u0026#39; Expected result: ConditionalCheckFailedException error Verify price unchanged: Price should still be 90  Screenshot Location: Add screenshot showing ConditionalCheckFailedException error in CloudShell\n\rExercise 2: Inventory Management with Conditions Step 1: Stock Decrement with Safety Check Implement safe inventory decrement:\n Purchase 2 items (should succeed):  aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET stock = stock - :quantity\u0026#34; \\  --condition-expression \u0026#34;stock \u0026gt;= :quantity\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:quantity\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;2\u0026#34;} }\u0026#39; Verify stock reduction: Stock should be 3 (was 5, bought 2)  Screenshot Location: Add screenshot showing stock reduced from 5 to 3\n\rStep 2: Prevent Overselling Attempt to buy more items than available:\n Try to purchase 5 items (should fail):  aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET stock = stock - :quantity\u0026#34; \\  --condition-expression \u0026#34;stock \u0026gt;= :quantity\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:quantity\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;5\u0026#34;} }\u0026#39; Expected result: ConditionalCheckFailedException (only 3 in stock) Verify stock unchanged: Stock should still be 3  This prevents overselling! ğŸ›¡ï¸\nScreenshot Location: Add screenshot showing failed purchase attempt due to insufficient stock\n\rStep 3: Advanced Inventory Patterns Implement complex inventory logic:\n Conditional stock update with minimum threshold:  aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET stock = stock - :quantity, last_sold = :timestamp\u0026#34; \\  --condition-expression \u0026#34;stock \u0026gt;= :quantity AND stock \u0026gt; :min_threshold\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:quantity\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;}, \u0026#34;:min_threshold\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;}, \u0026#34;:timestamp\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;2025-08-12T10:05:00Z\u0026#34;} }\u0026#39; Business rule: Keep at least 1 item in stock (reserve inventory)  Exercise 3: Optimistic Locking Pattern Step 1: Version-Based Updates Implement optimistic locking with version numbers:\n Read current version: Note the version number (should be 1)  aws dynamodb get-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --projection-expression \u0026#34;version\u0026#34; Update with version check:  aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET price = :new_price, version = version + :inc\u0026#34; \\  --condition-expression \u0026#34;version = :expected_version\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:new_price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;85\u0026#34;}, \u0026#34;:inc\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;}, \u0026#34;:expected_version\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;} }\u0026#39; Verify version increment: Version should now be 2  Screenshot Location: Add screenshot showing version incremented from 1 to 2\n\rStep 2: Simulate Concurrent Update Conflict Demonstrate optimistic locking protection:\n Attempt update with old version (should fail):  aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET price = :new_price, version = version + :inc\u0026#34; \\  --condition-expression \u0026#34;version = :old_version\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:new_price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;75\u0026#34;}, \u0026#34;:inc\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;}, \u0026#34;:old_version\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;} }\u0026#39; Expected result: ConditionalCheckFailedException (version is now 2, not 1) Protection achieved: Prevents overwriting changes from other users  Screenshot Location: Add screenshot showing version conflict error\n\rStep 3: Proper Version Update Flow Correct optimistic locking workflow:\n Read current item with version:  aws dynamodb get-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --projection-expression \u0026#34;#v, price, #n\u0026#34; \\  --expression-attribute-names \u0026#39;{ \u0026#34;#v\u0026#34;: \u0026#34;version\u0026#34;, \u0026#34;#n\u0026#34;: \u0026#34;name\u0026#34; }\u0026#39; Update with current version (should succeed):  aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET price = :new_price, version = version + :inc\u0026#34; \\  --condition-expression \u0026#34;version = :current_version\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:new_price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;88\u0026#34;}, \u0026#34;:inc\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;}, \u0026#34;:current_version\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;2\u0026#34;} }\u0026#39; Version now 3: Successful update with proper version control  Exercise 4: Advanced Conditional Patterns Step 1: Attribute Existence Conditions Create items only if they don\u0026rsquo;t exist:\n Create user profile (should succeed):  aws dynamodb put-item \\  --table-name demo-ecommerce-freetier \\  --item \u0026#39;{ \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;USER#conditional-user\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PROFILE\u0026#34;}, \u0026#34;email\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;user@example.com\u0026#34;}, \u0026#34;created_at\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;2025-08-12T10:10:00Z\u0026#34;} }\u0026#39; \\  --condition-expression \u0026#34;attribute_not_exists(PK)\u0026#34; Try to create same user again (should fail):  aws dynamodb put-item \\  --table-name demo-ecommerce-freetier \\  --item \u0026#39;{ \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;USER#conditional-user\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PROFILE\u0026#34;}, \u0026#34;email\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;different@example.com\u0026#34;}, \u0026#34;created_at\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;2025-08-12T10:15:00Z\u0026#34;} }\u0026#39; \\  --condition-expression \u0026#34;attribute_not_exists(PK)\u0026#34; Result: Second attempt fails (prevents duplicate users)\nScreenshot Location: Add screenshot showing duplicate prevention with attribute_not_exists\n\rStep 2: Complex Conditional Logic Combine multiple conditions with AND/OR:\n Update product with complex business rules:  aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET discount_price = :discount\u0026#34; \\  --condition-expression \u0026#34;price \u0026gt; :min_price AND stock \u0026gt; :min_stock\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:discount\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;70\u0026#34;}, \u0026#34;:min_price\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;50\u0026#34;}, \u0026#34;:min_stock\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;0\u0026#34;} }\u0026#39; Business rule: Only apply discount if price \u0026gt; $50 AND stock \u0026gt; 0  Step 3: Function-Based Conditions Use DynamoDB condition functions:\n Update if attribute contains specific value:  aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET featured = :featured\u0026#34; \\  --condition-expression \u0026#34;contains(#n, :keyword)\u0026#34; \\  --expression-attribute-names \u0026#39;{\u0026#34;#n\u0026#34;: \u0026#34;name\u0026#34;}\u0026#39; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:featured\u0026#34;: {\u0026#34;BOOL\u0026#34;: true}, \u0026#34;:keyword\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Test\u0026#34;} }\u0026#39; Condition: Only feature products with \u0026ldquo;Test\u0026rdquo; in the name  Screenshot Location: Add screenshot showing function-based condition success\n\rExercise 5: Production Conditional Patterns Step 1: E-commerce Order Processing Implement order creation with inventory check:\n# Create order only if product has sufficient stock aws dynamodb put-item \\  --table-name demo-ecommerce-freetier \\  --item \u0026#39;{ \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;ORDER#order-001\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DETAILS\u0026#34;}, \u0026#34;GSI1PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;USER#conditional-user\u0026#34;}, \u0026#34;GSI1SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;ORDER#order-001\u0026#34;}, \u0026#34;product_id\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;PRODUCT#conditional-test\u0026#34;}, \u0026#34;quantity\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;}, \u0026#34;status\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;pending\u0026#34;}, \u0026#34;created_at\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;2025-08-12T10:20:00Z\u0026#34;} }\u0026#39; \\  --condition-expression \u0026#34;attribute_not_exists(PK)\u0026#34; # Simultaneously decrement stock aws dynamodb update-item \\  --table-name demo-ecommerce-freetier \\  --key \u0026#39;{\u0026#34;PK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;PRODUCT#conditional-test\u0026#34;},\u0026#34;SK\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;DETAILS\u0026#34;}}\u0026#39; \\  --update-expression \u0026#34;SET stock = stock - :quantity, reserved_stock = if_not_exists(reserved_stock, :zero) + :quantity\u0026#34; \\  --condition-expression \u0026#34;stock \u0026gt;= :quantity\u0026#34; \\  --expression-attribute-values \u0026#39;{ \u0026#34;:quantity\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;}, \u0026#34;:zero\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;0\u0026#34;} }\u0026#39; Step 2: Error Handling for Conditions Handle conditional failures gracefully:\nimport boto3 from botocore.exceptions import ClientError def safe_inventory_decrement(product_id, quantity): \u0026#34;\u0026#34;\u0026#34; Safely decrement inventory with proper error handling \u0026#34;\u0026#34;\u0026#34; dynamodb = boto3.client(\u0026#39;dynamodb\u0026#39;) try: response = dynamodb.update_item( TableName=\u0026#39;demo-ecommerce-freetier\u0026#39;, Key={ \u0026#39;PK\u0026#39;: {\u0026#39;S\u0026#39;: f\u0026#39;PRODUCT#{product_id}\u0026#39;}, \u0026#39;SK\u0026#39;: {\u0026#39;S\u0026#39;: \u0026#39;DETAILS\u0026#39;} }, UpdateExpression=\u0026#39;SET stock = stock - :quantity\u0026#39;, ConditionExpression=\u0026#39;stock \u0026gt;= :quantity\u0026#39;, ExpressionAttributeValues={ \u0026#39;:quantity\u0026#39;: {\u0026#39;N\u0026#39;: str(quantity)} }, ReturnValues=\u0026#39;ALL_NEW\u0026#39; ) return {\u0026#39;success\u0026#39;: True, \u0026#39;item\u0026#39;: response[\u0026#39;Attributes\u0026#39;]} except ClientError as e: if e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] == \u0026#39;ConditionalCheckFailedException\u0026#39;: return {\u0026#39;success\u0026#39;: False, \u0026#39;error\u0026#39;: \u0026#39;Insufficient stock\u0026#39;} else: return {\u0026#39;success\u0026#39;: False, \u0026#39;error\u0026#39;: f\u0026#39;Database error: {e}\u0026#39;} # Usage example result = safe_inventory_decrement(\u0026#39;conditional-test\u0026#39;, 1) if result[\u0026#39;success\u0026#39;]: print(\u0026#34;Stock decremented successfully\u0026#34;) else: print(f\u0026#34;Failed: {result[\u0026#39;error\u0026#39;]}\u0026#34;) Step 3: Performance Monitoring Monitor conditional update patterns:\n  CloudWatch metrics to track:\n ConditionalCheckFailedException: Failed condition rate SuccessfulRequestLatency: Update performance ThrottledRequests: Capacity issues ItemCollectionMetrics: Item size growth    Optimize conditional performance:\n Minimize condition complexity: Simple conditions perform better Use efficient operators: = is faster than contains() Avoid unnecessary conditions: Only check what\u0026rsquo;s essential Monitor failure rates: High failure rates indicate design issues    Screenshot Location: Add screenshot of CloudWatch showing conditional update metrics\n\rConditional Updates Best Practices Design Guidelines Effective condition design:\n Keep conditions simple: Complex conditions impact performance Use appropriate operators: Choose the most efficient operator Minimize attribute references: Fewer attributes = better performance Plan for failures: Design application logic to handle condition failures  Common Patterns Production-ready conditional patterns:\nOptimistic Locking # Always increment version on updates UPDATE SET data = :new_data, version = version + 1 WHERE version = :expected_version Inventory Management # Prevent overselling UPDATE SET stock = stock - :quantity WHERE stock \u0026gt;= :quantity Unique Constraints # Prevent duplicates PUT item WHERE attribute_not_exists(primary_key) Business Rules # Enforce business logic UPDATE SET status = :new_status WHERE current_status = :expected_status AND user_role = :admin Error Handling Robust conditional update handling:\n Expect failures: Conditions are designed to fail sometimes Retry logic: Implement appropriate retry strategies User feedback: Provide meaningful error messages Logging: Track condition failures for analysis  Data Integrity Mastery: You\u0026rsquo;ve mastered conditional updates! Your applications can now handle concurrent access safely, prevent race conditions, and maintain data integrity at scale.\n\rExercise Summary You\u0026rsquo;ve implemented conditional update patterns:\n âœ… Basic conditional updates with price and stock management âœ… Inventory safety checks preventing overselling âœ… Optimistic locking with version control âœ… Attribute existence conditions for duplicate prevention âœ… Complex conditional logic with multiple criteria âœ… Production error handling for robust applications  Next Steps Continue to 6.3 Advanced Query Techniques to learn performance optimization strategies and advanced querying patterns.\n"
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://crimsondd.github.io/DynamoDB-Advanced-Patterns-and-Global-Tables-Streams/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]